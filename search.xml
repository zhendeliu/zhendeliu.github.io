<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>AI算法面试问题-机器学习（一）</title>
    <url>/2021/10/02/AI%E7%AE%97%E6%B3%95%E9%9D%A2%E8%AF%95%E9%97%AE%E9%A2%98-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%80%EF%BC%89/</url>
    <content><![CDATA[<h1 id="问题列表"><a href="#问题列表" class="headerlink" title="问题列表"></a>问题列表</h1><ol>
<li>详细说一下支持向量机的原理</li>
<li>数据归一化的原因</li>
<li>哪些算法不需要归一化处理</li>
<li>树形结构为什么不需要归一化</li>
<li>常用的距离计算有哪些，有什么区别</li>
<li>机器学习项目的流程</li>
<li>Logistic Regression 逻辑回归的原理</li>
<li>逻辑回归为什么要特征离散化</li>
<li>overfitting怎么解决</li>
<li>逻辑回归和SVM的区别与联系</li>
</ol>
<span id="more"></span>

<p>面试问题来源机器学习面试150题 <a href="https://www.zhihu.com/column/c_1284826692855771136">https://www.zhihu.com/column/c_1284826692855771136</a></p>
<h1 id="问题及解答"><a href="#问题及解答" class="headerlink" title="问题及解答"></a>问题及解答</h1><h2 id="详细说一下支持向量机的原理"><a href="#详细说一下支持向量机的原理" class="headerlink" title="详细说一下支持向量机的原理"></a>详细说一下支持向量机的原理</h2>]]></content>
      <categories>
        <category>学习</category>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>面试</tag>
        <tag>总结</tag>
        <tag>技巧</tag>
      </tags>
  </entry>
  <entry>
    <title>AI算法面试问题-机器学习（三）</title>
    <url>/2021/10/02/AI%E7%AE%97%E6%B3%95%E9%9D%A2%E8%AF%95%E9%97%AE%E9%A2%98-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%89%EF%BC%89/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>AI算法面试问题-机器学习（二）</title>
    <url>/2021/10/02/AI%E7%AE%97%E6%B3%95%E9%9D%A2%E8%AF%95%E9%97%AE%E9%A2%98-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%BA%8C%EF%BC%89/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>OpenCV图像处理基础（一）</title>
    <url>/2021/09/29/OpenCV%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E5%9F%BA%E7%A1%80%EF%BC%88%E4%B8%80%EF%BC%89/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>数字图像基础</p>
<p>读、写、显示（图片、视频）</p>
<p>缩放、补边、裁剪</p>
<p>绘制形状</p>
<p>书写文字</p>
<span id="more"></span>

<h1 id="图像的数字表示"><a href="#图像的数字表示" class="headerlink" title="图像的数字表示"></a>图像的数字表示</h1><p>在计算机系统中存储的是每张图像每个像素点的值</p>
<p>灰度图，单通道图像也就是8位图像，每个像素点的值占8字节，00000000-11111111也就是0-255</p>
<p>RGB/BGR图像，是三通道图像，也就是24位图像，每个像素点的值占24字节，也就是三个通道每个通道的值占8字节0-255</p>
<p>灰度图也可以表示成三通道，不过三通道上的值是相等的</p>
<p>像素值为0表示黑色，255表示白色</p>
<p>32位图像在24位的基础上增加了一个alpha分量，该分量用于记录图像的透明度信息。</p>
<h1 id="读、写、显示"><a href="#读、写、显示" class="headerlink" title="读、写、显示"></a>读、写、显示</h1><h2 id="图片读取"><a href="#图片读取" class="headerlink" title="图片读取"></a>图片读取</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="comment"># 读取函数imread()</span></span><br><span class="line">img = cv2.imread(<span class="string">&quot;Path.jpg/png&quot;</span>)</span><br><span class="line"><span class="comment"># 显示函数 imshow()</span></span><br><span class="line">cv2.imshow(<span class="string">&#x27;窗口名称&#x27;</span>,img)</span><br><span class="line"><span class="comment"># 写入函数 imwrite()</span></span><br><span class="line">cv2.imwrite(<span class="string">&#x27;写入的文件名.jpg&#x27;</span>,img)</span><br><span class="line"><span class="comment"># cv2.IMWRITE_JPEG_QUALITY指定jpg质量，范围0到100，默认95，越高画质越好，文件越大</span></span><br><span class="line">cv2.imwrite(<span class="string">&#x27;写入的文件名.jpg&#x27;</span>, img, (cv2.IMWRITE_JPEG_QUALITY, <span class="number">80</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># cv2.IMWRITE_PNG_COMPRESSION指定png质量，范围0到9，默认3，越高文件越小，画质越差</span></span><br><span class="line">cv2.imwrite(<span class="string">&#x27;写入的文件名.png&#x27;</span>, img, (cv2.IMWRITE_PNG_COMPRESSION, <span class="number">5</span>))</span><br></pre></td></tr></table></figure>

<h2 id="显示图片需要对窗口进行设置"><a href="#显示图片需要对窗口进行设置" class="headerlink" title="显示图片需要对窗口进行设置"></a>显示图片需要对窗口进行设置</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">cv2.imshow(<span class="string">&#x27;窗口名称&#x27;</span>,img)</span><br><span class="line">cv2.waitkey(<span class="number">0</span>)</span><br><span class="line">cv2.destoryAllWindows()</span><br></pre></td></tr></table></figure>

<p><code>cv2.waitKey()</code> 是键盘绑定函数，可以设置毫秒级数值，如果是0会一直等待，也可以设置为指定字母，比如a</p>
<p><code>cv2.destoryAllWindows()</code>销毁所有创建的窗口</p>
<h2 id="视频读取，显示"><a href="#视频读取，显示" class="headerlink" title="视频读取，显示"></a>视频读取，显示</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">cap = cv2.VideoCapture(<span class="string">&#x27;视频名称.mp4/avi&#x27;</span>) <span class="comment"># 获取视频</span></span><br><span class="line"><span class="comment"># cap = cv2.VideoCapture(0) # 获取摄像头 0内置 1外置摄像头</span></span><br><span class="line"><span class="comment"># 视频显示：while循环显示视频的每一帧</span></span><br><span class="line"><span class="comment"># 使用read()获取的视频帧，将每一帧显示100ms,如果期间检测到‘q‘则退出，关闭窗口</span></span><br><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">  reg,frame = cap.read()</span><br><span class="line">  cv2.imshow(<span class="string">&#x27;caputre&#x27;</span>,frame)</span><br><span class="line">  <span class="keyword">if</span> cv2.waitkey(<span class="number">100</span>) <span class="keyword">and</span> <span class="number">0xFF</span> == <span class="built_in">ord</span>(<span class="string">&#x27;q&#x27;</span>):</span><br><span class="line">    <span class="keyword">break</span></span><br></pre></td></tr></table></figure>

<h1 id="缩放，补边，裁剪"><a href="#缩放，补边，裁剪" class="headerlink" title="缩放，补边，裁剪"></a>缩放，补边，裁剪</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 缩放成200x200的方形图像</span></span><br><span class="line">img_200x200 = cv2.resize(img, (<span class="number">200</span>, <span class="number">200</span>))</span><br><span class="line"><span class="comment"># 根据缩放比例来</span></span><br><span class="line"><span class="comment"># 默认线性插值 cv2.INTER_LINEAR</span></span><br><span class="line">img = cv2.resize(img,(<span class="number">0</span>,<span class="number">0</span>),fx=<span class="number">0.5</span>,fy=<span class="number">0.5</span>,interpolation=cv2.INTER_NEAREST) <span class="comment"># 邻近插值</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 上下左右分布补边50，40，30，20宽，常数填充 都是黑边</span></span><br><span class="line">img = cv2.copyMakeBorder(img, <span class="number">50</span>, <span class="number">40</span>, <span class="number">30</span>, <span class="number">20</span>,cv2.BORDER_CONSTANT, value=(<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>,<span class="number">0</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 根据切片来做裁剪</span></span><br><span class="line">imgCropped = img[<span class="number">50</span>:<span class="number">250</span>,<span class="number">120</span>:<span class="number">330</span>]</span><br></pre></td></tr></table></figure>



<h1 id="绘制形状"><a href="#绘制形状" class="headerlink" title="绘制形状"></a>绘制形状</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cv2.line() 图像，起点，终点，画笔颜色，线宽</span></span><br><span class="line">img = cv2.line(img,(<span class="number">0</span>,<span class="number">0</span>),(<span class="number">511</span>,<span class="number">511</span>),(<span class="number">255</span>,<span class="number">0</span>,<span class="number">0</span>),<span class="number">5</span>)  </span><br><span class="line"><span class="comment"># cv2.circle() 图像，圆心，半径，颜色，-1是表示一个封闭的图形</span></span><br><span class="line">img = cv2.circle(img,(<span class="number">447</span>,<span class="number">63</span>),<span class="number">63</span>,(<span class="number">0</span>,<span class="number">0</span>,<span class="number">255</span>),-<span class="number">1</span>)  </span><br><span class="line"><span class="comment"># cv2.rectangle() 图像，左上角，右下角，颜色，线宽</span></span><br><span class="line">img = cv2.rectangle(img,(<span class="number">384</span>,<span class="number">0</span>),(<span class="number">510</span>,<span class="number">128</span>),(<span class="number">0</span>,<span class="number">255</span>,<span class="number">0</span>),<span class="number">3</span>) </span><br><span class="line"><span class="comment"># cv2.ellipse()</span></span><br><span class="line">img = cv2.ellipse(img,(<span class="number">256</span>,<span class="number">256</span>),(<span class="number">100</span>,<span class="number">50</span>),<span class="number">0</span>,<span class="number">0</span>,<span class="number">180</span>,<span class="number">255</span>,-<span class="number">1</span>) </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h1 id="书写文字"><a href="#书写文字" class="headerlink" title="书写文字"></a>书写文字</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cv2.putText()</span></span><br><span class="line"><span class="comment"># 各参数依次是：图片，添加的文字，左上角坐标，字体，字体大小，颜色，字体粗细</span></span><br><span class="line">img = cv2.putText(img, <span class="built_in">str</span>, (<span class="number">123</span>,<span class="number">456</span>)), font, <span class="number">2</span>, (<span class="number">0</span>,<span class="number">255</span>,<span class="number">0</span>), <span class="number">3</span>)</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>学习</category>
        <category>cv算法</category>
      </categories>
      <tags>
        <tag>OpenCV</tag>
        <tag>基础操作</tag>
        <tag>噪音</tag>
        <tag>编程</tag>
      </tags>
  </entry>
  <entry>
    <title>OpenCV图像处理基础（三）</title>
    <url>/2021/09/29/OpenCV%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E5%9F%BA%E7%A1%80%EF%BC%88%E4%B8%89%EF%BC%89/</url>
    <content><![CDATA[<h1 id="灰度图"><a href="#灰度图" class="headerlink" title="灰度图"></a>灰度图</h1><h1 id="灰度直方图等"><a href="#灰度直方图等" class="headerlink" title="灰度直方图等"></a>灰度直方图等</h1><h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>背景问题：</p>
<p>方法简述：</p>
<p>优点：</p>
<p>缺点：</p>
<span id="more"></span>
]]></content>
      <categories>
        <category>学习</category>
        <category>cv算法</category>
      </categories>
      <tags>
        <tag>OpenCV</tag>
        <tag>基础操作</tag>
        <tag>噪音</tag>
        <tag>编程</tag>
        <tag>灰度图</tag>
      </tags>
  </entry>
  <entry>
    <title>cv算法面试问题总结（一）</title>
    <url>/2021/09/29/cv%E7%AE%97%E6%B3%95%E9%9D%A2%E8%AF%95%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93%EF%BC%88%E4%B8%80%EF%BC%89/</url>
    <content><![CDATA[<h1 id="问题列表："><a href="#问题列表：" class="headerlink" title="问题列表："></a>问题列表：</h1><ol>
<li>CNN的特点和优势</li>
<li>decent 反卷积的作用</li>
<li>dropout 的作用和实现机制</li>
<li>深度学习中有哪些可加快收敛（降低训练难度）的方法</li>
<li>什么导致过拟合，如何防止过拟合</li>
<li>**LSTM防止梯度弥散和爆炸 **</li>
<li>Local Connected Conv</li>
<li>神经网络权值初始化的方式和区别</li>
<li>简述Convolution,pooling,和Normalization在卷积中的作用</li>
<li>Dilated conv(空洞卷积)优缺点和应用场景</li>
</ol>
<span id="more"></span>

<h1 id="常见问题及回答总结"><a href="#常见问题及回答总结" class="headerlink" title="常见问题及回答总结"></a>常见问题及回答总结</h1><h2 id="CNN的特点和优势"><a href="#CNN的特点和优势" class="headerlink" title="CNN的特点和优势"></a>CNN的特点和优势</h2><ol>
<li>改变全链接为局部连接，可以提取局部特征</li>
<li>权值共享，减少参数数量，降低训练难度（空间和时间都降低）</li>
<li>降维， 通过池化或者卷积stride实现</li>
<li>多层次结构：降低层次的局部特征组合为较高层次的特征。不同层次的特征应对不同任务</li>
<li>可以完全共享也可以局部共享 （比如眼睛鼻子嘴巴等位置样式固定的可以用和脸部不一样的卷积核）</li>
</ol>
<h2 id="decent的作用"><a href="#decent的作用" class="headerlink" title="decent的作用"></a>decent的作用</h2><ol>
<li>CNN可视化，将conv中得到的feature map还原到像素空间，观察特定的feature map对哪些图案比较敏感</li>
<li>Upsampling 上采样</li>
<li>Unsupervised learning 重构图像</li>
</ol>
<h2 id="dropout-的作用和实现机制"><a href="#dropout-的作用和实现机制" class="headerlink" title="dropout 的作用和实现机制"></a>dropout 的作用和实现机制</h2><ol>
<li>原理是 在深度学习网络训练中对于输入层和隐藏层的神经网络单元 按照一定的概率P（伯努利分布） 暂时性！！！的丢弃。对于随机梯度下降，由于是随机丢弃，所以对于每一个mini-batch都在训练不同的网络</li>
<li>作用是防止过拟合，提高效果</li>
<li>缺点是 收敛速度减慢，由于每一次迭代只有一部分参数更新，导致梯度下降的速度减慢</li>
<li>测试时，每个权重值需要乘概率p <a href="https://zhuanlan.zhihu.com/p/38200980">https://zhuanlan.zhihu.com/p/38200980</a> dropout 必读</li>
</ol>
<h2 id="深度学习中有哪些可加快收敛（降低训练难度）的方法"><a href="#深度学习中有哪些可加快收敛（降低训练难度）的方法" class="headerlink" title="深度学习中有哪些可加快收敛（降低训练难度）的方法"></a>深度学习中有哪些可加快收敛（降低训练难度）的方法</h2><ol>
<li>bottleneck瓶颈结构： 在计算比较大的卷积层的之前使用一个1<em>1的卷积来压缩大卷积层输入特征图的通道数，用来减少计算量。大卷积层完成之后按照实际情况，有时候需要1</em>1的卷积来将大卷积层的输出特征图的通道数复原</li>
<li>残差 （还不是很明白）</li>
<li>学习率，步长，动量</li>
<li>优化方法</li>
<li>预训练</li>
</ol>
<h2 id="什么导致过拟合，如何防止过拟合"><a href="#什么导致过拟合，如何防止过拟合" class="headerlink" title="什么导致过拟合，如何防止过拟合"></a>什么导致过拟合，如何防止过拟合</h2><ol>
<li>过拟合的原因： 样本量过小，样本抽取不均衡噪音过多，参数太多模型复杂度高，权值迭代次数足够多拟合了训练样本中的噪音和不具代表性的特征</li>
<li>防止方法：<ol>
<li>数据增强 data argumentation</li>
<li>early stop</li>
<li>dropout</li>
<li>Batch Normalization</li>
<li>使用更简单的模型</li>
<li>参数正则化： 通过一定方法使神经网络中的部分神经元关闭，降低模型的复杂程度。正则化就是为了减小测试误差的，虽然有的时候可能会以增大训练误差为代价。正则化是为了显著的减小方差而较小的增大偏差。也就是提升泛化能力</li>
<li>加噪音（输入数噪声，权重噪声，响应结果里面加噪声）</li>
<li>freeze预训练网络中的某几层 #代码 <a href="https://blog.csdn.net/weixin_41712499/article/details/111295683">https://blog.csdn.net/weixin_41712499/article/details/111295683</a></li>
<li>结合多个模型 <ol>
<li>Bagging: 可以将其理解为一个分段函数，使用不同的模型拟合不同部分的训练集。如随机森林就是训练了一堆互不关联的决策树</li>
<li>Boosting：使用多个模型最后将模型的输出加权平均</li>
</ol>
</li>
</ol>
</li>
</ol>
<h2 id="LSTM防止梯度弥散和爆炸"><a href="#LSTM防止梯度弥散和爆炸" class="headerlink" title="LSTM防止梯度弥散和爆炸"></a>LSTM防止梯度弥散和爆炸</h2><h2 id="Local-Connected-Conv"><a href="#Local-Connected-Conv" class="headerlink" title="Local Connected Conv"></a>Local Connected Conv</h2><ol>
<li>人脸在不同的区域存在不同的特征（眼睛／鼻子／嘴的分布位置相对固定），当不存在全局的局部特征分布时，Local-Conv更适合特征的提取</li>
</ol>
<h2 id="神经网络权值初始化的方式和区别"><a href="#神经网络权值初始化的方式和区别" class="headerlink" title="神经网络权值初始化的方式和区别"></a>神经网络权值初始化的方式和区别</h2><p>​    详见权值初始化blog</p>
<ol>
<li>常量初始化</li>
<li>随机高斯初始化： 将权重初始化为固定的均值和方差（例如均值取0，方差取0.01）如果初始的方差小，如0.1，就会导致在前向传播过程中，不同的层的输入不断减小。会导致权重的更新速度很慢很慢。初始化的方差如果太大，就会使得每一层的输出越来越大。形如tanh激活函数，就会容易导致梯度饱和的现象</li>
<li>均匀分布初始化</li>
<li>Xavier初始化</li>
<li>双线性初始化</li>
<li>msra初始化</li>
</ol>
<h2 id="简述Convolution-pooling-和Normalization在卷积中的作用"><a href="#简述Convolution-pooling-和Normalization在卷积中的作用" class="headerlink" title="简述Convolution,pooling,和Normalization在卷积中的作用"></a>简述Convolution,pooling,和Normalization在卷积中的作用</h2><ol>
<li>Convolution：通过卷积核运算，提取卷积核希望提取的特征</li>
<li>pooling层 减小图像大小，加速计算，使其检测出的特征更加健壮</li>
<li>fully connected： 用来做分类</li>
<li>激活层 使得函数更加复杂</li>
<li>Batch Normalization：<ol>
<li>背景：深度学习中数据分布在某一层开始有明显的偏移，网络加深会加剧，导致模型优化的难度增加，也就是梯度弥散。</li>
<li>方法：在每个卷积层之后重新调整数据分布，解决梯度问题。在网络每一层输入前，先做归一化处理，加一个归一化层。但并不是盲目加。变换重构，引入可学习的参数r,B, 从而控制归一化尽可能不影响特征提取</li>
<li>好处：降低对参数初始化的依赖。可使用更高的学习率，加速训练。一定程度上增加了泛化能力，可以取代dropout</li>
</ol>
</li>
</ol>
<h2 id="Dilated-conv-空洞卷积-优缺点和应用场景"><a href="#Dilated-conv-空洞卷积-优缺点和应用场景" class="headerlink" title="Dilated conv(空洞卷积)优缺点和应用场景"></a>Dilated conv(空洞卷积)优缺点和应用场景</h2><ol>
<li>背景： FCN全卷积的语义分割问题中，需要输入图像和输出图像的size保持一致。若使用池化层，会降低size之后需要上采样，导致特征信息丢失精度降低。若使用较小的卷积核，可保持size一致，但需要增大特征图通道数会导致计算量较大。</li>
<li>空洞卷积，在特征图上进行0填充扩大特征图size，既能保持感受野又能保持计算点不变</li>
</ol>
]]></content>
      <categories>
        <category>学习</category>
        <category>cv算法</category>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>面试</tag>
        <tag>总结</tag>
        <tag>技巧</tag>
      </tags>
  </entry>
  <entry>
    <title>cv算法面试问题总结（三）</title>
    <url>/2021/09/29/cv%E7%AE%97%E6%B3%95%E9%9D%A2%E8%AF%95%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93%EF%BC%88%E4%B8%89%EF%BC%89/</url>
    <content><![CDATA[<h1 id="问题列表"><a href="#问题列表" class="headerlink" title="问题列表"></a>问题列表</h1><ol>
<li>mAP计算</li>
<li>为什么要做特征归一化，标准化</li>
<li>常用的归一化和标准化方法</li>
<li>为什么线性回归使用mse作文损失函数</li>
<li>神经网络的深度和宽度分别是什么</li>
<li>下采样的作用？下采样的方法</li>
<li>上采样的原理和常用的方法</li>
<li>模型的FLOPs（计算量）是什么？怎么计算？</li>
<li>什么是深度可分离卷积？作用？</li>
<li>转置卷积的原理</li>
</ol>
<span id="more"></span>

<h1 id="问题及解答"><a href="#问题及解答" class="headerlink" title="问题及解答"></a>问题及解答</h1><h2 id="mAP计算"><a href="#mAP计算" class="headerlink" title="mAP计算"></a>mAP计算</h2><h2 id="为什么要做特征归一化，标准化"><a href="#为什么要做特征归一化，标准化" class="headerlink" title="为什么要做特征归一化，标准化"></a>为什么要做特征归一化，标准化</h2><h2 id="常用的归一化和标准化方法"><a href="#常用的归一化和标准化方法" class="headerlink" title="常用的归一化和标准化方法"></a>常用的归一化和标准化方法</h2><h2 id="为什么线性回归使用mse作文损失函数"><a href="#为什么线性回归使用mse作文损失函数" class="headerlink" title="为什么线性回归使用mse作文损失函数"></a>为什么线性回归使用mse作文损失函数</h2><h2 id="神经网络的深度和宽度分别是什么"><a href="#神经网络的深度和宽度分别是什么" class="headerlink" title="神经网络的深度和宽度分别是什么"></a>神经网络的深度和宽度分别是什么</h2><h2 id="下采样的作用？下采样的方法"><a href="#下采样的作用？下采样的方法" class="headerlink" title="下采样的作用？下采样的方法"></a>下采样的作用？下采样的方法</h2><h2 id="上采样的原理和常用的方法"><a href="#上采样的原理和常用的方法" class="headerlink" title="上采样的原理和常用的方法"></a>上采样的原理和常用的方法</h2><h2 id="模型的FLOPs（计算量）是什么？怎么计算？"><a href="#模型的FLOPs（计算量）是什么？怎么计算？" class="headerlink" title="模型的FLOPs（计算量）是什么？怎么计算？"></a>模型的FLOPs（计算量）是什么？怎么计算？</h2><h2 id="什么是深度可分离卷积？作用？"><a href="#什么是深度可分离卷积？作用？" class="headerlink" title="什么是深度可分离卷积？作用？"></a>什么是深度可分离卷积？作用？</h2><h2 id="转置卷积的原理"><a href="#转置卷积的原理" class="headerlink" title="转置卷积的原理"></a>转置卷积的原理</h2>]]></content>
      <categories>
        <category>学习</category>
        <category>cv算法</category>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>面试</tag>
        <tag>总结</tag>
        <tag>技巧</tag>
      </tags>
  </entry>
  <entry>
    <title>cv算法面试问题总结（二）</title>
    <url>/2021/09/29/cv%E7%AE%97%E6%B3%95%E9%9D%A2%E8%AF%95%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93%EF%BC%88%E4%BA%8C%EF%BC%89/</url>
    <content><![CDATA[<h1 id="问题列表"><a href="#问题列表" class="headerlink" title="问题列表"></a>问题列表</h1><ol>
<li>？？？ 判别模型和生成模型</li>
<li>如何决定算法是否收敛</li>
<li>正则的方法及特点</li>
<li>1*1卷积的作用</li>
<li>？？？ 无监督学习的方法有哪些</li>
<li>什么是感受野，增大感受野的方法</li>
<li>反卷积的棋盘效应和解决方法</li>
<li>神经网络参数量计算</li>
<li>？？？ 空洞卷积的原理和作用</li>
<li>？？？ 空洞卷积的感受野计算</li>
</ol>
<span id="more"></span>

<h1 id="问题及解答"><a href="#问题及解答" class="headerlink" title="问题及解答"></a>问题及解答</h1><h2 id="判别模型和生成模型"><a href="#判别模型和生成模型" class="headerlink" title="判别模型和生成模型"></a>判别模型和生成模型</h2><p>？？？</p>
<h2 id="如何决定算法是否收敛"><a href="#如何决定算法是否收敛" class="headerlink" title="如何决定算法是否收敛"></a>如何决定算法是否收敛</h2><ol>
<li><p>Loss 小于某个阈值</p>
</li>
<li><p>Loss趋于稳定，在某个之附近徘徊</p>
</li>
<li><p>迭代到一定的次数</p>
</li>
<li><p>看权值矩阵的变化，两次迭代权值变化很小</p>
</li>
</ol>
<h2 id="正则的方法及特点"><a href="#正则的方法及特点" class="headerlink" title="正则的方法及特点"></a>正则的方法及特点</h2><ol>
<li>正则的目的是防止过拟合，提高泛化能力</li>
<li>L1正则</li>
<li>L2正则</li>
<li>数据集扩增</li>
<li>dropout</li>
</ol>
<h2 id="1-1卷积的作用"><a href="#1-1卷积的作用" class="headerlink" title="1*1卷积的作用"></a>1*1卷积的作用</h2><ol>
<li>实现跨通道信息融合；不同通道上的一个线性组合，实际上就是加起来乘一个系数</li>
<li>feature map 通道数上的降维：<ol>
<li>假设输入特征维度是 100<em>100</em>128，卷积核的大小是5<em>5（stride=1，padding=2）通道数是256，经过卷积后输出的特征维度是（100-5+2</em>2）/1+1 = 100-》100<em>100</em>256 卷积参数量是128<em>5</em>5<em>256=819200 如果在5</em>5卷积之前使用一个64通道1<em>1的卷积，参数量是 128</em>1<em>1</em>64 + 64<em>5</em>5*256 = 417792</li>
</ol>
</li>
<li>增加非线性映射次数 1*1卷积之后会加一个非线性激活函数，使网络更深，也是网络更加具有判别信息的特征</li>
</ol>
<h2 id="无监督学习的方法有哪些"><a href="#无监督学习的方法有哪些" class="headerlink" title="无监督学习的方法有哪些"></a>无监督学习的方法有哪些</h2><h2 id="什么是感受野，增大感受野的方法"><a href="#什么是感受野，增大感受野的方法" class="headerlink" title="什么是感受野，增大感受野的方法"></a>什么是感受野，增大感受野的方法</h2><ol>
<li>感受野是卷积神经网络每一层输出的的feature map上的每个feature 在原图上映射的区域大小（原图：预处理resize，crop，wrap之后的图）</li>
<li>感受野越大，映射的原始图像的范围越广，可以蕴含更为全局，语义层次更高的信息</li>
<li>计算公式：一开始RF=1，然后RF= （RF-1）* stride + kernelsize （不管conv层还是pooling层，依次替换stride和kernelsize至最后）</li>
<li>增加感受野的方法：<ol>
<li>空洞卷积</li>
<li>增加pooling层，但会降低准确度 pooling会丢失信息</li>
<li>增大kernelsize， 会增加参数</li>
<li>增加卷积层的个数， 会面临梯度消失的问题。CPM中作者采用多阶段训练，并引入中间层监督来解决梯度消失的问题</li>
</ol>
</li>
</ol>
<h2 id="反卷积的棋盘效应和解决方法"><a href="#反卷积的棋盘效应和解决方法" class="headerlink" title="反卷积的棋盘效应和解决方法"></a>反卷积的棋盘效应和解决方法</h2><ol>
<li>反卷积是允许小图像中的点来绘制更大图像中的方块，很容易出现不均匀重叠，使得图像中的某个部分颜色比其他部分更深，也就是伪影。尤其是kernelsize不能被stride整除的时候。虽然网络可以通过学习权重来避免这种情况，但实践中很难完全避免</li>
<li>解决方法：<ol>
<li>修改反卷积形式<ol>
<li>确保反卷积核的大小可以被步长整除</li>
<li>网络末尾使用1*1的反卷积</li>
<li>调整卷积核权重</li>
</ol>
</li>
<li>修改上采样形式，采用插值方法代替反卷积进行上采样，如邻近差值和双线性差值</li>
<li>通过损失函数修正输出，在损失函数中加入total variation loss等损失函数，平滑输出图像，但图像边缘会模糊</li>
</ol>
</li>
</ol>
<h2 id="神经网络参数量计算"><a href="#神经网络参数量计算" class="headerlink" title="神经网络参数量计算"></a>神经网络参数量计算</h2><ol>
<li>带参数的层是：卷积层，BN层和全连接层。激活函数层，pooling层和Upsample层是没有参数的</li>
<li>卷积层参数计算：卷积核体积(Kw X Kh X Cin) X 卷积核个数Cout(也就是输出channel数) + 偏置项个数Cout (每个卷积核带有一个偏置项，但是不影响参数的数量级有时候会省略)</li>
<li>BN层的参数计算：2 X Cin(输入通道数) 有两个需要学习缩放因子和平移因子</li>
<li>FC层的参数计算：输入向量长度(和feature map相同size的一个卷积核)Ti X 输出向量长度To + 偏置量To  （全连接层逐渐被global average pooling取代 将最后一层的特征图 整张图进行一个均值化形成特征点，将这些特征点组合成最后的特征向量，通过softmax 进行计算）</li>
</ol>
<h2 id="空洞卷积的原理和作用"><a href="#空洞卷积的原理和作用" class="headerlink" title="空洞卷积的原理和作用"></a>空洞卷积的原理和作用</h2><h2 id="空洞卷积的感受野计算"><a href="#空洞卷积的感受野计算" class="headerlink" title="空洞卷积的感受野计算"></a>空洞卷积的感受野计算</h2>]]></content>
      <categories>
        <category>学习</category>
        <category>cv算法</category>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>面试</tag>
        <tag>总结</tag>
        <tag>技巧</tag>
      </tags>
  </entry>
  <entry>
    <title>cv算法面试问题总结（五）</title>
    <url>/2021/10/01/cv%E7%AE%97%E6%B3%95%E9%9D%A2%E8%AF%95%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93%EF%BC%88%E4%BA%94%EF%BC%89/</url>
    <content><![CDATA[<p>如何确定CNN的卷积核通道数和卷积输出层的通道数？<br>什么是卷积？<br>什么是CNN的池化pool层？<br>简述下什么是生成对抗网络<br>学梵高作画的原理是什么？<br>请简要介绍下tensorflow的计算图<br>你有哪些深度学习（rnn、cnn）调参的经验？<br>为什么不同的机器学习领域都可以使用CNN，CNN解决了这些领域的哪些共性问题？他是如何解决的？<br>LSTM结构推导，为什么比RNN好？<br>Sigmoid、Tanh、ReLu这三个激活函数有什么缺点或不足，有没改进的激活函数。<br>为什么引入非线性激励函数？<br>请问人工神经网络中为什么ReLu要好过于tanh和sigmoid function？<br>为什么LSTM模型中既存在sigmoid又存在tanh两种激活函数，而不是选择统一一种sigmoid或者tanh？这样做的目的是什么？<br>如何解决RNN梯度爆炸和弥散的问题？<br>什么样的数据集不适合用深度学习？<br>广义线性模型是怎被应用在深度学习中？<br>如何缓解梯度消失和梯度膨胀（微调、梯度截断、改良激活函数等）<br>简述神经网络的发展历史<br>深度学习常用方法<br>请简述神经网络的发展史。<br>神经网络中激活函数的真正意义？一个激活函数需要具有哪些必要的属性？还有哪些属性是好的属性但不必要的？<br>梯度下降法的神经网络容易收敛到局部最优，为什么应用广泛？<br>简单说说CNN常用的几个模型<br>为什么很多做人脸的Paper会最后加入一个Local Connected Conv？<br>什么是梯度爆炸？<br>梯度爆炸会引发什么问题？<br>如何确定是否出现梯度爆炸？<br>如何修复梯度爆炸问题？<br>LSTM神经网络输入输出究竟是怎样的？<br>什么是RNN？<br>请详细介绍一下RNN模型的几种经典结构<br>简单说下sigmoid激活函数<br>如何从RNN起步，一步一步通俗理解LSTM（全网最通俗的LSTM详解）<br>CNN究竟是怎样一步一步工作的？<br>rcnn、fast-rcnn和faster-rcnn三者的区别是什么<br>在神经网络中，有哪些办法防止过拟合？<br>CNN是什么，CNN关键的层有哪些？<br>GRU是什么？GRU对LSTM做了哪些改动？<br>如何解决深度学习中模型训练效果不佳的情况？<br>神经网络中，是否隐藏层如果具有足够数量的单位，它就可以近似任何连续函数？<br>为什么更深的网络更好？<br>更多的数据是否有利于更深的神经网络？<br>不平衡数据是否会影响神经网络的分类效果？<br>无监督降维提供的是帮助还是摧毁？<br>是否可以将任何非线性作为激活函数?<br>批大小如何影响测试正确率？<br>初始化如何影响训练?<br>不同层的权重是否以不同的速度收敛？<br>正则化如何影响权重？<br>什么是fine-tuning？<br>什么是边框回归Bounding-Box regression，以及为什么要做、怎么做<br>请阐述下Selective Search的主要思想<br>什么是非极大值抑制（NMS）？<br>什么是深度学习中的anchor？<br>CNN的特点以及优势<br>深度学习中有什么加快收敛/降低训练难度的方法？<br>请写出链式法则并证明<br>请写出Batch Normalization的计算方法及其应用<br>神经网络中会用到批量梯度下降（BGD）吗？为什么用随机梯度下降（SGD）?<br>当神经网络的调参效果不好时，从哪些角度思考？（不要首先归结于overfiting）<br>请阐述下卷积神经网络CNN的基本原理(全网最通俗版)<br>神经网络输出层为什么通常使用softmax?<br>了解无人驾驶的核心技术么？<br>如何形象的理解LSTM的三个门<br>通过一张张动图形象的理解LSTM<br>如何理解反向传播算法BackPropagation<br>请问什么是softmax函数？<br>通俗理解BN(Batch Normalization)<br>批量归一化BN到底解决了什么问题？<br>如何理解随机梯度下降，以及为什么SGD能够收敛？<br>模拟退火算法能解决陷入局部最优的问题么<br>请说下常见优化方法各自的优缺点（BGD、SGD、MBGD、Momentum、NAG、Adagrad、Adadelta、RMSprop、Adam）<br>Adam算法的原理机制是怎么样的？它与相关的AdaGrad和RMSProp方法有什么区别<br>什么是强化学习？<br>强化学习和监督学习、无监督学习的区别是什么？<br>强化学习适合解决什么样子的问题？<br>使用tensorflow进行深度学习算法实验时，如何调节超参数？<br>深度学习中的batch的大小对学习效果有何影响？<br>用梯度下降训练神经网络的参数，为什么参数有时会被训练为nan值？<br>卷积神经网络CNN中池化层有什么作用？<br>请列举几种常见的激活函数。激活函数有什么作用？<br>神经网络中Dropout的作用？具体是怎么实现的？<br>利用梯度下降法训练神经网络，发现模型loss不变，可能有哪些问题？怎么解决？<br>如何解决不平衡数据集的分类问题？<br>残差网络为什么能做到很深层？<br>相比sigmoid激活函数ReLU激活函数有什么优势？<br>卷积神经网络中空洞卷积的作用是什么？<br>适用于移动端部署的网络结构都有哪些？<br>深度学习模型参数初始化都有哪些方法？<br>卷积神经网络为什么会具有平移等不变性？</p>
<p>基于深度学习的目标检测技术演进：R-CNN、Fast R-CNN、Faster R-CNN、YOLO、SSD<br>请简单解释下目标检测中的这个IOU评价函数（intersection-over-union）<br>KNN与K-means区别？<br>K-means选择初始点的方法有哪些,优缺点是什么?(列出两种以上)<br>简述线性分类器的原理(要求对权重矩阵进行剖析)<br>请简述下log对数、Hinge Loss(折页)、Cross-Entropy Loss(交叉熵)这三个损失函数<br>简述正则化与奥卡姆剃刀原则<br>图像尺寸为 7<em>7, 卷积窗口大小为3</em>3, 步长为3, 是否能输出图像?如果能,输出图像大小为多少?如果不能,说明原因?<br>如果最后一个卷积层和第一个全连接层参数量太大怎么办?<br>为什么说神经网络是端到端的网络?<br>当参数量 &gt;&gt; 样本量时候, 神经网络是如何预防过拟合?<br>什么是感受野？<br>简述你对CBIR(Content-based Image Retrieval基于内容的图像检索)的理解<br>什么是计算机视觉单词模型？<br>简述什么是Local Feature(局部特征算子)？<br>KD-Tree相比KNN来进行快速图像特征比对的好处在哪里?<br>简述encode和decode思想<br>输入图片尺寸不匹配CNN网络input时候的解决方式？（三种以上）<br>FCN与CNN最大的区别？<br>遇到class-imbalanced data（数据类目不平衡）问题怎么办？<br>简述孪生随机网络（Siamese Network）<br>DPM（Deformable Parts Model）算法流程<br>什么是NMS（Non-maximum suppression 非极大值抑制）?<br>列举出常见的损失函数(三个以上)?<br>做过目标检测项目么？比如Mask R-CNN和Python做一个抢车位神器<br>如何理解Faster RCNN<br>one-stage和two-stage目标检测方法的区别和优缺点？<br>请画下YOLOv3的网络结构<br>请简单说下YOLOv1,v2,v3,v4各自的特点与发展史<br>如何理解YOLO：YOLO详解<br>怎么理解YOLOv4<br>神经网络参数共享(parameter sharing)是指什么？<br>2021年网易互联网 计算机视觉 一面<br>美团实习算法岗<br>2021商汤-视频理解研究员-校招-技术面</p>
<span id="more"></span>

]]></content>
      <categories>
        <category>学习</category>
        <category>cv算法</category>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>面试</tag>
        <tag>总结</tag>
        <tag>技巧</tag>
      </tags>
  </entry>
  <entry>
    <title>cv算法面试问题总结（六）</title>
    <url>/2021/10/01/cv%E7%AE%97%E6%B3%95%E9%9D%A2%E8%AF%95%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93%EF%BC%88%E5%85%AD%EF%BC%89/</url>
    <content><![CDATA[]]></content>
      <categories>
        <category>学习</category>
        <category>cv算法</category>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>面试</tag>
        <tag>总结</tag>
        <tag>技巧</tag>
      </tags>
  </entry>
  <entry>
    <title>cv算法面试问题总结（四）</title>
    <url>/2021/10/01/cv%E7%AE%97%E6%B3%95%E9%9D%A2%E8%AF%95%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93%EF%BC%88%E5%9B%9B%EF%BC%89/</url>
    <content><![CDATA[<h1 id="问题列表"><a href="#问题列表" class="headerlink" title="问题列表"></a>问题列表</h1><ol>
<li><p>神经网络中的Addition/concatenate区别</p>
</li>
<li><p>目标检测中的anchor机制？作用？</p>
</li>
<li><p>BN（Batch Normalization）的原理和作用</p>
</li>
<li><p>随机梯度下降相比全局梯度下降的好处</p>
</li>
<li><p>网络初始化时给网络赋予0的权重，这个网络能正常训练吗？</p>
</li>
<li><p>梯度消失和梯度爆炸的原因？</p>
</li>
<li><p>深度学习为什么在计算机视觉领域这么好？</p>
</li>
<li><p>什么是正则化？L1正则化和L2正则化有什么区别</p>
</li>
<li><p>常用的模型压缩方式有哪些</p>
</li>
<li><p>残差网络的设计思想和作用</p>
<span id="more"></span>

<h1 id><a href="#" class="headerlink" title></a></h1></li>
</ol>
]]></content>
      <categories>
        <category>学习</category>
        <category>cv算法</category>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>面试</tag>
        <tag>总结</tag>
        <tag>技巧</tag>
      </tags>
  </entry>
  <entry>
    <title>hello-world</title>
    <url>/2021/09/29/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>
]]></content>
  </entry>
  <entry>
    <title>kaggle比赛-Molecular Translation</title>
    <url>/2021/10/03/kaggle%E6%AF%94%E8%B5%9B-Molecular-Translation/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>kaggle比赛-NFL Helmet Assignment</title>
    <url>/2021/10/03/kaggle%E6%AF%94%E8%B5%9B-NFL-Helmet-Assignment/</url>
    <content><![CDATA[<p><a href="https://www.kaggle.com/c/nfl-health-and-safety-helmet-assignment">https://www.kaggle.com/c/nfl-health-and-safety-helmet-assignment</a></p>
]]></content>
      <categories>
        <category>学习</category>
        <category>cv算法</category>
        <category>深度学习</category>
        <category>项目比赛</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>Kaggle</tag>
      </tags>
  </entry>
  <entry>
    <title>kaggle比赛-脑肿瘤检测</title>
    <url>/2021/09/03/kaggle%E6%AF%94%E8%B5%9B-%E8%84%91%E8%82%BF%E7%98%A4%E6%A3%80%E6%B5%8B/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>数据集简介：脑肿瘤图片</p>
<p>算法简介：</p>
<p>Version1.0: EfficientDet 3D</p>
<p>Version2.0: EfficientDet 3D + ResNet 101</p>
<p>预测结果： 当前排名500/1300</p>
<p>Version1.0  Score: 0.642 Rank: 500/1400</p>
]]></content>
      <categories>
        <category>学习</category>
        <category>cv算法</category>
        <category>深度学习</category>
        <category>项目比赛</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>Kaggle</tag>
      </tags>
  </entry>
  <entry>
    <title>图像分类-EfficientNet&amp;EfficientDet[项目应用]</title>
    <url>/2021/09/29/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB-EfficientNet/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>背景问题：</p>
<p>方法简述：</p>
<p>优点：</p>
<p>缺点：</p>
<span id="more"></span>
]]></content>
      <categories>
        <category>学习</category>
        <category>cv算法</category>
        <category>深度学习</category>
        <category>图像分类</category>
        <category>目标检测</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>论文复现</tag>
        <tag>项目总结</tag>
        <tag>kaggle</tag>
      </tags>
  </entry>
  <entry>
    <title>图像分割-Unet[项目应用]</title>
    <url>/2021/09/29/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2-Unet/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>背景问题：</p>
<p>方法简述：</p>
<p>优点：</p>
<p>缺点：</p>
<span id="more"></span>
]]></content>
      <categories>
        <category>学习</category>
        <category>cv算法</category>
        <category>深度学习</category>
        <category>图像分割</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>论文复现</tag>
        <tag>项目总结</tag>
      </tags>
  </entry>
  <entry>
    <title>图像识别-AlexNet</title>
    <url>/2021/09/29/%E5%9B%BE%E5%83%8F%E8%AF%86%E5%88%AB-AlexNet/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>背景问题：</p>
<p>方法简述：</p>
<p>优点：</p>
<p>缺点：</p>
<span id="more"></span>
]]></content>
      <categories>
        <category>学习</category>
        <category>cv算法</category>
        <category>深度学习</category>
        <category>图像识别</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>论文复现</tag>
      </tags>
  </entry>
  <entry>
    <title>图像处理基础-滤波器</title>
    <url>/2021/09/29/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E5%9F%BA%E7%A1%80-%E6%BB%A4%E6%B3%A2%E5%99%A8/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>在图像处理中，经常需要对图像进行平滑去噪、锐化、边界增强等，这些功能可以通过滤波器filter来实现。</p>
<p>噪音的分类及如何添加</p>
<p>椒盐噪声：是一种随机出现的白点(salt)或者黑点(peppe)</p>
<p>高斯噪声：概率密度函数服从高斯分布（即正态分布）的一类噪声</p>
<p>常用滤波算法：</p>
<p>均值滤波器：把邻域内的平均值赋给中心元素。邻域内的像素权重是相等的；但不能保护细节，不能消除椒盐噪声</p>
<p>高斯滤波器：邻域内各个像素值不同权重的和，将中心点的权重增大，远离中心的的权重减小</p>
<p>中值滤波器：不使用权重，邻域内所有像素值的中间值来代替当前像素点的像素值</p>
<p>双边滤波器：同时考虑距离信息（距离越远，权重越小)和色彩信息（色彩差别越大，权重越小）</p>
<span id="more"></span>

<h1 id="滤波相关概念："><a href="#滤波相关概念：" class="headerlink" title="滤波相关概念："></a>滤波相关概念：</h1><p>图像的时域： 自变量是时间,即横轴是时间,纵轴是信号的变化。其动态信号x（t）是描述信号在不同时刻取值的函数</p>
<p>图像的频域：自变量是频率,即横轴是频率,纵轴是该频率信号的幅度,也就是通常说的频谱图。频谱图描述了信号的频率结构及频率与该频率信号幅度的关系</p>
<p>图像的频率： 图像的频率又称为空间频率，它反映了图像的像素灰度在空间中变化的情况</p>
<p>如何定量的测量图像的空间频率，最为常用的方法就是二维傅里叶变换。图像经过二维傅里叶变换后会形成与图像等大的复数矩阵，取其幅值形成幅度谱，取其相位形成相位谱。图像的频率能量分布主要体现在幅度谱中。通常习惯将低频成分放在幅度谱的中央，而将高频成分放在幅度谱边缘。</p>
<p>在图像频域里面，频率低的地方是比较平滑的，低频的区域中灰度值变化是比较小的；频率高的地方通常是边缘或者噪声，这些区域灰度值是突变的</p>
<p>高通滤波：让频率较高的部分通过，突出边缘等</p>
<p>低通滤波：保留频率比较低的部分，通常为平滑图像，弱化边缘，消除噪声</p>
<p>在时域中的滤波器和在频域中的滤波器组成了傅里叶变换对，这部分暂时不深入了。</p>
<h2 id="滤波器分类"><a href="#滤波器分类" class="headerlink" title="滤波器分类"></a>滤波器分类</h2><p>线性滤波： 对邻域中的像素的计算为线性运算时，如利用窗口函数进行平滑加权求和的运算，或者某种卷积运算，都可以称为线性滤波。常见的线性滤波有：方框滤波、均值滤波、高斯滤波、拉普拉斯滤波等等，通常线性滤波器之间只是模版的系数不同。</p>
<p>非线性滤波： 非线性滤波利用原始图像跟模版之间的一种逻辑关系得到结果，如最值滤波器，中值滤波器。比较常用的有中值滤波器和双边滤波器。</p>
<p>reference：<a href="https://blog.csdn.net/qq_44957388/article/details/105763906">https://blog.csdn.net/qq_44957388/article/details/105763906</a></p>
<h1 id="常见的滤波器："><a href="#常见的滤波器：" class="headerlink" title="常见的滤波器："></a>常见的滤波器：</h1><h2 id="均值滤波器"><a href="#均值滤波器" class="headerlink" title="均值滤波器"></a>均值滤波器</h2><p><img src="/2021/09/29/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E5%9F%BA%E7%A1%80-%E6%BB%A4%E6%B3%A2%E5%99%A8/%E5%9D%87%E5%80%BC.png"></p>
<h3 id="均值滤波的缺点"><a href="#均值滤波的缺点" class="headerlink" title="均值滤波的缺点"></a>均值滤波的缺点</h3><p>均值滤波本身存在着固有的缺陷，即它不能很好地保护图像细节，在图像去噪的同时也破坏了图像的细节部分，从而使图像变得模糊，不能很好地去除噪声点。特别是椒盐噪声。</p>
<h3 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h3><p>均值模糊可以模糊图像以便得到感兴趣物体的粗略描述，也就是说，去除图像中的不相关细节，其中“不相关”是指与滤波器模板尺寸相比较小的像素区域，从而对图像有一个整体的认知。即为了对感兴趣的物体得到一个大致的整体的描述而模糊一幅图像，忽略细小的细节。</p>
<h2 id="高斯滤波器"><a href="#高斯滤波器" class="headerlink" title="高斯滤波器"></a>高斯滤波器</h2><p><img src="/2021/09/29/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E5%9F%BA%E7%A1%80-%E6%BB%A4%E6%B3%A2%E5%99%A8/%E9%AB%98%E6%96%AF.png"></p>
<p><strong>应用：</strong> 高斯滤波是一种线性平滑滤波器，对于服从正态分布的噪声有很好的抑制作用。在实际场景中，我们通常会假定图像包含的噪声为高斯白噪声，所以在许多实际应用的预处理部分，都会采用高斯滤波抑制噪声，如传统车牌识别等。</p>
<h2 id="中值滤波器"><a href="#中值滤波器" class="headerlink" title="中值滤波器"></a>中值滤波器</h2><p><img src="/2021/09/29/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E5%9F%BA%E7%A1%80-%E6%BB%A4%E6%B3%A2%E5%99%A8/%E4%B8%AD%E5%80%BC.png"></p>
<p>中值滤波不再采用加权求和的方式计算滤波结果，它用邻域内所有像素值的中间值来代替当前像素点的像素值。</p>
<p>中值滤波会取当前像素点及其周围临近像素点的像素值，一般有奇数个像素点，将这些像素值排序，将排序后位于中间位置的像素值作为当前像素点的像素值。</p>
<p>中值滤波对于斑点噪声（speckle noise）和椒盐噪声（salt-and-pepper<br>noise）来说尤其有用，因为它不依赖于邻域内那些与典型值差别很大的值，而且噪声成分很难被选上，所以可以在几乎不影响原有图像的情况下去除全部噪声。但是由于需要进行排序操作，中值滤波的计算量较大。</p>
<p>中值滤波器在处理连续图像窗函数时与线性滤波器的工作方式类似，但滤波过程却不再是加权运算。</p>
<h2 id="双边滤波器"><a href="#双边滤波器" class="headerlink" title="双边滤波器"></a>双边滤波器</h2><p>双边滤波是综合考虑空间信息和色彩信息的滤波方式，在滤波过程中能有效的保护图像内的边缘信息。</p>
<p>双边滤波在计算某一个像素点的像素值时，同时考虑距离信息（距离越远，权重越小)和色彩信息（色彩差别越大，权重越小）。既能去除噪声，又能较好的保护边缘信息。</p>
<p><img src="/2021/09/29/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E5%9F%BA%E7%A1%80-%E6%BB%A4%E6%B3%A2%E5%99%A8/%E5%8F%8C%E8%BE%B9.png"></p>
<p>在双边滤波中，计算左侧白色区域的滤波结果时：</p>
<p>对于白色的点，权重较大<br>对于黑色的点，与白色的色彩差别较大（0和255），所以可以将他们的权重设置为0<br>计算右侧黑色区域的滤波结果时：</p>
<p>对于黑色的点，权重较大<br>对于白色的点，与黑色的色彩差别较大（255和0），所以可以将他们的权重设置为0<br>这样，左侧白色的滤波结果仍是白色，黑色的像素点权重为0，对它不会有影响；右侧黑色的滤波结果仍是黑色，白色的像素点权重为0，对它不会有影响。所以，双边滤波会将边缘信息保留。</p>
<p>参考链接：<a href="https://blog.csdn.net/qq_44957388/article/details/105763906">https://blog.csdn.net/qq_44957388/article/details/105763906</a></p>
]]></content>
      <categories>
        <category>学习</category>
        <category>cv算法</category>
      </categories>
      <tags>
        <tag>OpenCV</tag>
        <tag>基础操作</tag>
        <tag>噪音</tag>
        <tag>编程</tag>
        <tag>滤波器</tag>
      </tags>
  </entry>
  <entry>
    <title>图像识别-ResNet</title>
    <url>/2021/09/29/%E5%9B%BE%E5%83%8F%E8%AF%86%E5%88%AB-ResNet/</url>
    <content><![CDATA[<h1 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a>ResNet</h1><h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>背景问题：</p>
<p>方法简述：</p>
<p>优点：</p>
<p>缺点：</p>
<span id="more"></span>
]]></content>
      <categories>
        <category>学习</category>
        <category>cv算法</category>
        <category>深度学习</category>
        <category>图像识别</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>论文复现</tag>
      </tags>
  </entry>
  <entry>
    <title>图像识别-GoogLeNet</title>
    <url>/2021/09/29/%E5%9B%BE%E5%83%8F%E8%AF%86%E5%88%AB-GoogLeNet/</url>
    <content><![CDATA[<h1 id="GoogleNet"><a href="#GoogleNet" class="headerlink" title="GoogleNet"></a>GoogleNet</h1><h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>背景问题：</p>
<p>方法简述：</p>
<p>优点：</p>
<p>缺点：</p>
<span id="more"></span>
]]></content>
      <categories>
        <category>学习</category>
        <category>cv算法</category>
        <category>深度学习</category>
        <category>图像识别</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>论文复现</tag>
      </tags>
  </entry>
  <entry>
    <title>图像识别-VGGNet</title>
    <url>/2021/09/29/%E5%9B%BE%E5%83%8F%E8%AF%86%E5%88%AB-VGGNet/</url>
    <content><![CDATA[<h1 id="VGGNet"><a href="#VGGNet" class="headerlink" title="VGGNet"></a>VGGNet</h1><h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>背景问题：</p>
<p>方法简述：</p>
<p>优点：</p>
<p>缺点：</p>
<span id="more"></span>
]]></content>
      <categories>
        <category>学习</category>
        <category>cv算法</category>
        <category>深度学习</category>
        <category>图像识别</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>论文复现</tag>
      </tags>
  </entry>
  <entry>
    <title>常见的算法面试（一）</title>
    <url>/2021/10/03/%E5%B8%B8%E8%A7%81%E7%9A%84%E7%AE%97%E6%B3%95%E9%9D%A2%E8%AF%95%EF%BC%88%E4%B8%80%EF%BC%89/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>新文章</title>
    <url>/2021/09/29/%E6%96%B0%E6%96%87%E7%AB%A0-1/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>背景问题：</p>
<p>方法简述：</p>
<p>优点：</p>
<p>缺点：</p>
<span id="more"></span>
]]></content>
      <tags>
        <tag>编程</tag>
      </tags>
  </entry>
  <entry>
    <title>新文章</title>
    <url>/2021/09/29/%E6%96%B0%E6%96%87%E7%AB%A0/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>背景问题：</p>
<p>方法简述：</p>
<p>优点：</p>
<p>缺点：</p>
<span id="more"></span>
]]></content>
      <categories>
        <category>空文档</category>
      </categories>
  </entry>
  <entry>
    <title>机器学习-K-means算法</title>
    <url>/2021/10/01/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-K-means%E7%AE%97%E6%B3%95/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>背景问题：</p>
<p>方法简述：</p>
<p>优点：</p>
<p>缺点：</p>
<span id="more"></span>

]]></content>
      <categories>
        <category>学习</category>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习-主成分分析PCA</title>
    <url>/2021/10/01/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90PCA/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>背景问题：</p>
<p>方法简述：</p>
<p>优点：</p>
<p>缺点：</p>
<span id="more"></span>

]]></content>
      <categories>
        <category>学习</category>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习-KNN邻近算法</title>
    <url>/2021/10/01/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-KNN%E9%82%BB%E8%BF%91%E7%AE%97%E6%B3%95/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>背景问题：</p>
<p>方法简述：</p>
<p>优点：</p>
<p>缺点：</p>
<span id="more"></span>

]]></content>
      <categories>
        <category>学习</category>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title>【未完成】机器学习-优化算法：牛顿法和拟牛顿法</title>
    <url>/2021/10/02/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95%EF%BC%9A%E7%89%9B%E9%A1%BF%E6%B3%95%E5%92%8C%E6%8B%9F%E7%89%9B%E9%A1%BF%E6%B3%95/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>牛顿法（Newton method）和拟牛顿法（quasi Newton method）是求解无约束最优化问题的常用方法，有收敛速度快的优点。牛顿法是迭代算法，每一步都需求解目标函数的海塞矩阵（Hessian Matrix），计算比较复杂。拟牛顿法通过正定矩阵近似海塞矩阵的逆矩阵或海塞矩阵，简化了这一计算过程。</p>
<p>方法：</p>
<p>牛顿法：是使用函数F(x)的泰勒级数的前面几项来寻找方程F(x)=0的根。</p>
<p>拟牛顿法：牛顿法虽然收敛速度快，但是需要计算海塞矩阵的逆矩阵 ，而且有时目标函数的海塞矩阵无法保持正定，从而使得牛顿法失效。为了克服这两个问题，人们提出了拟牛顿法。这个方法的基本思想是：不用二阶偏导数而构造出可以近似海塞矩阵（或海塞矩阵的逆)的正定对称阵。不同的构造方法就产生了不同的拟牛顿法。</p>
<p>优点：</p>
<p>缺点：</p>
]]></content>
      <categories>
        <category>学习</category>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>优化</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习-回归树Regression-Tree</title>
    <url>/2021/10/02/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%9B%9E%E5%BD%92%E6%A0%91Regression-Tree/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>背景问题：决策树是解决分类问题的主要方法，分类是离散问题，但回归是连续问题，</p>
<p>方法简述： CART（Classification and Regression Tree）可以用来做回归，在分类问题中CART只用基尼系数作为特征选择和划分的依据；在回归问题中CART使用MSE(Mean Square Error)或者MAE(Mean Absolute Error)作为特征选择和划分的依据。每个叶子代表一个预测值，取值是连续的。</p>
<p>优点：</p>
<p>训练速度和预测速度较快；<br>善于获取数据集中的非线性关系；<br>了解数据集中的特征交互；<br>善于处理数据集中出现的异常值；<br>善于在数据集中找到最重要的特征；<br>不需要特征缩放；<br>结果可解释，并易于说明；</p>
<p>缺点：</p>
<p>预测精确度较低；<br>需要一些参数的调整；<br>不适用于小型数据集；<br>分离信号和噪声的效果不理想；<br>当新增数据时，不易更新模型；<br>在实践中很少使用，而是更多地使用集合树；<br>可能会出现过度拟合</p>
<span id="more"></span>

<p>参考：<a href="https://zhuanlan.zhihu.com/p/82054400">https://zhuanlan.zhihu.com/p/82054400</a></p>
]]></content>
      <categories>
        <category>学习</category>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习-决策树Decision Tree</title>
    <url>/2021/09/30/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%86%B3%E7%AD%96%E6%A0%91Decision-Tree/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>背景问题：根据属性值做if … else …划分。问题在于先行选择哪个属性做划分是最优的？用什么标准来定量的去做这个选择？</p>
<p>方法简述：</p>
<p>ID3 利用信息增益来选择特征的。信息增益最大的特征来建立决策树的当前节点</p>
<p>C4.5 是根据“信息增益比”指标来做特征选择</p>
<p>CART(Classification and Regression Tree) 使用基尼系数</p>
<p>优点：</p>
<p>缺点：</p>
<p>决策树的回归用法</p>
<span id="more"></span>

<h1 id="ID3-决策算法"><a href="#ID3-决策算法" class="headerlink" title="ID3 决策算法"></a>ID3 决策算法</h1><h1 id="C4-5-决策算法"><a href="#C4-5-决策算法" class="headerlink" title="C4.5 决策算法"></a>C4.5 决策算法</h1><p>该算法解决了ID3算法中的一下问题：</p>
<ol>
<li><p>不能处理连续特征，</p>
<ol start="2">
<li>用信息增益作为标准容易偏向于取值较多的特征</li>
<li>缺失值处理的问</li>
<li>过拟合问题。</li>
</ol>
</li>
</ol>
<h1 id="CART算法"><a href="#CART算法" class="headerlink" title="CART算法"></a>CART算法</h1><p>该算法解决的是C4.5算法中的一下问题：</p>
<ol>
<li><p>由于决策树算法非常容易过拟合，因此对于生成的决策树必须要进行剪枝。</p>
</li>
<li><p>C4.5生成的是多叉树，即一个父节点可以有多个节点。很多时候，在计算机中二叉树模型会比多叉树运算效率高。如果采用二叉树，可以提高效率。</p>
</li>
<li><p>C4.5只能用于分类，如果能将决策树用于回归的话可以扩大它的使用范围。</p>
</li>
<li><p>C4.5由于使用了熵模型，里面有大量的耗时的对数运算,如果是连续值还有大量的排序运算。</p>
</li>
</ol>
]]></content>
      <categories>
        <category>学习</category>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习-奇异值分解SVD</title>
    <url>/2021/10/01/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%A5%87%E5%BC%82%E5%80%BC%E5%88%86%E8%A7%A3SVD/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>背景问题：</p>
<p>方法简述：</p>
<p>优点：</p>
<p>缺点：</p>
<span id="more"></span>

]]></content>
      <categories>
        <category>学习</category>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习-支持向量机(SVM)</title>
    <url>/2021/09/30/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA-SVM/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>背景问题：逻辑回归是找到可以划分数据的超平面，但是如何找到最优的超平面来划分数据集呢？</p>
<p>方法简述：</p>
<p>线性支持向量机：</p>
<p>非线性支持向量机：</p>
<p>核函数：</p>
<p>软间隔支持向量机：</p>
<p>优点：</p>
<p>​    小规模数据集训练，比LR和随机森林准确率高，泛化能力强。</p>
<p>​    在非线性特征空间中效果较好，有大量的核函数可以使用来解决非线性分类问题</p>
<p>​    在高维度特征的分类问题和回归问题很有效，即便是特征维度大于样本量的时候</p>
<p>​    不需要依赖全部样本，仅仅使用一部分样本做支持向量来完成超平面决策</p>
<p>​    无局部极小值问题；（相对于神经网络等算法）</p>
<p>缺点：</p>
<p>​    SVM不能产生分类的概率值，</p>
<p>​    SVM对大规模训练数据集是不适用的，计算量十分复杂</p>
<p>​    解决非线性问题时，找到一个合适的核函数是比较困难的</p>
<p>​    多分类问题支持不友好</p>
<p>​    对缺失数据敏感</p>
<p>​    ？？？对于核函数的高维映射解释力不强，尤其是径向基函数；</p>
<p>应用：</p>
<p>文本分类领域效果最好的机器学习算法，在工业界主要应用在网页分类、微博情感分析、舆情监控、用户评论挖掘、文本过滤等诸多领域</p>
<span id="more"></span>
]]></content>
      <categories>
        <category>学习</category>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习-线性回归Linear Regression</title>
    <url>/2021/09/30/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92Linear-Regression/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>背景问题：找到一系列的参数W，使得f(x) = XW 和真实输出Y之间无限接近或一致。</p>
<p>方法简述：</p>
<p>优点：直接，快速，可解释性高</p>
<p>缺点：基于一系列假设；对异常值敏感；对数据分布敏感；存在多重共线性，自相关，异方差问题；容易出现过拟合与欠拟合问题；</p>
<span id="more"></span>

<h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><p>什么是回归模型？</p>
<p>回归是用来拟合输入变量和输出变量之间的关系，回归模型就是表示从输入变量到输出变量的映射函数。</p>
<p>所以线性回归的目标就是找到一系列的参数W，使得f(x) = XW 和真实输出Y之间无限接近或一致。</p>
<h1 id="理论"><a href="#理论" class="headerlink" title="理论"></a>理论</h1><h1 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h1><h1 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h1><h2 id="过拟合问题"><a href="#过拟合问题" class="headerlink" title="过拟合问题"></a>过拟合问题</h2><p>​    分析数据，重新做数据清冼，将征工程。</p>
<p>​    扩充数据集，收集更多数据。</p>
<p>​    减少特征数量 。</p>
<p>​    **采用正则化方法</p>
<h2 id="欠拟合问题"><a href="#欠拟合问题" class="headerlink" title="欠拟合问题 **"></a>欠拟合问题 **</h2><p>​    分析数据，增加特征维度</p>
<p>​    ** 增加多项式特征阶数</p>
<p>​    ** 减小正则项的超参数系数</p>
<p>​    ** 局部加权回归 </p>
<h2 id="多重共线性"><a href="#多重共线性" class="headerlink" title="多重共线性"></a>多重共线性</h2><p>​    那共线性会引发什么问题。。。。：</p>
<p>1、模型参数估计不准确，有时甚至会出现回归系数的符号与实际情况完全相反的情况，比如逻辑上应该系数为正的特征系数 算出来为负。</p>
<p>2、本应该显著的自变量不显著，本不显著的自变量却呈现出显著性（也就是说，无法从p-值的大小判断出变量是否显著——下面会给一个例子）</p>
<p>3、多重共线性使参数估计值的方差增大，模型参数不稳定，也就是每次训练得到的权重系数差异都比较大。</p>
<p>其实多重共线性这样理解会简单很多:</p>
<p>假设原始的线性回归公式为：</p>
<p>y=w1<em>x1+w2</em>x2+w3*x3</p>
<p>训练完毕的线性回归公式为：</p>
<p>y=5x1+7x2+10x3,</p>
<p>此时加入一个新特征x4，假设x4和x3高度相关，x4=2x3,则</p>
<p>y=w1<em>x1+w2</em>x2+w3<em>x3+w4</em>x4=w1<em>x1+w2</em>x2+(w3+2w4)*x3</p>
<p>因为我们之前拟合出来的最优的回归方程为：</p>
<p>y=5x1+7x2+10x3</p>
<p>显然w3+2w4可以合并成一个新的权重稀疏 w5，则</p>
<p>y=w1<em>x1+w2</em>x2+w5*x3,显然：</p>
<p>y=w1<em>x1+w2</em>x2+w3<em>x3和y=w1</em>x1+w2<em>x2+w5</em>x3是等价的。。。。</p>
<p>那么最终最优的模型应该也是 y=5x1+7x2+10x3</p>
<p>但是考虑到引入了x4，所以w4和w3的权重是分开计算出来的，这就导致了</p>
<p>w5=10=w3+2w4，显然这个方程有无穷多的解，比如w3=4，w4=3，或者w4=-1，w3=12等，因此导致了模型系数估计的不稳定并且可能会出现负系数的问题。</p>
<h1 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h1><h2 id="基于-Python-scikit-learn-工具包"><a href="#基于-Python-scikit-learn-工具包" class="headerlink" title="基于 Python scikit-learn 工具包"></a>基于 Python scikit-learn 工具包</h2><h2 id="Python自建实现"><a href="#Python自建实现" class="headerlink" title="Python自建实现"></a>Python自建实现</h2>]]></content>
      <categories>
        <category>学习</category>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习-朴素贝叶斯</title>
    <url>/2021/10/01/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>背景问题：</p>
<p>方法简述：</p>
<p>优点：</p>
<p>缺点：</p>
<span id="more"></span>

]]></content>
      <categories>
        <category>学习</category>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习-逻辑回归Logistic Regression</title>
    <url>/2021/09/30/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92Logistic-Regression/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>逻辑回归 = 线性回归+逻辑分布 （如：sigmoid函数）</p>
<p>背景问题：</p>
<p>回归是连续的，分类是离散的，怎么将解决分类问题转化为解决回归问题</p>
<p>方法简述：先拟合决策边界(不局限于线性，还可以是多项式)，再建立这个边界与分类的概率联系，从而得到了二分类情况下的概率。分类是离散的，但类别的概率是连续的，让模型拟合概率相关的一个指标（对数几率函数logit）</p>
<p>优点：</p>
<p>​    (1)对率函数任意阶可导，具有很好的数学性质，许多现有的数值优化算法都可以用来求最优解，训练速度快;</p>
<p>​    (2)简单易理解，模型的可解释性非常好，从特征的权重可以看到不同的特征对最后结果的影响;</p>
<p>​    (3)适合二分类问题，不需要缩放输入特征</p>
<p>​    (4)内存资源占用小，因为只需要存储各个维度的特征值;</p>
<p>​    (5)直接对分类可能性进行建模，无需事先假设数据分布，避免了假设分布不准确所带来的问题</p>
<p>​    (6)以概率的形式输出，对许多利用概率辅助决策的任务很有用</p>
<p>缺点：</p>
<p>​    (1)不能用于解决非线性问题</p>
<p>​    (2)对多重共线性数据较为敏感;</p>
<p>​    (3)很难处理数据不平衡的问题;</p>
<p>​    (4)准确率并不是很高，因为形式非常的简单(非常类似线性模型)，很难去拟合数据的真实分布;</p>
<p>​    (5)无法筛选特征</p>
<span id="more"></span>

<p>线性分类器：线性分类器的学习目标便是要在n维的数据空间中找到一个超平面，使得这个超平面可以将已知的数据点分为两个类别</p>
<p>逻辑回归不是解决回归问题是用来解决分类问题，本质就是假设数据符合这个分布，然后使用极大似然估计做参数的估计</p>
<h1 id="Logistic-分布函数"><a href="#Logistic-分布函数" class="headerlink" title="Logistic 分布函数"></a>Logistic 分布函数</h1><p>Logistic 分布是一种连续型的分布，它形状与正态分布的形状相似，但是 Logistic 分布的尾部更长，所以我们可以使用 Logistic 分布来建模比正态分布具有更长尾部和更高波峰的数据分布。在深度学习中常用到的 Sigmoid 函数就是 Logistic 的分布函数在 <img src="https://www.zhihu.com/equation?tex=%5Cmu=0,+%5Cgamma=1" alt="[公式]"> 的特殊形式。</p>
<p>logistic regression是使用线性回归的预测值逼近真实分类的对数几率，优点是：</p>
<ol>
<li>直接对分类概率建模，无需进行假设，避免假设带来的不准确</li>
<li>不仅可预测出类别，还能得到类别的概率，</li>
<li>对数几率函数是任意阶可导的函数，有许多数值优化算法都是可以求出最优解的</li>
</ol>
<p>损失函数：</p>
<p>优化的主要目标是找到一个方向，参数朝这个方向移动之后使得损失函数的值能够减小，</p>
<p>优点：</p>
<p>(1)对率函数任意阶可导，具有很好的数学性质，许多现有的数值优化算法都可以用来求最优解，训练速度快;</p>
<p>(2)简单易理解，模型的可解释性非常好，从<a href="https://www.cda.cn/map/tezheng/">特征</a>的权重可以看到不同的<a href="https://www.cda.cn/map/tezheng/">特征</a>对最后结果的影响;</p>
<p>(3)适合二分类问题，不需要缩放输入<a href="https://www.cda.cn/map/tezheng/">特征</a>;</p>
<p>(4)内存资源占用小，因为只需要存储各个维度的<a href="https://www.cda.cn/map/tezheng/">特征</a>值;</p>
<p>(5)直接对分类可能性进行建模，无需事先假设数据分布，避免了假设分布不准确所带来的问题</p>
<p>(6)以概率的形式输出，而非知识0.1判定，对许多利用概率辅助决策的任务很有用</p>
<p>缺点：</p>
<p>(1)不能用<a href="https://www.cda.cn/map/luojihuigui/">逻辑回归</a>去解决非线性问题，因为Logistic的决策面试线性的;</p>
<p>(2)对多重共线性数据较为敏感;</p>
<p>(3)很难处理数据不平衡的问题;</p>
<p>(4)准确率并不是很高，因为形式非常的简单(非常类似线性模型)，很难去拟合数据的真实分布;</p>
<p>(5)<a href="https://www.cda.cn/map/luojihuigui/">逻辑回归</a>本身无法筛选<a href="https://www.cda.cn/map/tezheng/">特征</a>，有时会用gbdt来筛选<a href="https://www.cda.cn/map/tezheng/">特征</a>，然后再上<a href="https://www.cda.cn/map/luojihuigui/">逻辑回归</a>。</p>
]]></content>
      <categories>
        <category>学习</category>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习-集成算法-AdaBoost</title>
    <url>/2021/10/01/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E9%9B%86%E6%88%90%E7%AE%97%E6%B3%95-AdaBoost/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>背景问题：</p>
<p>方法简述：</p>
<p>优点：</p>
<p>缺点：</p>
<span id="more"></span>

]]></content>
      <categories>
        <category>学习</category>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习-集成算法-随机森林</title>
    <url>/2021/10/01/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E9%9B%86%E6%88%90%E7%AE%97%E6%B3%95-%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>背景问题：</p>
<p>方法简述：</p>
<p>优点：</p>
<p>缺点：</p>
<span id="more"></span>

]]></content>
      <categories>
        <category>学习</category>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习经典算法概览</title>
    <url>/2021/09/29/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%BB%8F%E5%85%B8%E7%AE%97%E6%B3%95%E6%A6%82%E8%A7%88/</url>
    <content><![CDATA[<p>这是一篇机器学习经典算法的简述，包含了线性回归、逻辑回归、支持向量机(SVM)、最近邻居(KNN)、决策树、k平均、随机森林、朴素贝叶斯、降维、梯度增强（更新ing）</p>
<span id="more"></span>

<h1 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h1><h1 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h1><h1 id="集成算法-Ensemble-algorithms"><a href="#集成算法-Ensemble-algorithms" class="headerlink" title="集成算法 Ensemble algorithms"></a>集成算法 Ensemble algorithms</h1><p>将多个弱模型组合，弱模型单独训练，将哥哥弱模型的预测结果以某种方式结合完成总体的预测</p>
<p>主要问题在于找到可以组合的弱模型和弱模型结果的结合方式</p>
<ol>
<li>Boosting</li>
<li>Bagging</li>
<li>AdaBoost</li>
<li>Blending</li>
<li>Random Forest随机森林</li>
<li>** GBM（Gradient Boosting Machine）梯度推进机</li>
<li>** GBRT（Gradient Boosted Regression Tree） 梯度提升回归树</li>
</ol>
<p>优点：结合最优秀的模型们，可以得到更加优秀的预测结果</p>
<p>缺点：多模型融合计算量大</p>
<h3 id><a href="#" class="headerlink" title></a></h3><h1 id="决策树算法（Decision-Tree-Algorithm"><a href="#决策树算法（Decision-Tree-Algorithm" class="headerlink" title="决策树算法（Decision Tree Algorithm)"></a>决策树算法（Decision Tree Algorithm)</h1><p>Step1 选择分裂节点：当根据某个属性的值不能明确分到样本哪个类别是就将此属性作为节点对其分裂</p>
<p>Step2 选择一个合适的阈值进行分裂使其分类的错误率最小</p>
<h2 id="1-ID3"><a href="#1-ID3" class="headerlink" title="1.ID3"></a>1.ID3</h2>]]></content>
      <categories>
        <category>学习</category>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>概览</tag>
      </tags>
  </entry>
  <entry>
    <title>模型评价方法及指标</title>
    <url>/2021/10/01/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BB%B7%E6%96%B9%E6%B3%95%E5%8F%8A%E6%8C%87%E6%A0%87/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>背景问题：</p>
<p>方法简述：</p>
<p>优点：</p>
<p>缺点：</p>
<span id="more"></span>
]]></content>
      <categories>
        <category>学习</category>
        <category>深度学习</category>
        <category>神经网络</category>
      </categories>
      <tags>
        <tag>总结</tag>
        <tag>编程</tag>
        <tag>优化</tag>
        <tag>调参</tag>
      </tags>
  </entry>
  <entry>
    <title>深度学习-[2013]ZFNet网络模型</title>
    <url>/2021/10/01/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-2013-ZFNet%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>背景问题：</p>
<p>方法简述：</p>
<p>优点：</p>
<p>缺点：</p>
<span id="more"></span>
]]></content>
      <categories>
        <category>学习</category>
        <category>cv算法</category>
        <category>深度学习</category>
        <category>目标检测</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>论文复现</tag>
      </tags>
  </entry>
  <entry>
    <title>模型过拟合问题解决方法</title>
    <url>/2021/10/01/%E6%A8%A1%E5%9E%8B%E8%BF%87%E6%8B%9F%E5%90%88%E9%97%AE%E9%A2%98%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>背景问题：</p>
<p>方法简述：</p>
<p>优点：</p>
<p>缺点：</p>
<span id="more"></span>
]]></content>
      <categories>
        <category>学习</category>
        <category>深度学习</category>
        <category>神经网络</category>
      </categories>
      <tags>
        <tag>总结</tag>
        <tag>编程</tag>
        <tag>优化</tag>
        <tag>调参</tag>
      </tags>
  </entry>
  <entry>
    <title>深度学习-Bottleneck结构的理解</title>
    <url>/2021/10/01/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Bottleneck%E7%BB%93%E6%9E%84%E7%9A%84%E7%90%86%E8%A7%A3/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>背景问题：</p>
<p>方法简述：</p>
<p>优点：</p>
<p>缺点：</p>
<span id="more"></span>
]]></content>
      <categories>
        <category>学习</category>
        <category>cv算法</category>
        <category>深度学习</category>
        <category>目标检测</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>论文复现</tag>
      </tags>
  </entry>
  <entry>
    <title>深度学习-Inception结构的理解</title>
    <url>/2021/10/01/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Inception%E7%BB%93%E6%9E%84%E7%9A%84%E7%90%86%E8%A7%A3/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>背景问题：</p>
<p>方法简述：</p>
<p>优点：</p>
<p>缺点：</p>
<span id="more"></span>
]]></content>
      <categories>
        <category>学习</category>
        <category>cv算法</category>
        <category>深度学习</category>
        <category>目标检测</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>论文复现</tag>
      </tags>
  </entry>
  <entry>
    <title>深度学习-LSTM长短期记忆</title>
    <url>/2021/10/01/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-LSTM%E9%95%BF%E7%9F%AD%E6%9C%9F%E8%AE%B0%E5%BF%86/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>深度学习-RNN循环神经网络</title>
    <url>/2021/10/01/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-RNN%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>深度学习-深度可分离卷积Depthwise Separable Conv</title>
    <url>/2021/10/01/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E6%B7%B1%E5%BA%A6%E5%8F%AF%E5%88%86%E7%A6%BB%E5%8D%B7%E7%A7%AFDepthwise-Separable-Conv/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>背景问题：</p>
<p>方法简述：</p>
<p>优点：</p>
<p>缺点：</p>
<span id="more"></span>



]]></content>
      <categories>
        <category>学习</category>
        <category>cv算法</category>
        <category>深度学习</category>
        <category>目标检测</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>论文复现</tag>
      </tags>
  </entry>
  <entry>
    <title>深度学习</title>
    <url>/2021/09/29/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/</url>
    <content><![CDATA[<p>#设置标签<br>在编辑文章的时候，tags:后面是设置标签的地方，如果有多个标签的话，可以用下面两种办法来设置：</p>
<h2 id="设置小标签"><a href="#设置小标签" class="headerlink" title="设置小标签"></a>设置小标签</h2><p>在编辑文章的时候，tags:后面是设置标签的地方，如果有多个标签的话，可以用下面两种办法来设置：</p>
]]></content>
      <categories>
        <category>机器学习</category>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>OpenCV</tag>
        <tag>编程</tag>
      </tags>
  </entry>
  <entry>
    <title>目标检测-R-CNN系列</title>
    <url>/2021/09/29/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B-R-CNN%E7%B3%BB%E5%88%97/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>背景问题：</p>
<p>方法简述：</p>
<p>优点：</p>
<p>缺点：</p>
<span id="more"></span>
]]></content>
      <categories>
        <category>学习</category>
        <category>cv算法</category>
        <category>深度学习</category>
        <category>目标检测</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>论文复现</tag>
      </tags>
  </entry>
  <entry>
    <title>目标检测-Retina-Net</title>
    <url>/2021/09/29/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B-Retina-Net/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>背景问题：</p>
<p>方法简述：</p>
<p>优点：</p>
<p>缺点：</p>
<span id="more"></span>
]]></content>
      <categories>
        <category>学习</category>
        <category>cv算法</category>
        <category>深度学习</category>
        <category>目标检测</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>论文复现</tag>
      </tags>
  </entry>
  <entry>
    <title>目标检测-SSD系列</title>
    <url>/2021/09/29/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B-SSD%E7%B3%BB%E5%88%97/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>背景问题：</p>
<p>方法简述：</p>
<p>优点：</p>
<p>缺点：</p>
<span id="more"></span>
]]></content>
      <categories>
        <category>学习</category>
        <category>cv算法</category>
        <category>深度学习</category>
        <category>目标检测</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>论文复现</tag>
      </tags>
  </entry>
  <entry>
    <title>目标检测-YOLOv2</title>
    <url>/2021/10/01/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B-YOLOv2/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>背景问题：</p>
<p>方法简述：</p>
<p>优点：</p>
<p>缺点：</p>
<span id="more"></span>

]]></content>
      <categories>
        <category>学习</category>
        <category>cv算法</category>
        <category>深度学习</category>
        <category>目标检测</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>论文复现</tag>
        <tag>YOLO</tag>
      </tags>
  </entry>
  <entry>
    <title>目标检测-YOLOv3</title>
    <url>/2021/10/01/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B-YOLOv3/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>背景问题：</p>
<p>方法简述：</p>
<p>优点：</p>
<p>缺点：</p>
<span id="more"></span>

]]></content>
      <categories>
        <category>学习</category>
        <category>cv算法</category>
        <category>深度学习</category>
        <category>目标检测</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>论文复现</tag>
        <tag>YOLO</tag>
      </tags>
  </entry>
  <entry>
    <title>目标检测-YOLOv4</title>
    <url>/2021/10/01/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B-YOLOv4/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>背景问题：</p>
<p>方法简述：</p>
<p>优点：</p>
<p>缺点：</p>
<span id="more"></span>

]]></content>
      <categories>
        <category>学习</category>
        <category>cv算法</category>
        <category>深度学习</category>
        <category>目标检测</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>论文复现</tag>
        <tag>YOLO</tag>
      </tags>
  </entry>
  <entry>
    <title>目标检测-YOLOv5</title>
    <url>/2021/10/01/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B-YOLOv5/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>背景问题：</p>
<p>方法简述：</p>
<p>优点：</p>
<p>缺点：</p>
<span id="more"></span>
]]></content>
      <categories>
        <category>学习</category>
        <category>cv算法</category>
        <category>深度学习</category>
        <category>目标检测</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>论文复现</tag>
        <tag>YOLO</tag>
      </tags>
  </entry>
  <entry>
    <title>目标检测-YOLOx</title>
    <url>/2021/10/01/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B-YOLOx/</url>
    <content><![CDATA[<ul>
<li><h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>背景问题：</p>
<p>方法简述：</p>
<p>优点：</p>
<p>缺点：</p>
<span id="more"></span></li>
</ul>
]]></content>
      <categories>
        <category>学习</category>
        <category>cv算法</category>
        <category>深度学习</category>
        <category>目标检测</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>论文复现</tag>
        <tag>YOLO</tag>
      </tags>
  </entry>
  <entry>
    <title>目标检测-YOLOv1</title>
    <url>/2021/09/29/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B-YOLO%E7%B3%BB%E5%88%97/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>背景问题：</p>
<p>方法简述：</p>
<p>优点：</p>
<p>缺点：</p>
<span id="more"></span>

<h1 id="YOLOv1"><a href="#YOLOv1" class="headerlink" title="YOLOv1"></a>YOLOv1</h1><ol>
<li><p>核心思想 回归思想</p>
<p>整张图作为网络的输入，在输出层回归Bounding Box的位置和Bounding Box的类别</p>
<p>相比Faster R-CNN虽然也是整张图输入，但Proposal+ classifier没变, 只是将Proposal也放在了CNN中</p>
</li>
<li><p>实现方法</p>
<ol>
<li>将整张图分为S*S的网格，如果某个object的中心落在这个网格中，这个网格就负责预测这个object</li>
<li>每个网格要预测B个bounding box，每个bounding box除了要回归自身的位置之外，还要附带预测一个confidence值。这个confidence代表了所预测的box中含有object的置信度和这个box预测的有多准两重信息</li>
<li>每个bounding Box 有五个值要预测(x, y, w, h)和confidence。每个网格还要预测一个类别信息，记为C类。则SxS个网格，每个网格要预测B个bounding box还要预测C个categories。输出就是S x S x (5*B+C)的一个tensor。</li>
<li><strong>简单的概括就是：</strong><ul>
<li>给个一个输入图像，首先将图像划分成7*7的网格；</li>
<li>对于每个网格，我们都预测2个边框（包括每个边框是目标的置信度以及每个边框区域在多个类别上的概率）；</li>
<li>根据上一步可以预测出7<em>7</em>2个目标窗口，然后根据阈值去除可能性比较低的目标窗口，最后NMS去除冗余窗口即可。</li>
</ul>
</li>
</ol>
</li>
<li><p>在YOLOv1的损失函数中：</p>
<p>Loss= 坐标预测的Loss + 含object的Box的confidence预测 + 不含object的Box的confidence预测 + 类别预测</p>
<p>只有当某个网格中有object的时候才对classification error进行惩罚。<br>只有当某个box predictor对某个ground truth box负责的时候，才会对box的coordinate error进行惩罚，而对哪个ground truth box负责就看其预测值和ground truth box的IoU是不是在那个cell的所有box中最大。<br>注：</p>
<p>YOLOv1方法模型训练依赖于物体识别标注数据，因此，对于非常规的物体形状或比例，YOLOv1的检测效果并不理想。<br>YOLOv1采用了多个下采样层，网络学到的物体特征并不精细，因此也会影响检测效果。<br>YOLOv1的loss函数中，大物体IOU误差和小物体IOU误差对网络训练中loss贡献值接近（虽然采用求平方根方式，但没有根本解决问题）。因此，对于小物体，小的IOU误差也会对网络优化过程造成很大的影响，从而降低了物体检测的定位准确性。<br>YOLO的缺点</p>
<p>YOLO对相互靠的很近的物体和很小的群体检测效果不好，这是因为一个网格中只预测了两个框，并且只属于一类；<br>同一类物体出现的新的不常见的长宽比和其他情况时，泛化能力偏弱；<br>由于损失函数的问题，定位误差是影响检测效果的主要原因。尤其是大小物体的处理上，还有待加强。</p>
</li>
</ol>
<h1 id="YOLOv2"><a href="#YOLOv2" class="headerlink" title="YOLOv2"></a>YOLOv2</h1><p>从预测<strong>更准确（Better）</strong>，<strong>速度更快（Faster）</strong>，<strong>识别对象更多（Stronger）</strong>这三个方面进行了改进。</p>
<p>采用联合训练算法的基本思路就是：同时在检测数据集和分类数据集上训练物体检测器（Object Detectors ），<strong>用检测数据集的数据学习物体的准确位置，用分类数据集的数据来增加分类的类别量、提升健壮性。</strong></p>
<ol>
<li><p>改进点</p>
<ol>
<li><p>BN层</p>
</li>
<li><p>高分辨率分类 224*224 -》448 * 448</p>
</li>
<li><h3 id="Convolution-with-anchor-boxes：-YOLOv1包含有全连接层，从而能直接预测Bounding-Boxes的坐标值。Faster-R-CNN算法只用卷积层与Region-Proposal-Network来预测Anchor-Box的偏移值与置信度，而不是直接预测坐标值，YOLOv2作者发现通过预测偏移量而不是坐标值能够简化问题，让神经网络学习起来更容易。"><a href="#Convolution-with-anchor-boxes：-YOLOv1包含有全连接层，从而能直接预测Bounding-Boxes的坐标值。Faster-R-CNN算法只用卷积层与Region-Proposal-Network来预测Anchor-Box的偏移值与置信度，而不是直接预测坐标值，YOLOv2作者发现通过预测偏移量而不是坐标值能够简化问题，让神经网络学习起来更容易。" class="headerlink" title="Convolution with anchor boxes： YOLOv1包含有全连接层，从而能直接预测Bounding Boxes的坐标值。Faster R-CNN算法只用卷积层与Region Proposal Network来预测Anchor Box的偏移值与置信度，而不是直接预测坐标值，YOLOv2作者发现通过预测偏移量而不是坐标值能够简化问题，让神经网络学习起来更容易。"></a>Convolution with anchor boxes： YOLOv1包含有全连接层，从而能直接预测Bounding Boxes的坐标值。Faster R-CNN算法只用卷积层与Region Proposal Network来预测Anchor Box的偏移值与置信度，而不是直接预测坐标值，YOLOv2作者发现通过预测偏移量而不是坐标值能够简化问题，让神经网络学习起来更容易。</h3><p>借鉴Faster RCNN的做法，YOLOv2也尝试采用先验框（anchor）。在每个grid预先设定一组不同大小和宽高比的边框，来覆盖整个图像的不同位置和多种尺度，这些先验框作为预定义的候选区在神经网络中将检测其中是否存在对象，以及微调边框的位置</p>
</li>
<li><p>YOLOv2的做法是对训练集中标注的边框进行K-means聚类分析，以寻找尽可能匹配样本的边框尺寸。</p>
<p><img src="/2021/09/29/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B-YOLO%E7%B3%BB%E5%88%97/1.png"></p>
</li>
</ol>
</li>
</ol>
<p>下面我们具体看看y1,y2,y3是如何而来的。<br>网络中作者进行了三次检测，分别是在32倍降采样，16倍降采样，8倍降采样时进行检测，这样在多尺度的feature map上检测跟SSD有点像。在网络中使用up-sample（上采样）的原因:网络越深的特征表达效果越好，比如在进行16倍降采样检测，如果直接使用第四次下采样的特征来检测，这样就使用了浅层特征，这样效果一般并不好。如果想使用32倍降采样后的特征，但深层特征的大小太小，因此YOLOv3使用了步长为2的up-sample（上采样），把32倍降采样得到的feature map的大小提升一倍，也就成了16倍降采样后的维度。同理8倍采样也是对16倍降采样的特征进行步长为2的上采样，这样就可以使用深层特征进行detection。</p>
<p>作者通过上采样将深层特征提取，其维度是与将要融合的特征层维度相同的（channel不同）。如下图所示，85层将13×13×256的特征上采样得到26×26×256，再将其与61层的特征拼接起来得到26×26×768。为了得到channel255，还需要进行一系列的3×3，1×1卷积操作，这样既可以提高非线性程度增加泛化性能提高网络精度，又能减少参数提高实时性。52×52×255的特征也是类似的过程。</p>
]]></content>
      <categories>
        <category>学习</category>
        <category>cv算法</category>
        <category>深度学习</category>
        <category>目标检测</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>论文复现</tag>
        <tag>YOLO</tag>
      </tags>
  </entry>
  <entry>
    <title>神经网络-learning rate学习率的优化总结</title>
    <url>/2021/10/01/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-learning-rate%E5%AD%A6%E4%B9%A0%E7%8E%87%E7%9A%84%E4%BC%98%E5%8C%96%E6%80%BB%E7%BB%93/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>背景问题：</p>
<p>方法简述：</p>
<p>优点：</p>
<p>缺点：</p>
<span id="more"></span>
]]></content>
      <categories>
        <category>学习</category>
        <category>深度学习</category>
        <category>神经网络</category>
      </categories>
      <tags>
        <tag>总结</tag>
        <tag>编程</tag>
        <tag>优化</tag>
        <tag>调参</tag>
      </tags>
  </entry>
  <entry>
    <title>神经网络-损失函数</title>
    <url>/2021/10/01/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>背景问题：</p>
<p>方法简述：</p>
<p>优点：</p>
<p>缺点：</p>
<span id="more"></span>
]]></content>
      <categories>
        <category>学习</category>
        <category>深度学习</category>
        <category>神经网络</category>
      </categories>
      <tags>
        <tag>总结</tag>
        <tag>编程</tag>
        <tag>优化</tag>
        <tag>调参</tag>
      </tags>
  </entry>
  <entry>
    <title>神经网络-权值初始化的方法</title>
    <url>/2021/10/01/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E6%9D%83%E5%80%BC%E5%88%9D%E5%A7%8B%E5%8C%96%E7%9A%84%E6%96%B9%E6%B3%95/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>神经网络-梯度优化方法</title>
    <url>/2021/10/01/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E6%A2%AF%E5%BA%A6%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>背景问题：</p>
<p>方法简述：</p>
<p>优点：</p>
<p>缺点：</p>
<span id="more"></span>
]]></content>
      <categories>
        <category>学习</category>
        <category>深度学习</category>
        <category>神经网络</category>
      </categories>
      <tags>
        <tag>总结</tag>
        <tag>编程</tag>
        <tag>优化</tag>
        <tag>调参</tag>
      </tags>
  </entry>
  <entry>
    <title>神经网络-梯度爆炸和梯度消失</title>
    <url>/2021/10/01/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8%E5%92%8C%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>神经网络-激活函数</title>
    <url>/2021/10/01/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>背景问题：</p>
<p>方法简述：</p>
<p>优点：</p>
<p>缺点：</p>
<span id="more"></span>
]]></content>
      <categories>
        <category>学习</category>
        <category>深度学习</category>
        <category>神经网络</category>
      </categories>
      <tags>
        <tag>总结</tag>
        <tag>编程</tag>
        <tag>优化</tag>
        <tag>调参</tag>
      </tags>
  </entry>
  <entry>
    <title>站内说明</title>
    <url>/2021/09/14/%E7%AB%99%E5%86%85%E8%AF%B4%E6%98%8E/</url>
    <content><![CDATA[<p>本blog是作为Leo同学的自我拓展过程中的自律自省记录</p>
<p>对该blog的计划是对一下类别和领域的学习与探索：</p>
<p>理论方面：</p>
<p>​    计算机视觉</p>
<p>​    深度学习算法</p>
<p>​    机器学习算法</p>
<p>Coding方面：</p>
<p>​    OpenCV</p>
<p>​    Python</p>
<p>​    Pytorch</p>
<p>​    </p>
]]></content>
      <categories>
        <category>其他</category>
      </categories>
      <tags>
        <tag>概览</tag>
      </tags>
  </entry>
  <entry>
    <title>计算机视觉CV算法概览（更新ing）</title>
    <url>/2021/08/29/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89CV%E7%AE%97%E6%B3%95%E6%A6%82%E8%A7%88/</url>
    <content><![CDATA[<h1 id="图像识别"><a href="#图像识别" class="headerlink" title="图像识别"></a>图像识别</h1><h2 id="1-AlexNet"><a href="#1-AlexNet" class="headerlink" title="1. AlexNet"></a>1. AlexNet</h2><h2 id="2-VGGNet"><a href="#2-VGGNet" class="headerlink" title="2. VGGNet"></a>2. VGGNet</h2><h2 id="3-GoogLeNet"><a href="#3-GoogLeNet" class="headerlink" title="3. GoogLeNet"></a>3. GoogLeNet</h2><h2 id="4-ResNet"><a href="#4-ResNet" class="headerlink" title="4. ResNet"></a>4. ResNet</h2><h2 id="5-DenseNet"><a href="#5-DenseNet" class="headerlink" title="5. DenseNet"></a>5. DenseNet</h2><h1 id="目标检测"><a href="#目标检测" class="headerlink" title="目标检测"></a>目标检测</h1><h2 id="1-R-CNN-系列"><a href="#1-R-CNN-系列" class="headerlink" title="1. R-CNN 系列"></a>1. R-CNN 系列</h2><h3 id="1-R-CNN"><a href="#1-R-CNN" class="headerlink" title="1. R-CNN"></a>1. R-CNN</h3><h3 id="2-Fast-R-CNN"><a href="#2-Fast-R-CNN" class="headerlink" title="2. Fast R-CNN"></a>2. Fast R-CNN</h3><h3 id="3-Faster-R-CNN"><a href="#3-Faster-R-CNN" class="headerlink" title="3. Faster R-CNN"></a>3. Faster R-CNN</h3><h2 id="2-Yolo系列"><a href="#2-Yolo系列" class="headerlink" title="2. Yolo系列"></a>2. Yolo系列</h2><h3 id="1-Yolo-V1"><a href="#1-Yolo-V1" class="headerlink" title="1.Yolo V1"></a>1.Yolo V1</h3><h3 id="2-Yolo-V2"><a href="#2-Yolo-V2" class="headerlink" title="2.Yolo V2"></a>2.Yolo V2</h3><h3 id="3-Yolo-V3"><a href="#3-Yolo-V3" class="headerlink" title="3.Yolo V3"></a>3.Yolo V3</h3><h3 id="4-Yolo-V4"><a href="#4-Yolo-V4" class="headerlink" title="4.Yolo V4"></a>4.Yolo V4</h3><h3 id="5-Yolo-V5"><a href="#5-Yolo-V5" class="headerlink" title="5.Yolo V5"></a>5.Yolo V5</h3><h2 id="3-SSD"><a href="#3-SSD" class="headerlink" title="3. SSD"></a>3. SSD</h2><h2 id="4-Retina-Net"><a href="#4-Retina-Net" class="headerlink" title="4. Retina-Net"></a>4. Retina-Net</h2><h1 id="图像分割"><a href="#图像分割" class="headerlink" title="图像分割"></a>图像分割</h1><h2 id="1-FCN"><a href="#1-FCN" class="headerlink" title="1. FCN"></a>1. FCN</h2><h2 id="2-Mask-R-CNN"><a href="#2-Mask-R-CNN" class="headerlink" title="2. Mask R-CNN"></a>2. Mask R-CNN</h2><h1 id="目标追踪"><a href="#目标追踪" class="headerlink" title="目标追踪"></a>目标追踪</h1><h2 id="1-Goturn"><a href="#1-Goturn" class="headerlink" title="1. Goturn"></a>1. Goturn</h2><h2 id="2"><a href="#2" class="headerlink" title="2."></a>2.</h2><h1 id="图像生成"><a href="#图像生成" class="headerlink" title="图像生成"></a>图像生成</h1><h2 id="1-GAN"><a href="#1-GAN" class="headerlink" title="1. GAN"></a>1. GAN</h2>]]></content>
      <categories>
        <category>学习</category>
        <category>cv算法</category>
      </categories>
      <tags>
        <tag>概览</tag>
      </tags>
  </entry>
  <entry>
    <title>轻量化网络-MobileNet</title>
    <url>/2021/09/29/%E8%BD%BB%E9%87%8F%E5%8C%96%E7%BD%91%E7%BB%9C-MobileNet/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>背景问题：</p>
<p>方法简述：</p>
<p>优点：</p>
<p>缺点：</p>
<span id="more"></span>
]]></content>
      <categories>
        <category>学习</category>
        <category>cv算法</category>
        <category>深度学习</category>
        <category>轻量化模型</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>论文复现</tag>
      </tags>
  </entry>
</search>
