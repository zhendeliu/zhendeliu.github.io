<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>AI算法面试问题-机器学习（一）</title>
    <url>/2021/10/02/AI%E7%AE%97%E6%B3%95%E9%9D%A2%E8%AF%95%E9%97%AE%E9%A2%98-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%80%EF%BC%89/</url>
    <content><![CDATA[<h1 id="问题列表"><a href="#问题列表" class="headerlink" title="问题列表"></a>问题列表</h1><ol>
<li>详细说一下支持向量机的原理</li>
<li>数据归一化的原因</li>
<li>哪些算法不需要归一化处理</li>
<li>树形结构为什么不需要归一化</li>
<li>常用的距离计算有哪些，有什么区别</li>
<li>机器学习项目的流程</li>
<li>Logistic Regression 逻辑回归的原理</li>
<li>逻辑回归为什么要特征离散化</li>
<li>overfitting怎么解决</li>
<li>逻辑回归和SVM的区别与联系</li>
</ol>
<span id="more"></span>

<p>面试问题来源机器学习面试150题 <a href="https://www.zhihu.com/column/c_1284826692855771136">https://www.zhihu.com/column/c_1284826692855771136</a></p>
<h1 id="问题及解答"><a href="#问题及解答" class="headerlink" title="问题及解答"></a>问题及解答</h1><h2 id="详细说一下支持向量机的原理"><a href="#详细说一下支持向量机的原理" class="headerlink" title="详细说一下支持向量机的原理"></a>详细说一下支持向量机的原理</h2>]]></content>
      <categories>
        <category>学习</category>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>面试</tag>
        <tag>总结</tag>
        <tag>技巧</tag>
      </tags>
  </entry>
  <entry>
    <title>AI算法面试问题-机器学习（三）</title>
    <url>/2021/10/02/AI%E7%AE%97%E6%B3%95%E9%9D%A2%E8%AF%95%E9%97%AE%E9%A2%98-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%89%EF%BC%89/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>OpenCV图像处理基础（三）</title>
    <url>/2021/09/29/OpenCV%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E5%9F%BA%E7%A1%80%EF%BC%88%E4%B8%89%EF%BC%89/</url>
    <content><![CDATA[<h1 id="灰度图"><a href="#灰度图" class="headerlink" title="灰度图"></a>灰度图</h1><h1 id="灰度直方图等"><a href="#灰度直方图等" class="headerlink" title="灰度直方图等"></a>灰度直方图等</h1><h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>背景问题：</p>
<p>方法简述：</p>
<p>优点：</p>
<p>缺点：</p>
<span id="more"></span>
]]></content>
      <categories>
        <category>学习</category>
        <category>cv算法</category>
      </categories>
      <tags>
        <tag>OpenCV</tag>
        <tag>基础操作</tag>
        <tag>噪音</tag>
        <tag>灰度图</tag>
        <tag>编程</tag>
      </tags>
  </entry>
  <entry>
    <title>OpenCV图像处理基础（一）</title>
    <url>/2021/09/29/OpenCV%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E5%9F%BA%E7%A1%80%EF%BC%88%E4%B8%80%EF%BC%89/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>数字图像基础</p>
<p>读、写、显示（图片、视频）</p>
<p>缩放、补边、裁剪</p>
<p>绘制形状</p>
<p>书写文字</p>
<span id="more"></span>

<h1 id="图像的数字表示"><a href="#图像的数字表示" class="headerlink" title="图像的数字表示"></a>图像的数字表示</h1><p>在计算机系统中存储的是每张图像每个像素点的值</p>
<p>灰度图，单通道图像也就是8位图像，每个像素点的值占8字节，00000000-11111111也就是0-255</p>
<p>RGB/BGR图像，是三通道图像，也就是24位图像，每个像素点的值占24字节，也就是三个通道每个通道的值占8字节0-255</p>
<p>灰度图也可以表示成三通道，不过三通道上的值是相等的</p>
<p>像素值为0表示黑色，255表示白色</p>
<p>32位图像在24位的基础上增加了一个alpha分量，该分量用于记录图像的透明度信息。</p>
<h1 id="读、写、显示"><a href="#读、写、显示" class="headerlink" title="读、写、显示"></a>读、写、显示</h1><h2 id="图片读取"><a href="#图片读取" class="headerlink" title="图片读取"></a>图片读取</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="comment"># 读取函数imread()</span></span><br><span class="line">img = cv2.imread(<span class="string">&quot;Path.jpg/png&quot;</span>)</span><br><span class="line"><span class="comment"># 显示函数 imshow()</span></span><br><span class="line">cv2.imshow(<span class="string">&#x27;窗口名称&#x27;</span>,img)</span><br><span class="line"><span class="comment"># 写入函数 imwrite()</span></span><br><span class="line">cv2.imwrite(<span class="string">&#x27;写入的文件名.jpg&#x27;</span>,img)</span><br><span class="line"><span class="comment"># cv2.IMWRITE_JPEG_QUALITY指定jpg质量，范围0到100，默认95，越高画质越好，文件越大</span></span><br><span class="line">cv2.imwrite(<span class="string">&#x27;写入的文件名.jpg&#x27;</span>, img, (cv2.IMWRITE_JPEG_QUALITY, <span class="number">80</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># cv2.IMWRITE_PNG_COMPRESSION指定png质量，范围0到9，默认3，越高文件越小，画质越差</span></span><br><span class="line">cv2.imwrite(<span class="string">&#x27;写入的文件名.png&#x27;</span>, img, (cv2.IMWRITE_PNG_COMPRESSION, <span class="number">5</span>))</span><br></pre></td></tr></table></figure>

<h2 id="显示图片需要对窗口进行设置"><a href="#显示图片需要对窗口进行设置" class="headerlink" title="显示图片需要对窗口进行设置"></a>显示图片需要对窗口进行设置</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">cv2.imshow(<span class="string">&#x27;窗口名称&#x27;</span>,img)</span><br><span class="line">cv2.waitkey(<span class="number">0</span>)</span><br><span class="line">cv2.destoryAllWindows()</span><br></pre></td></tr></table></figure>

<p><code>cv2.waitKey()</code> 是键盘绑定函数，可以设置毫秒级数值，如果是0会一直等待，也可以设置为指定字母，比如a</p>
<p><code>cv2.destoryAllWindows()</code>销毁所有创建的窗口</p>
<h2 id="视频读取，显示"><a href="#视频读取，显示" class="headerlink" title="视频读取，显示"></a>视频读取，显示</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">cap = cv2.VideoCapture(<span class="string">&#x27;视频名称.mp4/avi&#x27;</span>) <span class="comment"># 获取视频</span></span><br><span class="line"><span class="comment"># cap = cv2.VideoCapture(0) # 获取摄像头 0内置 1外置摄像头</span></span><br><span class="line"><span class="comment"># 视频显示：while循环显示视频的每一帧</span></span><br><span class="line"><span class="comment"># 使用read()获取的视频帧，将每一帧显示100ms,如果期间检测到‘q‘则退出，关闭窗口</span></span><br><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">  reg,frame = cap.read()</span><br><span class="line">  cv2.imshow(<span class="string">&#x27;caputre&#x27;</span>,frame)</span><br><span class="line">  <span class="keyword">if</span> cv2.waitkey(<span class="number">100</span>) <span class="keyword">and</span> <span class="number">0xFF</span> == <span class="built_in">ord</span>(<span class="string">&#x27;q&#x27;</span>):</span><br><span class="line">    <span class="keyword">break</span></span><br></pre></td></tr></table></figure>

<h1 id="缩放，补边，裁剪"><a href="#缩放，补边，裁剪" class="headerlink" title="缩放，补边，裁剪"></a>缩放，补边，裁剪</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 缩放成200x200的方形图像</span></span><br><span class="line">img_200x200 = cv2.resize(img, (<span class="number">200</span>, <span class="number">200</span>))</span><br><span class="line"><span class="comment"># 根据缩放比例来</span></span><br><span class="line"><span class="comment"># 默认线性插值 cv2.INTER_LINEAR</span></span><br><span class="line">img = cv2.resize(img,(<span class="number">0</span>,<span class="number">0</span>),fx=<span class="number">0.5</span>,fy=<span class="number">0.5</span>,interpolation=cv2.INTER_NEAREST) <span class="comment"># 邻近插值</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 上下左右分布补边50，40，30，20宽，常数填充 都是黑边</span></span><br><span class="line">img = cv2.copyMakeBorder(img, <span class="number">50</span>, <span class="number">40</span>, <span class="number">30</span>, <span class="number">20</span>,cv2.BORDER_CONSTANT, value=(<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>,<span class="number">0</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 根据切片来做裁剪</span></span><br><span class="line">imgCropped = img[<span class="number">50</span>:<span class="number">250</span>,<span class="number">120</span>:<span class="number">330</span>]</span><br></pre></td></tr></table></figure>



<h1 id="绘制形状"><a href="#绘制形状" class="headerlink" title="绘制形状"></a>绘制形状</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cv2.line() 图像，起点，终点，画笔颜色，线宽</span></span><br><span class="line">img = cv2.line(img,(<span class="number">0</span>,<span class="number">0</span>),(<span class="number">511</span>,<span class="number">511</span>),(<span class="number">255</span>,<span class="number">0</span>,<span class="number">0</span>),<span class="number">5</span>)  </span><br><span class="line"><span class="comment"># cv2.circle() 图像，圆心，半径，颜色，-1是表示一个封闭的图形</span></span><br><span class="line">img = cv2.circle(img,(<span class="number">447</span>,<span class="number">63</span>),<span class="number">63</span>,(<span class="number">0</span>,<span class="number">0</span>,<span class="number">255</span>),-<span class="number">1</span>)  </span><br><span class="line"><span class="comment"># cv2.rectangle() 图像，左上角，右下角，颜色，线宽</span></span><br><span class="line">img = cv2.rectangle(img,(<span class="number">384</span>,<span class="number">0</span>),(<span class="number">510</span>,<span class="number">128</span>),(<span class="number">0</span>,<span class="number">255</span>,<span class="number">0</span>),<span class="number">3</span>) </span><br><span class="line"><span class="comment"># cv2.ellipse()</span></span><br><span class="line">img = cv2.ellipse(img,(<span class="number">256</span>,<span class="number">256</span>),(<span class="number">100</span>,<span class="number">50</span>),<span class="number">0</span>,<span class="number">0</span>,<span class="number">180</span>,<span class="number">255</span>,-<span class="number">1</span>) </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h1 id="书写文字"><a href="#书写文字" class="headerlink" title="书写文字"></a>书写文字</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cv2.putText()</span></span><br><span class="line"><span class="comment"># 各参数依次是：图片，添加的文字，左上角坐标，字体，字体大小，颜色，字体粗细</span></span><br><span class="line">img = cv2.putText(img, <span class="built_in">str</span>, (<span class="number">123</span>,<span class="number">456</span>)), font, <span class="number">2</span>, (<span class="number">0</span>,<span class="number">255</span>,<span class="number">0</span>), <span class="number">3</span>)</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>学习</category>
        <category>cv算法</category>
      </categories>
      <tags>
        <tag>OpenCV</tag>
        <tag>基础操作</tag>
        <tag>噪音</tag>
        <tag>编程</tag>
      </tags>
  </entry>
  <entry>
    <title>AI算法面试问题-机器学习（二）</title>
    <url>/2021/10/02/AI%E7%AE%97%E6%B3%95%E9%9D%A2%E8%AF%95%E9%97%AE%E9%A2%98-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%BA%8C%EF%BC%89/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>cv算法面试问题总结（一）</title>
    <url>/2021/09/29/cv%E7%AE%97%E6%B3%95%E9%9D%A2%E8%AF%95%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93%EF%BC%88%E4%B8%80%EF%BC%89/</url>
    <content><![CDATA[<h1 id="问题列表："><a href="#问题列表：" class="headerlink" title="问题列表："></a>问题列表：</h1><ol>
<li>CNN的特点和优势</li>
<li>decent 反卷积的作用</li>
<li>dropout 的作用和实现机制</li>
<li>深度学习中有哪些可加快收敛（降低训练难度）的方法</li>
<li>什么导致过拟合，如何防止过拟合</li>
<li>**LSTM防止梯度弥散和爆炸 **</li>
<li>Local Connected Conv</li>
<li>神经网络权值初始化的方式和区别</li>
<li>简述Convolution,pooling,和Normalization在卷积中的作用</li>
<li>Dilated conv(空洞卷积)优缺点和应用场景</li>
</ol>
<span id="more"></span>

<h1 id="常见问题及回答总结"><a href="#常见问题及回答总结" class="headerlink" title="常见问题及回答总结"></a>常见问题及回答总结</h1><h2 id="CNN的特点和优势"><a href="#CNN的特点和优势" class="headerlink" title="CNN的特点和优势"></a>CNN的特点和优势</h2><ol>
<li>改变全链接为局部连接，可以提取局部特征</li>
<li>权值共享，减少参数数量，降低训练难度（空间和时间都降低）</li>
<li>降维， 通过池化或者卷积stride实现</li>
<li>多层次结构：降低层次的局部特征组合为较高层次的特征。不同层次的特征应对不同任务</li>
<li>可以完全共享也可以局部共享 （比如眼睛鼻子嘴巴等位置样式固定的可以用和脸部不一样的卷积核）</li>
</ol>
<h2 id="decent的作用"><a href="#decent的作用" class="headerlink" title="decent的作用"></a>decent的作用</h2><ol>
<li>CNN可视化，将conv中得到的feature map还原到像素空间，观察特定的feature map对哪些图案比较敏感</li>
<li>Upsampling 上采样</li>
<li>Unsupervised learning 重构图像</li>
</ol>
<h2 id="dropout-的作用和实现机制"><a href="#dropout-的作用和实现机制" class="headerlink" title="dropout 的作用和实现机制"></a>dropout 的作用和实现机制</h2><ol>
<li>原理是 在深度学习网络训练中对于输入层和隐藏层的神经网络单元 按照一定的概率P（伯努利分布） 暂时性！！！的丢弃。对于随机梯度下降，由于是随机丢弃，所以对于每一个mini-batch都在训练不同的网络</li>
<li>作用是防止过拟合，提高效果</li>
<li>缺点是 收敛速度减慢，由于每一次迭代只有一部分参数更新，导致梯度下降的速度减慢</li>
<li>测试时，每个权重值需要乘概率p <a href="https://zhuanlan.zhihu.com/p/38200980">https://zhuanlan.zhihu.com/p/38200980</a> dropout 必读</li>
</ol>
<h2 id="深度学习中有哪些可加快收敛（降低训练难度）的方法"><a href="#深度学习中有哪些可加快收敛（降低训练难度）的方法" class="headerlink" title="深度学习中有哪些可加快收敛（降低训练难度）的方法"></a>深度学习中有哪些可加快收敛（降低训练难度）的方法</h2><ol>
<li>bottleneck瓶颈结构： 在计算比较大的卷积层的之前使用一个1<em>1的卷积来压缩大卷积层输入特征图的通道数，用来减少计算量。大卷积层完成之后按照实际情况，有时候需要1</em>1的卷积来将大卷积层的输出特征图的通道数复原</li>
<li>残差 （还不是很明白）</li>
<li>学习率，步长，动量</li>
<li>优化方法</li>
<li>预训练</li>
</ol>
<h2 id="什么导致过拟合，如何防止过拟合"><a href="#什么导致过拟合，如何防止过拟合" class="headerlink" title="什么导致过拟合，如何防止过拟合"></a>什么导致过拟合，如何防止过拟合</h2><ol>
<li>过拟合的原因： 样本量过小，样本抽取不均衡噪音过多，参数太多模型复杂度高，权值迭代次数足够多拟合了训练样本中的噪音和不具代表性的特征</li>
<li>防止方法：<ol>
<li>数据增强 data argumentation</li>
<li>early stop</li>
<li>dropout</li>
<li>Batch Normalization</li>
<li>使用更简单的模型</li>
<li>参数正则化： 通过一定方法使神经网络中的部分神经元关闭，降低模型的复杂程度。正则化就是为了减小测试误差的，虽然有的时候可能会以增大训练误差为代价。正则化是为了显著的减小方差而较小的增大偏差。也就是提升泛化能力</li>
<li>加噪音（输入数噪声，权重噪声，响应结果里面加噪声）</li>
<li>freeze预训练网络中的某几层 #代码 <a href="https://blog.csdn.net/weixin_41712499/article/details/111295683">https://blog.csdn.net/weixin_41712499/article/details/111295683</a></li>
<li>结合多个模型 <ol>
<li>Bagging: 可以将其理解为一个分段函数，使用不同的模型拟合不同部分的训练集。如随机森林就是训练了一堆互不关联的决策树</li>
<li>Boosting：使用多个模型最后将模型的输出加权平均</li>
</ol>
</li>
</ol>
</li>
</ol>
<h2 id="LSTM防止梯度弥散和爆炸"><a href="#LSTM防止梯度弥散和爆炸" class="headerlink" title="LSTM防止梯度弥散和爆炸"></a>LSTM防止梯度弥散和爆炸</h2><h2 id="Local-Connected-Conv"><a href="#Local-Connected-Conv" class="headerlink" title="Local Connected Conv"></a>Local Connected Conv</h2><ol>
<li>人脸在不同的区域存在不同的特征（眼睛／鼻子／嘴的分布位置相对固定），当不存在全局的局部特征分布时，Local-Conv更适合特征的提取</li>
</ol>
<h2 id="神经网络权值初始化的方式和区别"><a href="#神经网络权值初始化的方式和区别" class="headerlink" title="神经网络权值初始化的方式和区别"></a>神经网络权值初始化的方式和区别</h2><p>​    详见权值初始化blog</p>
<ol>
<li>常量初始化</li>
<li>随机高斯初始化： 将权重初始化为固定的均值和方差（例如均值取0，方差取0.01）如果初始的方差小，如0.1，就会导致在前向传播过程中，不同的层的输入不断减小。会导致权重的更新速度很慢很慢。初始化的方差如果太大，就会使得每一层的输出越来越大。形如tanh激活函数，就会容易导致梯度饱和的现象</li>
<li>均匀分布初始化</li>
<li>Xavier初始化</li>
<li>双线性初始化</li>
<li>msra初始化</li>
</ol>
<h2 id="简述Convolution-pooling-和Normalization在卷积中的作用"><a href="#简述Convolution-pooling-和Normalization在卷积中的作用" class="headerlink" title="简述Convolution,pooling,和Normalization在卷积中的作用"></a>简述Convolution,pooling,和Normalization在卷积中的作用</h2><ol>
<li>Convolution：通过卷积核运算，提取卷积核希望提取的特征</li>
<li>pooling层 减小图像大小，加速计算，使其检测出的特征更加健壮</li>
<li>fully connected： 用来做分类</li>
<li>激活层 使得函数更加复杂</li>
<li>Batch Normalization：<ol>
<li>背景：深度学习中数据分布在某一层开始有明显的偏移，网络加深会加剧，导致模型优化的难度增加，也就是梯度弥散。</li>
<li>方法：在每个卷积层之后重新调整数据分布，解决梯度问题。在网络每一层输入前，先做归一化处理，加一个归一化层。但并不是盲目加。变换重构，引入可学习的参数r,B, 从而控制归一化尽可能不影响特征提取</li>
<li>好处：降低对参数初始化的依赖。可使用更高的学习率，加速训练。一定程度上增加了泛化能力，可以取代dropout</li>
</ol>
</li>
</ol>
<h2 id="Dilated-conv-空洞卷积-优缺点和应用场景"><a href="#Dilated-conv-空洞卷积-优缺点和应用场景" class="headerlink" title="Dilated conv(空洞卷积)优缺点和应用场景"></a>Dilated conv(空洞卷积)优缺点和应用场景</h2><ol>
<li>背景： FCN全卷积的语义分割问题中，需要输入图像和输出图像的size保持一致。若使用池化层，会降低size之后需要上采样，导致特征信息丢失精度降低。若使用较小的卷积核，可保持size一致，但需要增大特征图通道数会导致计算量较大。</li>
<li>空洞卷积，在特征图上进行0填充扩大特征图size，既能保持感受野又能保持计算点不变</li>
</ol>
]]></content>
      <categories>
        <category>学习</category>
        <category>cv算法</category>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>面试</tag>
        <tag>总结</tag>
        <tag>技巧</tag>
      </tags>
  </entry>
  <entry>
    <title>cv算法面试问题总结（二）</title>
    <url>/2021/09/29/cv%E7%AE%97%E6%B3%95%E9%9D%A2%E8%AF%95%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93%EF%BC%88%E4%BA%8C%EF%BC%89/</url>
    <content><![CDATA[<h1 id="问题列表"><a href="#问题列表" class="headerlink" title="问题列表"></a>问题列表</h1><ol>
<li>？？？ 判别模型和生成模型</li>
<li>如何决定算法是否收敛</li>
<li>正则的方法及特点</li>
<li>1*1卷积的作用</li>
<li>？？？ 无监督学习的方法有哪些</li>
<li>什么是感受野，增大感受野的方法</li>
<li>反卷积的棋盘效应和解决方法</li>
<li>神经网络参数量计算</li>
<li>？？？ 空洞卷积的原理和作用</li>
<li>？？？ 空洞卷积的感受野计算</li>
</ol>
<span id="more"></span>

<h1 id="问题及解答"><a href="#问题及解答" class="headerlink" title="问题及解答"></a>问题及解答</h1><h2 id="判别模型和生成模型"><a href="#判别模型和生成模型" class="headerlink" title="判别模型和生成模型"></a>判别模型和生成模型</h2><p>？？？</p>
<h2 id="如何决定算法是否收敛"><a href="#如何决定算法是否收敛" class="headerlink" title="如何决定算法是否收敛"></a>如何决定算法是否收敛</h2><ol>
<li><p>Loss 小于某个阈值</p>
</li>
<li><p>Loss趋于稳定，在某个之附近徘徊</p>
</li>
<li><p>迭代到一定的次数</p>
</li>
<li><p>看权值矩阵的变化，两次迭代权值变化很小</p>
</li>
</ol>
<h2 id="正则的方法及特点"><a href="#正则的方法及特点" class="headerlink" title="正则的方法及特点"></a>正则的方法及特点</h2><ol>
<li>正则的目的是防止过拟合，提高泛化能力</li>
<li>L1正则</li>
<li>L2正则</li>
<li>数据集扩增</li>
<li>dropout</li>
</ol>
<h2 id="1-1卷积的作用"><a href="#1-1卷积的作用" class="headerlink" title="1*1卷积的作用"></a>1*1卷积的作用</h2><ol>
<li>实现跨通道信息融合；不同通道上的一个线性组合，实际上就是加起来乘一个系数</li>
<li>feature map 通道数上的降维：<ol>
<li>假设输入特征维度是 100<em>100</em>128，卷积核的大小是5<em>5（stride=1，padding=2）通道数是256，经过卷积后输出的特征维度是（100-5+2</em>2）/1+1 = 100-》100<em>100</em>256 卷积参数量是128<em>5</em>5<em>256=819200 如果在5</em>5卷积之前使用一个64通道1<em>1的卷积，参数量是 128</em>1<em>1</em>64 + 64<em>5</em>5*256 = 417792</li>
</ol>
</li>
<li>增加非线性映射次数 1*1卷积之后会加一个非线性激活函数，使网络更深，也是网络更加具有判别信息的特征</li>
</ol>
<h2 id="无监督学习的方法有哪些"><a href="#无监督学习的方法有哪些" class="headerlink" title="无监督学习的方法有哪些"></a>无监督学习的方法有哪些</h2><h2 id="什么是感受野，增大感受野的方法"><a href="#什么是感受野，增大感受野的方法" class="headerlink" title="什么是感受野，增大感受野的方法"></a>什么是感受野，增大感受野的方法</h2><ol>
<li>感受野是卷积神经网络每一层输出的的feature map上的每个feature 在原图上映射的区域大小（原图：预处理resize，crop，wrap之后的图）</li>
<li>感受野越大，映射的原始图像的范围越广，可以蕴含更为全局，语义层次更高的信息</li>
<li>计算公式：一开始RF=1，然后RF= （RF-1）* stride + kernelsize （不管conv层还是pooling层，依次替换stride和kernelsize至最后）</li>
<li>增加感受野的方法：<ol>
<li>空洞卷积</li>
<li>增加pooling层，但会降低准确度 pooling会丢失信息</li>
<li>增大kernelsize， 会增加参数</li>
<li>增加卷积层的个数， 会面临梯度消失的问题。CPM中作者采用多阶段训练，并引入中间层监督来解决梯度消失的问题</li>
</ol>
</li>
</ol>
<h2 id="反卷积的棋盘效应和解决方法"><a href="#反卷积的棋盘效应和解决方法" class="headerlink" title="反卷积的棋盘效应和解决方法"></a>反卷积的棋盘效应和解决方法</h2><ol>
<li>反卷积是允许小图像中的点来绘制更大图像中的方块，很容易出现不均匀重叠，使得图像中的某个部分颜色比其他部分更深，也就是伪影。尤其是kernelsize不能被stride整除的时候。虽然网络可以通过学习权重来避免这种情况，但实践中很难完全避免</li>
<li>解决方法：<ol>
<li>修改反卷积形式<ol>
<li>确保反卷积核的大小可以被步长整除</li>
<li>网络末尾使用1*1的反卷积</li>
<li>调整卷积核权重</li>
</ol>
</li>
<li>修改上采样形式，采用插值方法代替反卷积进行上采样，如邻近差值和双线性差值</li>
<li>通过损失函数修正输出，在损失函数中加入total variation loss等损失函数，平滑输出图像，但图像边缘会模糊</li>
</ol>
</li>
</ol>
<h2 id="神经网络参数量计算"><a href="#神经网络参数量计算" class="headerlink" title="神经网络参数量计算"></a>神经网络参数量计算</h2><ol>
<li>带参数的层是：卷积层，BN层和全连接层。激活函数层，pooling层和Upsample层是没有参数的</li>
<li>卷积层参数计算：卷积核体积(Kw X Kh X Cin) X 卷积核个数Cout(也就是输出channel数) + 偏置项个数Cout (每个卷积核带有一个偏置项，但是不影响参数的数量级有时候会省略)</li>
<li>BN层的参数计算：2 X Cin(输入通道数) 有两个需要学习缩放因子和平移因子</li>
<li>FC层的参数计算：输入向量长度(和feature map相同size的一个卷积核)Ti X 输出向量长度To + 偏置量To  （全连接层逐渐被global average pooling取代 将最后一层的特征图 整张图进行一个均值化形成特征点，将这些特征点组合成最后的特征向量，通过softmax 进行计算）</li>
</ol>
<h2 id="空洞卷积的原理和作用"><a href="#空洞卷积的原理和作用" class="headerlink" title="空洞卷积的原理和作用"></a>空洞卷积的原理和作用</h2><h2 id="空洞卷积的感受野计算"><a href="#空洞卷积的感受野计算" class="headerlink" title="空洞卷积的感受野计算"></a>空洞卷积的感受野计算</h2>]]></content>
      <categories>
        <category>学习</category>
        <category>cv算法</category>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>面试</tag>
        <tag>总结</tag>
        <tag>技巧</tag>
      </tags>
  </entry>
  <entry>
    <title>cv算法面试问题总结（三）</title>
    <url>/2021/09/29/cv%E7%AE%97%E6%B3%95%E9%9D%A2%E8%AF%95%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93%EF%BC%88%E4%B8%89%EF%BC%89/</url>
    <content><![CDATA[<h1 id="问题列表"><a href="#问题列表" class="headerlink" title="问题列表"></a>问题列表</h1><ol>
<li>mAP计算</li>
<li>为什么要做特征归一化，标准化</li>
<li>常用的归一化和标准化方法</li>
<li>为什么线性回归使用mse作文损失函数</li>
<li>神经网络的深度和宽度分别是什么</li>
<li>下采样的作用？下采样的方法</li>
<li>上采样的原理和常用的方法</li>
<li>模型的FLOPs（计算量）是什么？怎么计算？</li>
<li>什么是深度可分离卷积？作用？</li>
<li>转置卷积的原理</li>
</ol>
<span id="more"></span>

<h1 id="问题及解答"><a href="#问题及解答" class="headerlink" title="问题及解答"></a>问题及解答</h1><h2 id="mAP计算"><a href="#mAP计算" class="headerlink" title="mAP计算"></a>mAP计算</h2><h2 id="为什么要做特征归一化，标准化"><a href="#为什么要做特征归一化，标准化" class="headerlink" title="为什么要做特征归一化，标准化"></a>为什么要做特征归一化，标准化</h2><h2 id="常用的归一化和标准化方法"><a href="#常用的归一化和标准化方法" class="headerlink" title="常用的归一化和标准化方法"></a>常用的归一化和标准化方法</h2><h2 id="为什么线性回归使用mse作文损失函数"><a href="#为什么线性回归使用mse作文损失函数" class="headerlink" title="为什么线性回归使用mse作文损失函数"></a>为什么线性回归使用mse作文损失函数</h2><h2 id="神经网络的深度和宽度分别是什么"><a href="#神经网络的深度和宽度分别是什么" class="headerlink" title="神经网络的深度和宽度分别是什么"></a>神经网络的深度和宽度分别是什么</h2><h2 id="下采样的作用？下采样的方法"><a href="#下采样的作用？下采样的方法" class="headerlink" title="下采样的作用？下采样的方法"></a>下采样的作用？下采样的方法</h2><h2 id="上采样的原理和常用的方法"><a href="#上采样的原理和常用的方法" class="headerlink" title="上采样的原理和常用的方法"></a>上采样的原理和常用的方法</h2><h2 id="模型的FLOPs（计算量）是什么？怎么计算？"><a href="#模型的FLOPs（计算量）是什么？怎么计算？" class="headerlink" title="模型的FLOPs（计算量）是什么？怎么计算？"></a>模型的FLOPs（计算量）是什么？怎么计算？</h2><h2 id="什么是深度可分离卷积？作用？"><a href="#什么是深度可分离卷积？作用？" class="headerlink" title="什么是深度可分离卷积？作用？"></a>什么是深度可分离卷积？作用？</h2><h2 id="转置卷积的原理"><a href="#转置卷积的原理" class="headerlink" title="转置卷积的原理"></a>转置卷积的原理</h2>]]></content>
      <categories>
        <category>学习</category>
        <category>cv算法</category>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>面试</tag>
        <tag>总结</tag>
        <tag>技巧</tag>
      </tags>
  </entry>
  <entry>
    <title>cv算法面试问题总结（六）</title>
    <url>/2021/10/01/cv%E7%AE%97%E6%B3%95%E9%9D%A2%E8%AF%95%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93%EF%BC%88%E5%85%AD%EF%BC%89/</url>
    <content><![CDATA[]]></content>
      <categories>
        <category>学习</category>
        <category>cv算法</category>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>面试</tag>
        <tag>总结</tag>
        <tag>技巧</tag>
      </tags>
  </entry>
  <entry>
    <title>cv算法面试问题总结（五）</title>
    <url>/2021/10/01/cv%E7%AE%97%E6%B3%95%E9%9D%A2%E8%AF%95%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93%EF%BC%88%E4%BA%94%EF%BC%89/</url>
    <content><![CDATA[]]></content>
      <categories>
        <category>学习</category>
        <category>cv算法</category>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>面试</tag>
        <tag>总结</tag>
        <tag>技巧</tag>
      </tags>
  </entry>
  <entry>
    <title>cv算法面试问题总结（四）</title>
    <url>/2021/10/01/cv%E7%AE%97%E6%B3%95%E9%9D%A2%E8%AF%95%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93%EF%BC%88%E5%9B%9B%EF%BC%89/</url>
    <content><![CDATA[<h1 id="问题列表"><a href="#问题列表" class="headerlink" title="问题列表"></a>问题列表</h1><ol>
<li><p>神经网络中的Addition/concatenate区别</p>
</li>
<li><p>目标检测中的anchor机制？作用？</p>
</li>
<li><p>BN（Batch Normalization）的原理和作用</p>
</li>
<li><p>随机梯度下降相比全局梯度下降的好处</p>
</li>
<li><p>网络初始化时给网络赋予0的权重，这个网络能正常训练吗？</p>
</li>
<li><p>梯度消失和梯度爆炸的原因？</p>
</li>
<li><p>深度学习为什么在计算机视觉领域这么好？</p>
</li>
<li><p>什么是正则化？L1正则化和L2正则化有什么区别</p>
</li>
<li><p>常用的模型压缩方式有哪些</p>
</li>
<li><p>残差网络的设计思想和作用</p>
<span id="more"></span>

<h1 id><a href="#" class="headerlink" title></a></h1></li>
</ol>
]]></content>
      <categories>
        <category>学习</category>
        <category>cv算法</category>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>面试</tag>
        <tag>总结</tag>
        <tag>技巧</tag>
      </tags>
  </entry>
  <entry>
    <title>hello-world</title>
    <url>/2021/09/29/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>
]]></content>
  </entry>
  <entry>
    <title>图像分类-EfficientNet&amp;EfficientDet[项目应用]</title>
    <url>/2021/09/29/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB-EfficientNet/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>背景问题：</p>
<p>方法简述：</p>
<p>优点：</p>
<p>缺点：</p>
<span id="more"></span>
]]></content>
      <categories>
        <category>学习</category>
        <category>cv算法</category>
        <category>深度学习</category>
        <category>图像分类</category>
        <category>目标检测</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>论文复现</tag>
        <tag>项目总结</tag>
        <tag>kaggle</tag>
      </tags>
  </entry>
  <entry>
    <title>图像分割-Unet[项目应用]</title>
    <url>/2021/09/29/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2-Unet/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>背景问题：</p>
<p>方法简述：</p>
<p>优点：</p>
<p>缺点：</p>
<span id="more"></span>
]]></content>
      <categories>
        <category>学习</category>
        <category>cv算法</category>
        <category>深度学习</category>
        <category>图像分割</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>论文复现</tag>
        <tag>项目总结</tag>
      </tags>
  </entry>
  <entry>
    <title>图像识别-AlexNet</title>
    <url>/2021/09/29/%E5%9B%BE%E5%83%8F%E8%AF%86%E5%88%AB-AlexNet/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>背景问题：</p>
<p>方法简述：</p>
<p>优点：</p>
<p>缺点：</p>
<span id="more"></span>
]]></content>
      <categories>
        <category>学习</category>
        <category>cv算法</category>
        <category>深度学习</category>
        <category>图像识别</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>论文复现</tag>
      </tags>
  </entry>
  <entry>
    <title>图像处理基础-滤波器</title>
    <url>/2021/09/29/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E5%9F%BA%E7%A1%80-%E6%BB%A4%E6%B3%A2%E5%99%A8/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>在图像处理中，经常需要对图像进行平滑去噪、锐化、边界增强等，这些功能可以通过滤波器filter来实现。</p>
<p>噪音的分类及如何添加</p>
<p>椒盐噪声：是一种随机出现的白点(salt)或者黑点(peppe)</p>
<p>高斯噪声：概率密度函数服从高斯分布（即正态分布）的一类噪声</p>
<p>常用滤波算法：</p>
<p>均值滤波器：把邻域内的平均值赋给中心元素。邻域内的像素权重是相等的；但不能保护细节，不能消除椒盐噪声</p>
<p>高斯滤波器：邻域内各个像素值不同权重的和，将中心点的权重增大，远离中心的的权重减小</p>
<p>中值滤波器：不使用权重，邻域内所有像素值的中间值来代替当前像素点的像素值</p>
<p>双边滤波器：同时考虑距离信息（距离越远，权重越小)和色彩信息（色彩差别越大，权重越小）</p>
<span id="more"></span>

<h1 id="滤波相关概念："><a href="#滤波相关概念：" class="headerlink" title="滤波相关概念："></a>滤波相关概念：</h1><p>图像的时域： 自变量是时间,即横轴是时间,纵轴是信号的变化。其动态信号x（t）是描述信号在不同时刻取值的函数</p>
<p>图像的频域：自变量是频率,即横轴是频率,纵轴是该频率信号的幅度,也就是通常说的频谱图。频谱图描述了信号的频率结构及频率与该频率信号幅度的关系</p>
<p>图像的频率： 图像的频率又称为空间频率，它反映了图像的像素灰度在空间中变化的情况</p>
<p>如何定量的测量图像的空间频率，最为常用的方法就是二维傅里叶变换。图像经过二维傅里叶变换后会形成与图像等大的复数矩阵，取其幅值形成幅度谱，取其相位形成相位谱。图像的频率能量分布主要体现在幅度谱中。通常习惯将低频成分放在幅度谱的中央，而将高频成分放在幅度谱边缘。</p>
<p>在图像频域里面，频率低的地方是比较平滑的，低频的区域中灰度值变化是比较小的；频率高的地方通常是边缘或者噪声，这些区域灰度值是突变的</p>
<p>高通滤波：让频率较高的部分通过，突出边缘等</p>
<p>低通滤波：保留频率比较低的部分，通常为平滑图像，弱化边缘，消除噪声</p>
<p>在时域中的滤波器和在频域中的滤波器组成了傅里叶变换对，这部分暂时不深入了。</p>
<h2 id="滤波器分类"><a href="#滤波器分类" class="headerlink" title="滤波器分类"></a>滤波器分类</h2><p>线性滤波： 对邻域中的像素的计算为线性运算时，如利用窗口函数进行平滑加权求和的运算，或者某种卷积运算，都可以称为线性滤波。常见的线性滤波有：方框滤波、均值滤波、高斯滤波、拉普拉斯滤波等等，通常线性滤波器之间只是模版的系数不同。</p>
<p>非线性滤波： 非线性滤波利用原始图像跟模版之间的一种逻辑关系得到结果，如最值滤波器，中值滤波器。比较常用的有中值滤波器和双边滤波器。</p>
<p>reference：<a href="https://blog.csdn.net/qq_44957388/article/details/105763906">https://blog.csdn.net/qq_44957388/article/details/105763906</a></p>
<h1 id="常见的滤波器："><a href="#常见的滤波器：" class="headerlink" title="常见的滤波器："></a>常见的滤波器：</h1><h2 id="均值滤波器"><a href="#均值滤波器" class="headerlink" title="均值滤波器"></a>均值滤波器</h2><p><img src="/2021/09/29/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E5%9F%BA%E7%A1%80-%E6%BB%A4%E6%B3%A2%E5%99%A8/%E5%9D%87%E5%80%BC.png"></p>
<h3 id="均值滤波的缺点"><a href="#均值滤波的缺点" class="headerlink" title="均值滤波的缺点"></a>均值滤波的缺点</h3><p>均值滤波本身存在着固有的缺陷，即它不能很好地保护图像细节，在图像去噪的同时也破坏了图像的细节部分，从而使图像变得模糊，不能很好地去除噪声点。特别是椒盐噪声。</p>
<h3 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h3><p>均值模糊可以模糊图像以便得到感兴趣物体的粗略描述，也就是说，去除图像中的不相关细节，其中“不相关”是指与滤波器模板尺寸相比较小的像素区域，从而对图像有一个整体的认知。即为了对感兴趣的物体得到一个大致的整体的描述而模糊一幅图像，忽略细小的细节。</p>
<h2 id="高斯滤波器"><a href="#高斯滤波器" class="headerlink" title="高斯滤波器"></a>高斯滤波器</h2><p><img src="/2021/09/29/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E5%9F%BA%E7%A1%80-%E6%BB%A4%E6%B3%A2%E5%99%A8/%E9%AB%98%E6%96%AF.png"></p>
<p><strong>应用：</strong> 高斯滤波是一种线性平滑滤波器，对于服从正态分布的噪声有很好的抑制作用。在实际场景中，我们通常会假定图像包含的噪声为高斯白噪声，所以在许多实际应用的预处理部分，都会采用高斯滤波抑制噪声，如传统车牌识别等。</p>
<h2 id="中值滤波器"><a href="#中值滤波器" class="headerlink" title="中值滤波器"></a>中值滤波器</h2><p><img src="/2021/09/29/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E5%9F%BA%E7%A1%80-%E6%BB%A4%E6%B3%A2%E5%99%A8/%E4%B8%AD%E5%80%BC.png"></p>
<p>中值滤波不再采用加权求和的方式计算滤波结果，它用邻域内所有像素值的中间值来代替当前像素点的像素值。</p>
<p>中值滤波会取当前像素点及其周围临近像素点的像素值，一般有奇数个像素点，将这些像素值排序，将排序后位于中间位置的像素值作为当前像素点的像素值。</p>
<p>中值滤波对于斑点噪声（speckle noise）和椒盐噪声（salt-and-pepper<br>noise）来说尤其有用，因为它不依赖于邻域内那些与典型值差别很大的值，而且噪声成分很难被选上，所以可以在几乎不影响原有图像的情况下去除全部噪声。但是由于需要进行排序操作，中值滤波的计算量较大。</p>
<p>中值滤波器在处理连续图像窗函数时与线性滤波器的工作方式类似，但滤波过程却不再是加权运算。</p>
<h2 id="双边滤波器"><a href="#双边滤波器" class="headerlink" title="双边滤波器"></a>双边滤波器</h2><p>双边滤波是综合考虑空间信息和色彩信息的滤波方式，在滤波过程中能有效的保护图像内的边缘信息。</p>
<p>双边滤波在计算某一个像素点的像素值时，同时考虑距离信息（距离越远，权重越小)和色彩信息（色彩差别越大，权重越小）。既能去除噪声，又能较好的保护边缘信息。</p>
<p><img src="/2021/09/29/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E5%9F%BA%E7%A1%80-%E6%BB%A4%E6%B3%A2%E5%99%A8/%E5%8F%8C%E8%BE%B9.png"></p>
<p>在双边滤波中，计算左侧白色区域的滤波结果时：</p>
<p>对于白色的点，权重较大<br>对于黑色的点，与白色的色彩差别较大（0和255），所以可以将他们的权重设置为0<br>计算右侧黑色区域的滤波结果时：</p>
<p>对于黑色的点，权重较大<br>对于白色的点，与黑色的色彩差别较大（255和0），所以可以将他们的权重设置为0<br>这样，左侧白色的滤波结果仍是白色，黑色的像素点权重为0，对它不会有影响；右侧黑色的滤波结果仍是黑色，白色的像素点权重为0，对它不会有影响。所以，双边滤波会将边缘信息保留。</p>
<p>参考链接：<a href="https://blog.csdn.net/qq_44957388/article/details/105763906">https://blog.csdn.net/qq_44957388/article/details/105763906</a></p>
]]></content>
      <categories>
        <category>学习</category>
        <category>cv算法</category>
      </categories>
      <tags>
        <tag>OpenCV</tag>
        <tag>基础操作</tag>
        <tag>噪音</tag>
        <tag>编程</tag>
        <tag>滤波器</tag>
      </tags>
  </entry>
  <entry>
    <title>图像识别-GoogLeNet</title>
    <url>/2021/09/29/%E5%9B%BE%E5%83%8F%E8%AF%86%E5%88%AB-GoogLeNet/</url>
    <content><![CDATA[<h1 id="GoogleNet"><a href="#GoogleNet" class="headerlink" title="GoogleNet"></a>GoogleNet</h1><h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>背景问题：</p>
<p>方法简述：</p>
<p>优点：</p>
<p>缺点：</p>
<span id="more"></span>
]]></content>
      <categories>
        <category>学习</category>
        <category>cv算法</category>
        <category>深度学习</category>
        <category>图像识别</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>论文复现</tag>
      </tags>
  </entry>
  <entry>
    <title>图像识别-ResNet</title>
    <url>/2021/09/29/%E5%9B%BE%E5%83%8F%E8%AF%86%E5%88%AB-ResNet/</url>
    <content><![CDATA[<h1 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a>ResNet</h1><h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>背景问题：</p>
<p>方法简述：</p>
<p>优点：</p>
<p>缺点：</p>
<span id="more"></span>
]]></content>
      <categories>
        <category>学习</category>
        <category>cv算法</category>
        <category>深度学习</category>
        <category>图像识别</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>论文复现</tag>
      </tags>
  </entry>
  <entry>
    <title>图像识别-VGGNet</title>
    <url>/2021/09/29/%E5%9B%BE%E5%83%8F%E8%AF%86%E5%88%AB-VGGNet/</url>
    <content><![CDATA[<h1 id="VGGNet"><a href="#VGGNet" class="headerlink" title="VGGNet"></a>VGGNet</h1><h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>背景问题：</p>
<p>方法简述：</p>
<p>优点：</p>
<p>缺点：</p>
<span id="more"></span>
]]></content>
      <categories>
        <category>学习</category>
        <category>cv算法</category>
        <category>深度学习</category>
        <category>图像识别</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>论文复现</tag>
      </tags>
  </entry>
  <entry>
    <title>新文章</title>
    <url>/2021/09/29/%E6%96%B0%E6%96%87%E7%AB%A0-1/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>背景问题：</p>
<p>方法简述：</p>
<p>优点：</p>
<p>缺点：</p>
<span id="more"></span>
]]></content>
      <tags>
        <tag>编程</tag>
      </tags>
  </entry>
  <entry>
    <title>新文章</title>
    <url>/2021/09/29/%E6%96%B0%E6%96%87%E7%AB%A0/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>背景问题：</p>
<p>方法简述：</p>
<p>优点：</p>
<p>缺点：</p>
<span id="more"></span>
]]></content>
      <categories>
        <category>空文档</category>
      </categories>
  </entry>
  <entry>
    <title>机器学习-K-means算法</title>
    <url>/2021/10/01/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-K-means%E7%AE%97%E6%B3%95/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>背景问题：</p>
<p>方法简述：</p>
<p>优点：</p>
<p>缺点：</p>
<span id="more"></span>

]]></content>
      <categories>
        <category>学习</category>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习-KNN邻近算法</title>
    <url>/2021/10/01/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-KNN%E9%82%BB%E8%BF%91%E7%AE%97%E6%B3%95/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>背景问题：</p>
<p>方法简述：</p>
<p>优点：</p>
<p>缺点：</p>
<span id="more"></span>

]]></content>
      <categories>
        <category>学习</category>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习-主成分分析PCA</title>
    <url>/2021/10/01/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90PCA/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>背景问题：</p>
<p>方法简述：</p>
<p>优点：</p>
<p>缺点：</p>
<span id="more"></span>

]]></content>
      <categories>
        <category>学习</category>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习-优化算法：牛顿法和拟牛顿法</title>
    <url>/2021/10/02/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95%EF%BC%9A%E7%89%9B%E9%A1%BF%E6%B3%95%E5%92%8C%E6%8B%9F%E7%89%9B%E9%A1%BF%E6%B3%95/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>机器学习-决策树Decision Tree</title>
    <url>/2021/09/30/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%86%B3%E7%AD%96%E6%A0%91Decision-Tree/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>背景问题：根据属性值做if … else …划分。问题在于先行选择哪个属性做划分是最优的？用什么标准来定量的去做这个选择？</p>
<p>方法简述：</p>
<p>ID3 利用信息增益来选择特征的。信息增益最大的特征来建立决策树的当前节点</p>
<p>C4.5 是根据“信息增益比”指标来做特征选择</p>
<p>CART(Classification and Regression Tree) 使用基尼系数</p>
<p>优点：</p>
<p>缺点：</p>
<p>决策树的回归用法</p>
<span id="more"></span>

<h1 id="ID3-决策算法"><a href="#ID3-决策算法" class="headerlink" title="ID3 决策算法"></a>ID3 决策算法</h1><h1 id="C4-5-决策算法"><a href="#C4-5-决策算法" class="headerlink" title="C4.5 决策算法"></a>C4.5 决策算法</h1><p>该算法解决了ID3算法中的一下问题：</p>
<ol>
<li><p>不能处理连续特征，</p>
<ol start="2">
<li>用信息增益作为标准容易偏向于取值较多的特征</li>
<li>缺失值处理的问</li>
<li>过拟合问题。</li>
</ol>
</li>
</ol>
<h1 id="CART算法"><a href="#CART算法" class="headerlink" title="CART算法"></a>CART算法</h1><p>该算法解决的是C4.5算法中的一下问题：</p>
<ol>
<li><p>由于决策树算法非常容易过拟合，因此对于生成的决策树必须要进行剪枝。</p>
</li>
<li><p>C4.5生成的是多叉树，即一个父节点可以有多个节点。很多时候，在计算机中二叉树模型会比多叉树运算效率高。如果采用二叉树，可以提高效率。</p>
</li>
<li><p>C4.5只能用于分类，如果能将决策树用于回归的话可以扩大它的使用范围。</p>
</li>
<li><p>C4.5由于使用了熵模型，里面有大量的耗时的对数运算,如果是连续值还有大量的排序运算。</p>
</li>
</ol>
]]></content>
      <categories>
        <category>学习</category>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习-回归树Regression-Tree</title>
    <url>/2021/10/02/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%9B%9E%E5%BD%92%E6%A0%91Regression-Tree/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>机器学习-奇异值分解SVD</title>
    <url>/2021/10/01/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%A5%87%E5%BC%82%E5%80%BC%E5%88%86%E8%A7%A3SVD/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>背景问题：</p>
<p>方法简述：</p>
<p>优点：</p>
<p>缺点：</p>
<span id="more"></span>

]]></content>
      <categories>
        <category>学习</category>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习-支持向量机(SVM)</title>
    <url>/2021/09/30/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA-SVM/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>背景问题：逻辑回归是找到可以划分数据的超平面，但是如何找到最优的超平面来划分数据集呢？</p>
<p>方法简述：</p>
<p>线性支持向量机：</p>
<p>非线性支持向量机：</p>
<p>核函数：</p>
<p>软间隔支持向量机：</p>
<p>优点：</p>
<p>​    小规模数据集训练，比LR和随机森林准确率高，泛化能力强。</p>
<p>​    在非线性特征空间中效果较好，有大量的核函数可以使用来解决非线性分类问题</p>
<p>​    在高维度特征的分类问题和回归问题很有效，即便是特征维度大于样本量的时候</p>
<p>​    不需要依赖全部样本，仅仅使用一部分样本做支持向量来完成超平面决策</p>
<p>​    无局部极小值问题；（相对于神经网络等算法）</p>
<p>缺点：</p>
<p>​    SVM不能产生分类的概率值，</p>
<p>​    SVM对大规模训练数据集是不适用的，计算量十分复杂</p>
<p>​    解决非线性问题时，找到一个合适的核函数是比较困难的</p>
<p>​    多分类问题支持不友好</p>
<p>​    对缺失数据敏感</p>
<p>​    ？？？对于核函数的高维映射解释力不强，尤其是径向基函数；</p>
<p>应用：</p>
<p>文本分类领域效果最好的机器学习算法，在工业界主要应用在网页分类、微博情感分析、舆情监控、用户评论挖掘、文本过滤等诸多领域</p>
<span id="more"></span>
]]></content>
      <categories>
        <category>学习</category>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习-朴素贝叶斯</title>
    <url>/2021/10/01/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>背景问题：</p>
<p>方法简述：</p>
<p>优点：</p>
<p>缺点：</p>
<span id="more"></span>

]]></content>
      <categories>
        <category>学习</category>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习-线性回归Linear Regression</title>
    <url>/2021/09/30/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92Linear-Regression/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>背景问题：找到一系列的参数W，使得f(x) = XW 和真实输出Y之间无限接近或一致。</p>
<p>方法简述：</p>
<p>优点：直接，快速，可解释性高</p>
<p>缺点：基于一系列假设；对异常值敏感；对数据分布敏感；存在多重共线性，自相关，异方差问题；容易出现过拟合与欠拟合问题；</p>
<span id="more"></span>

<h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><p>什么是回归模型？</p>
<p>回归是用来拟合输入变量和输出变量之间的关系，回归模型就是表示从输入变量到输出变量的映射函数。</p>
<p>所以线性回归的目标就是找到一系列的参数W，使得f(x) = XW 和真实输出Y之间无限接近或一致。</p>
<h1 id="理论"><a href="#理论" class="headerlink" title="理论"></a>理论</h1><h1 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h1><h1 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h1><h2 id="过拟合问题"><a href="#过拟合问题" class="headerlink" title="过拟合问题"></a>过拟合问题</h2><p>​    分析数据，重新做数据清冼，将征工程。</p>
<p>​    扩充数据集，收集更多数据。</p>
<p>​    减少特征数量 。</p>
<p>​    **采用正则化方法</p>
<h2 id="欠拟合问题"><a href="#欠拟合问题" class="headerlink" title="欠拟合问题 **"></a>欠拟合问题 **</h2><p>​    分析数据，增加特征维度</p>
<p>​    ** 增加多项式特征阶数</p>
<p>​    ** 减小正则项的超参数系数</p>
<p>​    ** 局部加权回归 </p>
<h2 id="多重共线性"><a href="#多重共线性" class="headerlink" title="多重共线性"></a>多重共线性</h2><p>​    那共线性会引发什么问题。。。。：</p>
<p>1、模型参数估计不准确，有时甚至会出现回归系数的符号与实际情况完全相反的情况，比如逻辑上应该系数为正的特征系数 算出来为负。</p>
<p>2、本应该显著的自变量不显著，本不显著的自变量却呈现出显著性（也就是说，无法从p-值的大小判断出变量是否显著——下面会给一个例子）</p>
<p>3、多重共线性使参数估计值的方差增大，模型参数不稳定，也就是每次训练得到的权重系数差异都比较大。</p>
<p>其实多重共线性这样理解会简单很多:</p>
<p>假设原始的线性回归公式为：</p>
<p>y=w1<em>x1+w2</em>x2+w3*x3</p>
<p>训练完毕的线性回归公式为：</p>
<p>y=5x1+7x2+10x3,</p>
<p>此时加入一个新特征x4，假设x4和x3高度相关，x4=2x3,则</p>
<p>y=w1<em>x1+w2</em>x2+w3<em>x3+w4</em>x4=w1<em>x1+w2</em>x2+(w3+2w4)*x3</p>
<p>因为我们之前拟合出来的最优的回归方程为：</p>
<p>y=5x1+7x2+10x3</p>
<p>显然w3+2w4可以合并成一个新的权重稀疏 w5，则</p>
<p>y=w1<em>x1+w2</em>x2+w5*x3,显然：</p>
<p>y=w1<em>x1+w2</em>x2+w3<em>x3和y=w1</em>x1+w2<em>x2+w5</em>x3是等价的。。。。</p>
<p>那么最终最优的模型应该也是 y=5x1+7x2+10x3</p>
<p>但是考虑到引入了x4，所以w4和w3的权重是分开计算出来的，这就导致了</p>
<p>w5=10=w3+2w4，显然这个方程有无穷多的解，比如w3=4，w4=3，或者w4=-1，w3=12等，因此导致了模型系数估计的不稳定并且可能会出现负系数的问题。</p>
<h1 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h1><h2 id="基于-Python-scikit-learn-工具包"><a href="#基于-Python-scikit-learn-工具包" class="headerlink" title="基于 Python scikit-learn 工具包"></a>基于 Python scikit-learn 工具包</h2><h2 id="Python自建实现"><a href="#Python自建实现" class="headerlink" title="Python自建实现"></a>Python自建实现</h2>]]></content>
      <categories>
        <category>学习</category>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习-逻辑回归Logistic Regression</title>
    <url>/2021/09/30/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92Logistic-Regression/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>逻辑回归 = 线性回归+逻辑分布 （如：sigmoid函数）</p>
<p>背景问题：</p>
<p>回归是连续的，分类是离散的，怎么将解决分类问题转化为解决回归问题</p>
<p>方法简述：先拟合决策边界(不局限于线性，还可以是多项式)，再建立这个边界与分类的概率联系，从而得到了二分类情况下的概率。分类是离散的，但类别的概率是连续的，让模型拟合概率相关的一个指标（对数几率函数logit）</p>
<p>优点：</p>
<p>​    (1)对率函数任意阶可导，具有很好的数学性质，许多现有的数值优化算法都可以用来求最优解，训练速度快;</p>
<p>​    (2)简单易理解，模型的可解释性非常好，从特征的权重可以看到不同的特征对最后结果的影响;</p>
<p>​    (3)适合二分类问题，不需要缩放输入特征</p>
<p>​    (4)内存资源占用小，因为只需要存储各个维度的特征值;</p>
<p>​    (5)直接对分类可能性进行建模，无需事先假设数据分布，避免了假设分布不准确所带来的问题</p>
<p>​    (6)以概率的形式输出，对许多利用概率辅助决策的任务很有用</p>
<p>缺点：</p>
<p>​    (1)不能用于解决非线性问题</p>
<p>​    (2)对多重共线性数据较为敏感;</p>
<p>​    (3)很难处理数据不平衡的问题;</p>
<p>​    (4)准确率并不是很高，因为形式非常的简单(非常类似线性模型)，很难去拟合数据的真实分布;</p>
<p>​    (5)无法筛选特征</p>
<span id="more"></span>

<p>线性分类器：线性分类器的学习目标便是要在n维的数据空间中找到一个超平面，使得这个超平面可以将已知的数据点分为两个类别</p>
<p>逻辑回归不是解决回归问题是用来解决分类问题，本质就是假设数据符合这个分布，然后使用极大似然估计做参数的估计</p>
<h1 id="Logistic-分布函数"><a href="#Logistic-分布函数" class="headerlink" title="Logistic 分布函数"></a>Logistic 分布函数</h1><p>Logistic 分布是一种连续型的分布，它形状与正态分布的形状相似，但是 Logistic 分布的尾部更长，所以我们可以使用 Logistic 分布来建模比正态分布具有更长尾部和更高波峰的数据分布。在深度学习中常用到的 Sigmoid 函数就是 Logistic 的分布函数在 <img src="https://www.zhihu.com/equation?tex=%5Cmu=0,+%5Cgamma=1" alt="[公式]"> 的特殊形式。</p>
<p>logistic regression是使用线性回归的预测值逼近真实分类的对数几率，优点是：</p>
<ol>
<li>直接对分类概率建模，无需进行假设，避免假设带来的不准确</li>
<li>不仅可预测出类别，还能得到类别的概率，</li>
<li>对数几率函数是任意阶可导的函数，有许多数值优化算法都是可以求出最优解的</li>
</ol>
<p>损失函数：</p>
<p>优化的主要目标是找到一个方向，参数朝这个方向移动之后使得损失函数的值能够减小，</p>
<p>优点：</p>
<p>(1)对率函数任意阶可导，具有很好的数学性质，许多现有的数值优化算法都可以用来求最优解，训练速度快;</p>
<p>(2)简单易理解，模型的可解释性非常好，从<a href="https://www.cda.cn/map/tezheng/">特征</a>的权重可以看到不同的<a href="https://www.cda.cn/map/tezheng/">特征</a>对最后结果的影响;</p>
<p>(3)适合二分类问题，不需要缩放输入<a href="https://www.cda.cn/map/tezheng/">特征</a>;</p>
<p>(4)内存资源占用小，因为只需要存储各个维度的<a href="https://www.cda.cn/map/tezheng/">特征</a>值;</p>
<p>(5)直接对分类可能性进行建模，无需事先假设数据分布，避免了假设分布不准确所带来的问题</p>
<p>(6)以概率的形式输出，而非知识0.1判定，对许多利用概率辅助决策的任务很有用</p>
<p>缺点：</p>
<p>(1)不能用<a href="https://www.cda.cn/map/luojihuigui/">逻辑回归</a>去解决非线性问题，因为Logistic的决策面试线性的;</p>
<p>(2)对多重共线性数据较为敏感;</p>
<p>(3)很难处理数据不平衡的问题;</p>
<p>(4)准确率并不是很高，因为形式非常的简单(非常类似线性模型)，很难去拟合数据的真实分布;</p>
<p>(5)<a href="https://www.cda.cn/map/luojihuigui/">逻辑回归</a>本身无法筛选<a href="https://www.cda.cn/map/tezheng/">特征</a>，有时会用gbdt来筛选<a href="https://www.cda.cn/map/tezheng/">特征</a>，然后再上<a href="https://www.cda.cn/map/luojihuigui/">逻辑回归</a>。</p>
]]></content>
      <categories>
        <category>学习</category>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习-集成算法-AdaBoost</title>
    <url>/2021/10/01/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E9%9B%86%E6%88%90%E7%AE%97%E6%B3%95-AdaBoost/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>背景问题：</p>
<p>方法简述：</p>
<p>优点：</p>
<p>缺点：</p>
<span id="more"></span>

]]></content>
      <categories>
        <category>学习</category>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习-集成算法-随机森林</title>
    <url>/2021/10/01/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E9%9B%86%E6%88%90%E7%AE%97%E6%B3%95-%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>背景问题：</p>
<p>方法简述：</p>
<p>优点：</p>
<p>缺点：</p>
<span id="more"></span>

]]></content>
      <categories>
        <category>学习</category>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习经典算法概览</title>
    <url>/2021/09/29/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%BB%8F%E5%85%B8%E7%AE%97%E6%B3%95%E6%A6%82%E8%A7%88/</url>
    <content><![CDATA[<p>这是一篇机器学习经典算法的简述，包含了线性回归、逻辑回归、支持向量机(SVM)、最近邻居(KNN)、决策树、k平均、随机森林、朴素贝叶斯、降维、梯度增强（更新ing）</p>
<span id="more"></span>

<h1 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h1><h1 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h1><h1 id="集成算法-Ensemble-algorithms"><a href="#集成算法-Ensemble-algorithms" class="headerlink" title="集成算法 Ensemble algorithms"></a>集成算法 Ensemble algorithms</h1><p>将多个弱模型组合，弱模型单独训练，将哥哥弱模型的预测结果以某种方式结合完成总体的预测</p>
<p>主要问题在于找到可以组合的弱模型和弱模型结果的结合方式</p>
<ol>
<li>Boosting</li>
<li>Bagging</li>
<li>AdaBoost</li>
<li>Blending</li>
<li>Random Forest随机森林</li>
<li>** GBM（Gradient Boosting Machine）梯度推进机</li>
<li>** GBRT（Gradient Boosted Regression Tree） 梯度提升回归树</li>
</ol>
<p>优点：结合最优秀的模型们，可以得到更加优秀的预测结果</p>
<p>缺点：多模型融合计算量大</p>
<h3 id><a href="#" class="headerlink" title></a></h3><h1 id="决策树算法（Decision-Tree-Algorithm"><a href="#决策树算法（Decision-Tree-Algorithm" class="headerlink" title="决策树算法（Decision Tree Algorithm)"></a>决策树算法（Decision Tree Algorithm)</h1><p>Step1 选择分裂节点：当根据某个属性的值不能明确分到样本哪个类别是就将此属性作为节点对其分裂</p>
<p>Step2 选择一个合适的阈值进行分裂使其分类的错误率最小</p>
<h2 id="1-ID3"><a href="#1-ID3" class="headerlink" title="1.ID3"></a>1.ID3</h2>]]></content>
      <categories>
        <category>学习</category>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>概览</tag>
      </tags>
  </entry>
  <entry>
    <title>模型评价方法及指标</title>
    <url>/2021/10/01/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BB%B7%E6%96%B9%E6%B3%95%E5%8F%8A%E6%8C%87%E6%A0%87/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>背景问题：</p>
<p>方法简述：</p>
<p>优点：</p>
<p>缺点：</p>
<span id="more"></span>
]]></content>
      <categories>
        <category>学习</category>
        <category>深度学习</category>
        <category>神经网络</category>
      </categories>
      <tags>
        <tag>总结</tag>
        <tag>编程</tag>
        <tag>调参</tag>
        <tag>优化</tag>
      </tags>
  </entry>
  <entry>
    <title>模型过拟合问题解决方法</title>
    <url>/2021/10/01/%E6%A8%A1%E5%9E%8B%E8%BF%87%E6%8B%9F%E5%90%88%E9%97%AE%E9%A2%98%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>背景问题：</p>
<p>方法简述：</p>
<p>优点：</p>
<p>缺点：</p>
<span id="more"></span>
]]></content>
      <categories>
        <category>学习</category>
        <category>深度学习</category>
        <category>神经网络</category>
      </categories>
      <tags>
        <tag>总结</tag>
        <tag>编程</tag>
        <tag>调参</tag>
        <tag>优化</tag>
      </tags>
  </entry>
  <entry>
    <title>深度学习-[2013]ZFNet网络模型</title>
    <url>/2021/10/01/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-2013-ZFNet%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>背景问题：</p>
<p>方法简述：</p>
<p>优点：</p>
<p>缺点：</p>
<span id="more"></span>
]]></content>
      <categories>
        <category>学习</category>
        <category>cv算法</category>
        <category>深度学习</category>
        <category>目标检测</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>论文复现</tag>
      </tags>
  </entry>
  <entry>
    <title>深度学习-Bottleneck结构的理解</title>
    <url>/2021/10/01/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Bottleneck%E7%BB%93%E6%9E%84%E7%9A%84%E7%90%86%E8%A7%A3/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>背景问题：</p>
<p>方法简述：</p>
<p>优点：</p>
<p>缺点：</p>
<span id="more"></span>
]]></content>
      <categories>
        <category>学习</category>
        <category>cv算法</category>
        <category>深度学习</category>
        <category>目标检测</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>论文复现</tag>
      </tags>
  </entry>
  <entry>
    <title>深度学习-Inception结构的理解</title>
    <url>/2021/10/01/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Inception%E7%BB%93%E6%9E%84%E7%9A%84%E7%90%86%E8%A7%A3/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>背景问题：</p>
<p>方法简述：</p>
<p>优点：</p>
<p>缺点：</p>
<span id="more"></span>
]]></content>
      <categories>
        <category>学习</category>
        <category>cv算法</category>
        <category>深度学习</category>
        <category>目标检测</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>论文复现</tag>
      </tags>
  </entry>
  <entry>
    <title>深度学习-LSTM长短期记忆</title>
    <url>/2021/10/01/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-LSTM%E9%95%BF%E7%9F%AD%E6%9C%9F%E8%AE%B0%E5%BF%86/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>深度学习-RNN循环神经网络</title>
    <url>/2021/10/01/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-RNN%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>深度学习-深度可分离卷积Depthwise Separable Conv</title>
    <url>/2021/10/01/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E6%B7%B1%E5%BA%A6%E5%8F%AF%E5%88%86%E7%A6%BB%E5%8D%B7%E7%A7%AFDepthwise-Separable-Conv/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>背景问题：</p>
<p>方法简述：</p>
<p>优点：</p>
<p>缺点：</p>
<span id="more"></span>



]]></content>
      <categories>
        <category>学习</category>
        <category>cv算法</category>
        <category>深度学习</category>
        <category>目标检测</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>论文复现</tag>
      </tags>
  </entry>
  <entry>
    <title>深度学习</title>
    <url>/2021/09/29/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/</url>
    <content><![CDATA[<p>#设置标签<br>在编辑文章的时候，tags:后面是设置标签的地方，如果有多个标签的话，可以用下面两种办法来设置：</p>
<h2 id="设置小标签"><a href="#设置小标签" class="headerlink" title="设置小标签"></a>设置小标签</h2><p>在编辑文章的时候，tags:后面是设置标签的地方，如果有多个标签的话，可以用下面两种办法来设置：</p>
]]></content>
      <categories>
        <category>机器学习</category>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>OpenCV</tag>
        <tag>编程</tag>
      </tags>
  </entry>
  <entry>
    <title>目标检测-R-CNN系列</title>
    <url>/2021/09/29/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B-R-CNN%E7%B3%BB%E5%88%97/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>背景问题：</p>
<p>方法简述：</p>
<p>优点：</p>
<p>缺点：</p>
<span id="more"></span>
]]></content>
      <categories>
        <category>学习</category>
        <category>cv算法</category>
        <category>深度学习</category>
        <category>目标检测</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>论文复现</tag>
      </tags>
  </entry>
  <entry>
    <title>目标检测-Retina-Net</title>
    <url>/2021/09/29/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B-Retina-Net/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>背景问题：</p>
<p>方法简述：</p>
<p>优点：</p>
<p>缺点：</p>
<span id="more"></span>
]]></content>
      <categories>
        <category>学习</category>
        <category>cv算法</category>
        <category>深度学习</category>
        <category>目标检测</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>论文复现</tag>
      </tags>
  </entry>
  <entry>
    <title>目标检测-SSD系列</title>
    <url>/2021/09/29/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B-SSD%E7%B3%BB%E5%88%97/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>背景问题：</p>
<p>方法简述：</p>
<p>优点：</p>
<p>缺点：</p>
<span id="more"></span>
]]></content>
      <categories>
        <category>学习</category>
        <category>cv算法</category>
        <category>深度学习</category>
        <category>目标检测</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>论文复现</tag>
      </tags>
  </entry>
  <entry>
    <title>目标检测-YOLOv2</title>
    <url>/2021/10/01/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B-YOLOv2/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>背景问题：</p>
<p>方法简述：</p>
<p>优点：</p>
<p>缺点：</p>
<span id="more"></span>

]]></content>
      <categories>
        <category>学习</category>
        <category>cv算法</category>
        <category>深度学习</category>
        <category>目标检测</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>论文复现</tag>
        <tag>YOLO</tag>
      </tags>
  </entry>
  <entry>
    <title>目标检测-YOLOv3</title>
    <url>/2021/10/01/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B-YOLOv3/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>背景问题：</p>
<p>方法简述：</p>
<p>优点：</p>
<p>缺点：</p>
<span id="more"></span>

]]></content>
      <categories>
        <category>学习</category>
        <category>cv算法</category>
        <category>深度学习</category>
        <category>目标检测</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>论文复现</tag>
        <tag>YOLO</tag>
      </tags>
  </entry>
  <entry>
    <title>目标检测-YOLOv4</title>
    <url>/2021/10/01/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B-YOLOv4/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>背景问题：</p>
<p>方法简述：</p>
<p>优点：</p>
<p>缺点：</p>
<span id="more"></span>

]]></content>
      <categories>
        <category>学习</category>
        <category>cv算法</category>
        <category>深度学习</category>
        <category>目标检测</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>论文复现</tag>
        <tag>YOLO</tag>
      </tags>
  </entry>
  <entry>
    <title>目标检测-YOLOv5</title>
    <url>/2021/10/01/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B-YOLOv5/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>背景问题：</p>
<p>方法简述：</p>
<p>优点：</p>
<p>缺点：</p>
<span id="more"></span>
]]></content>
      <categories>
        <category>学习</category>
        <category>cv算法</category>
        <category>深度学习</category>
        <category>目标检测</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>论文复现</tag>
        <tag>YOLO</tag>
      </tags>
  </entry>
  <entry>
    <title>目标检测-YOLOx</title>
    <url>/2021/10/01/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B-YOLOx/</url>
    <content><![CDATA[<ul>
<li><h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>背景问题：</p>
<p>方法简述：</p>
<p>优点：</p>
<p>缺点：</p>
<span id="more"></span></li>
</ul>
]]></content>
      <categories>
        <category>学习</category>
        <category>cv算法</category>
        <category>深度学习</category>
        <category>目标检测</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>论文复现</tag>
        <tag>YOLO</tag>
      </tags>
  </entry>
  <entry>
    <title>目标检测-YOLOv1</title>
    <url>/2021/09/29/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B-YOLO%E7%B3%BB%E5%88%97/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>背景问题：</p>
<p>方法简述：</p>
<p>优点：</p>
<p>缺点：</p>
<span id="more"></span>

<h1 id="YOLOv1"><a href="#YOLOv1" class="headerlink" title="YOLOv1"></a>YOLOv1</h1><ol>
<li><p>核心思想 回归思想</p>
<p>整张图作为网络的输入，在输出层回归Bounding Box的位置和Bounding Box的类别</p>
<p>相比Faster R-CNN虽然也是整张图输入，但Proposal+ classifier没变, 只是将Proposal也放在了CNN中</p>
</li>
<li><p>实现方法</p>
<ol>
<li>将整张图分为S*S的网格，如果某个object的中心落在这个网格中，这个网格就负责预测这个object</li>
<li>每个网格要预测B个bounding box，每个bounding box除了要回归自身的位置之外，还要附带预测一个confidence值。这个confidence代表了所预测的box中含有object的置信度和这个box预测的有多准两重信息</li>
<li>每个bounding Box 有五个值要预测(x, y, w, h)和confidence。每个网格还要预测一个类别信息，记为C类。则SxS个网格，每个网格要预测B个bounding box还要预测C个categories。输出就是S x S x (5*B+C)的一个tensor。</li>
<li><strong>简单的概括就是：</strong><ul>
<li>给个一个输入图像，首先将图像划分成7*7的网格；</li>
<li>对于每个网格，我们都预测2个边框（包括每个边框是目标的置信度以及每个边框区域在多个类别上的概率）；</li>
<li>根据上一步可以预测出7<em>7</em>2个目标窗口，然后根据阈值去除可能性比较低的目标窗口，最后NMS去除冗余窗口即可。</li>
</ul>
</li>
</ol>
</li>
<li><p>在YOLOv1的损失函数中：</p>
<p>Loss= 坐标预测的Loss + 含object的Box的confidence预测 + 不含object的Box的confidence预测 + 类别预测</p>
<p>只有当某个网格中有object的时候才对classification error进行惩罚。<br>只有当某个box predictor对某个ground truth box负责的时候，才会对box的coordinate error进行惩罚，而对哪个ground truth box负责就看其预测值和ground truth box的IoU是不是在那个cell的所有box中最大。<br>注：</p>
<p>YOLOv1方法模型训练依赖于物体识别标注数据，因此，对于非常规的物体形状或比例，YOLOv1的检测效果并不理想。<br>YOLOv1采用了多个下采样层，网络学到的物体特征并不精细，因此也会影响检测效果。<br>YOLOv1的loss函数中，大物体IOU误差和小物体IOU误差对网络训练中loss贡献值接近（虽然采用求平方根方式，但没有根本解决问题）。因此，对于小物体，小的IOU误差也会对网络优化过程造成很大的影响，从而降低了物体检测的定位准确性。<br>YOLO的缺点</p>
<p>YOLO对相互靠的很近的物体和很小的群体检测效果不好，这是因为一个网格中只预测了两个框，并且只属于一类；<br>同一类物体出现的新的不常见的长宽比和其他情况时，泛化能力偏弱；<br>由于损失函数的问题，定位误差是影响检测效果的主要原因。尤其是大小物体的处理上，还有待加强。</p>
</li>
</ol>
<h1 id="YOLOv2"><a href="#YOLOv2" class="headerlink" title="YOLOv2"></a>YOLOv2</h1><p>从预测<strong>更准确（Better）</strong>，<strong>速度更快（Faster）</strong>，<strong>识别对象更多（Stronger）</strong>这三个方面进行了改进。</p>
<p>采用联合训练算法的基本思路就是：同时在检测数据集和分类数据集上训练物体检测器（Object Detectors ），<strong>用检测数据集的数据学习物体的准确位置，用分类数据集的数据来增加分类的类别量、提升健壮性。</strong></p>
<ol>
<li><p>改进点</p>
<ol>
<li><p>BN层</p>
</li>
<li><p>高分辨率分类 224*224 -》448 * 448</p>
</li>
<li><h3 id="Convolution-with-anchor-boxes：-YOLOv1包含有全连接层，从而能直接预测Bounding-Boxes的坐标值。Faster-R-CNN算法只用卷积层与Region-Proposal-Network来预测Anchor-Box的偏移值与置信度，而不是直接预测坐标值，YOLOv2作者发现通过预测偏移量而不是坐标值能够简化问题，让神经网络学习起来更容易。"><a href="#Convolution-with-anchor-boxes：-YOLOv1包含有全连接层，从而能直接预测Bounding-Boxes的坐标值。Faster-R-CNN算法只用卷积层与Region-Proposal-Network来预测Anchor-Box的偏移值与置信度，而不是直接预测坐标值，YOLOv2作者发现通过预测偏移量而不是坐标值能够简化问题，让神经网络学习起来更容易。" class="headerlink" title="Convolution with anchor boxes： YOLOv1包含有全连接层，从而能直接预测Bounding Boxes的坐标值。Faster R-CNN算法只用卷积层与Region Proposal Network来预测Anchor Box的偏移值与置信度，而不是直接预测坐标值，YOLOv2作者发现通过预测偏移量而不是坐标值能够简化问题，让神经网络学习起来更容易。"></a>Convolution with anchor boxes： YOLOv1包含有全连接层，从而能直接预测Bounding Boxes的坐标值。Faster R-CNN算法只用卷积层与Region Proposal Network来预测Anchor Box的偏移值与置信度，而不是直接预测坐标值，YOLOv2作者发现通过预测偏移量而不是坐标值能够简化问题，让神经网络学习起来更容易。</h3><p>借鉴Faster RCNN的做法，YOLOv2也尝试采用先验框（anchor）。在每个grid预先设定一组不同大小和宽高比的边框，来覆盖整个图像的不同位置和多种尺度，这些先验框作为预定义的候选区在神经网络中将检测其中是否存在对象，以及微调边框的位置</p>
</li>
<li><p>YOLOv2的做法是对训练集中标注的边框进行K-means聚类分析，以寻找尽可能匹配样本的边框尺寸。</p>
<p><img src="/2021/09/29/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B-YOLO%E7%B3%BB%E5%88%97/1.png"></p>
</li>
</ol>
</li>
</ol>
<p>下面我们具体看看y1,y2,y3是如何而来的。<br>网络中作者进行了三次检测，分别是在32倍降采样，16倍降采样，8倍降采样时进行检测，这样在多尺度的feature map上检测跟SSD有点像。在网络中使用up-sample（上采样）的原因:网络越深的特征表达效果越好，比如在进行16倍降采样检测，如果直接使用第四次下采样的特征来检测，这样就使用了浅层特征，这样效果一般并不好。如果想使用32倍降采样后的特征，但深层特征的大小太小，因此YOLOv3使用了步长为2的up-sample（上采样），把32倍降采样得到的feature map的大小提升一倍，也就成了16倍降采样后的维度。同理8倍采样也是对16倍降采样的特征进行步长为2的上采样，这样就可以使用深层特征进行detection。</p>
<p>作者通过上采样将深层特征提取，其维度是与将要融合的特征层维度相同的（channel不同）。如下图所示，85层将13×13×256的特征上采样得到26×26×256，再将其与61层的特征拼接起来得到26×26×768。为了得到channel255，还需要进行一系列的3×3，1×1卷积操作，这样既可以提高非线性程度增加泛化性能提高网络精度，又能减少参数提高实时性。52×52×255的特征也是类似的过程。</p>
]]></content>
      <categories>
        <category>学习</category>
        <category>cv算法</category>
        <category>深度学习</category>
        <category>目标检测</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>论文复现</tag>
        <tag>YOLO</tag>
      </tags>
  </entry>
  <entry>
    <title>神经网络-learning rate学习率的优化总结</title>
    <url>/2021/10/01/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-learning-rate%E5%AD%A6%E4%B9%A0%E7%8E%87%E7%9A%84%E4%BC%98%E5%8C%96%E6%80%BB%E7%BB%93/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>背景问题：</p>
<p>方法简述：</p>
<p>优点：</p>
<p>缺点：</p>
<span id="more"></span>
]]></content>
      <categories>
        <category>学习</category>
        <category>深度学习</category>
        <category>神经网络</category>
      </categories>
      <tags>
        <tag>总结</tag>
        <tag>编程</tag>
        <tag>调参</tag>
        <tag>优化</tag>
      </tags>
  </entry>
  <entry>
    <title>神经网络-损失函数</title>
    <url>/2021/10/01/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>背景问题：</p>
<p>方法简述：</p>
<p>优点：</p>
<p>缺点：</p>
<span id="more"></span>
]]></content>
      <categories>
        <category>学习</category>
        <category>深度学习</category>
        <category>神经网络</category>
      </categories>
      <tags>
        <tag>总结</tag>
        <tag>编程</tag>
        <tag>调参</tag>
        <tag>优化</tag>
      </tags>
  </entry>
  <entry>
    <title>神经网络-权值初始化的方法</title>
    <url>/2021/10/01/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E6%9D%83%E5%80%BC%E5%88%9D%E5%A7%8B%E5%8C%96%E7%9A%84%E6%96%B9%E6%B3%95/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>神经网络-梯度优化方法</title>
    <url>/2021/10/01/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E6%A2%AF%E5%BA%A6%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>背景问题：</p>
<p>方法简述：</p>
<p>优点：</p>
<p>缺点：</p>
<span id="more"></span>
]]></content>
      <categories>
        <category>学习</category>
        <category>深度学习</category>
        <category>神经网络</category>
      </categories>
      <tags>
        <tag>总结</tag>
        <tag>编程</tag>
        <tag>调参</tag>
        <tag>优化</tag>
      </tags>
  </entry>
  <entry>
    <title>神经网络-梯度爆炸和梯度消失</title>
    <url>/2021/10/01/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8%E5%92%8C%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>神经网络-激活函数</title>
    <url>/2021/10/01/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>背景问题：</p>
<p>方法简述：</p>
<p>优点：</p>
<p>缺点：</p>
<span id="more"></span>
]]></content>
      <categories>
        <category>学习</category>
        <category>深度学习</category>
        <category>神经网络</category>
      </categories>
      <tags>
        <tag>总结</tag>
        <tag>编程</tag>
        <tag>调参</tag>
        <tag>优化</tag>
      </tags>
  </entry>
  <entry>
    <title>站内说明</title>
    <url>/2021/09/14/%E7%AB%99%E5%86%85%E8%AF%B4%E6%98%8E/</url>
    <content><![CDATA[<p>本blog是作为Leo同学的自我拓展过程中的自律自省记录</p>
<p>对该blog的计划是对一下类别和领域的学习与探索：</p>
<p>理论方面：</p>
<p>​    计算机视觉</p>
<p>​    深度学习算法</p>
<p>​    机器学习算法</p>
<p>Coding方面：</p>
<p>​    OpenCV</p>
<p>​    Python</p>
<p>​    Pytorch</p>
<p>​    </p>
]]></content>
      <categories>
        <category>其他</category>
      </categories>
      <tags>
        <tag>概览</tag>
      </tags>
  </entry>
  <entry>
    <title>计算机视觉CV算法概览（更新ing）</title>
    <url>/2021/08/29/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89CV%E7%AE%97%E6%B3%95%E6%A6%82%E8%A7%88/</url>
    <content><![CDATA[<h1 id="图像识别"><a href="#图像识别" class="headerlink" title="图像识别"></a>图像识别</h1><h2 id="1-AlexNet"><a href="#1-AlexNet" class="headerlink" title="1. AlexNet"></a>1. AlexNet</h2><h2 id="2-VGGNet"><a href="#2-VGGNet" class="headerlink" title="2. VGGNet"></a>2. VGGNet</h2><h2 id="3-GoogLeNet"><a href="#3-GoogLeNet" class="headerlink" title="3. GoogLeNet"></a>3. GoogLeNet</h2><h2 id="4-ResNet"><a href="#4-ResNet" class="headerlink" title="4. ResNet"></a>4. ResNet</h2><h2 id="5-DenseNet"><a href="#5-DenseNet" class="headerlink" title="5. DenseNet"></a>5. DenseNet</h2><h1 id="目标检测"><a href="#目标检测" class="headerlink" title="目标检测"></a>目标检测</h1><h2 id="1-R-CNN-系列"><a href="#1-R-CNN-系列" class="headerlink" title="1. R-CNN 系列"></a>1. R-CNN 系列</h2><h3 id="1-R-CNN"><a href="#1-R-CNN" class="headerlink" title="1. R-CNN"></a>1. R-CNN</h3><h3 id="2-Fast-R-CNN"><a href="#2-Fast-R-CNN" class="headerlink" title="2. Fast R-CNN"></a>2. Fast R-CNN</h3><h3 id="3-Faster-R-CNN"><a href="#3-Faster-R-CNN" class="headerlink" title="3. Faster R-CNN"></a>3. Faster R-CNN</h3><h2 id="2-Yolo系列"><a href="#2-Yolo系列" class="headerlink" title="2. Yolo系列"></a>2. Yolo系列</h2><h3 id="1-Yolo-V1"><a href="#1-Yolo-V1" class="headerlink" title="1.Yolo V1"></a>1.Yolo V1</h3><h3 id="2-Yolo-V2"><a href="#2-Yolo-V2" class="headerlink" title="2.Yolo V2"></a>2.Yolo V2</h3><h3 id="3-Yolo-V3"><a href="#3-Yolo-V3" class="headerlink" title="3.Yolo V3"></a>3.Yolo V3</h3><h3 id="4-Yolo-V4"><a href="#4-Yolo-V4" class="headerlink" title="4.Yolo V4"></a>4.Yolo V4</h3><h3 id="5-Yolo-V5"><a href="#5-Yolo-V5" class="headerlink" title="5.Yolo V5"></a>5.Yolo V5</h3><h2 id="3-SSD"><a href="#3-SSD" class="headerlink" title="3. SSD"></a>3. SSD</h2><h2 id="4-Retina-Net"><a href="#4-Retina-Net" class="headerlink" title="4. Retina-Net"></a>4. Retina-Net</h2><h1 id="图像分割"><a href="#图像分割" class="headerlink" title="图像分割"></a>图像分割</h1><h2 id="1-FCN"><a href="#1-FCN" class="headerlink" title="1. FCN"></a>1. FCN</h2><h2 id="2-Mask-R-CNN"><a href="#2-Mask-R-CNN" class="headerlink" title="2. Mask R-CNN"></a>2. Mask R-CNN</h2><h1 id="目标追踪"><a href="#目标追踪" class="headerlink" title="目标追踪"></a>目标追踪</h1><h2 id="1-Goturn"><a href="#1-Goturn" class="headerlink" title="1. Goturn"></a>1. Goturn</h2><h2 id="2"><a href="#2" class="headerlink" title="2."></a>2.</h2><h1 id="图像生成"><a href="#图像生成" class="headerlink" title="图像生成"></a>图像生成</h1><h2 id="1-GAN"><a href="#1-GAN" class="headerlink" title="1. GAN"></a>1. GAN</h2>]]></content>
      <categories>
        <category>学习</category>
        <category>cv算法</category>
      </categories>
      <tags>
        <tag>概览</tag>
      </tags>
  </entry>
  <entry>
    <title>轻量化网络-MobileNet</title>
    <url>/2021/09/29/%E8%BD%BB%E9%87%8F%E5%8C%96%E7%BD%91%E7%BB%9C-MobileNet/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>背景问题：</p>
<p>方法简述：</p>
<p>优点：</p>
<p>缺点：</p>
<span id="more"></span>
]]></content>
      <categories>
        <category>学习</category>
        <category>cv算法</category>
        <category>深度学习</category>
        <category>轻量化模型</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>论文复现</tag>
      </tags>
  </entry>
</search>
