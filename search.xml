<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>AI算法面试问题-机器学习（一）</title>
    <url>/2021/10/02/AI%E7%AE%97%E6%B3%95%E9%9D%A2%E8%AF%95%E9%97%AE%E9%A2%98-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%80%EF%BC%89/</url>
    <content><![CDATA[<h1 id="问题列表"><a href="#问题列表" class="headerlink" title="问题列表"></a>问题列表</h1><ol>
<li>详细说一下支持向量机的原理</li>
<li>数据归一化的原因</li>
<li>哪些算法不需要归一化处理</li>
<li>树形结构为什么不需要归一化</li>
<li>常用的距离计算有哪些，有什么区别</li>
<li>机器学习项目的流程</li>
<li>Logistic Regression 逻辑回归的原理</li>
<li>逻辑回归为什么要特征离散化</li>
<li>overfitting怎么解决</li>
<li>逻辑回归和SVM的区别与联系</li>
</ol>
<span id="more"></span>

<p>面试问题来源机器学习面试150题 <a href="https://www.zhihu.com/column/c_1284826692855771136">https://www.zhihu.com/column/c_1284826692855771136</a></p>
<h1 id="问题及解答"><a href="#问题及解答" class="headerlink" title="问题及解答"></a>问题及解答</h1><h2 id="详细说一下支持向量机的原理"><a href="#详细说一下支持向量机的原理" class="headerlink" title="详细说一下支持向量机的原理"></a>详细说一下支持向量机的原理</h2>]]></content>
      <categories>
        <category>学习</category>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>面试</tag>
        <tag>总结</tag>
        <tag>技巧</tag>
      </tags>
  </entry>
  <entry>
    <title>AI算法面试问题-机器学习（三）</title>
    <url>/2021/10/02/AI%E7%AE%97%E6%B3%95%E9%9D%A2%E8%AF%95%E9%97%AE%E9%A2%98-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%89%EF%BC%89/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>AI算法面试问题-机器学习（二）</title>
    <url>/2021/10/02/AI%E7%AE%97%E6%B3%95%E9%9D%A2%E8%AF%95%E9%97%AE%E9%A2%98-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%BA%8C%EF%BC%89/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>Global-Wheat-Detection全球小麦麦穗检测</title>
    <url>/2021/10/02/Global-Wheat-Detection%E5%85%A8%E7%90%83%E5%B0%8F%E9%BA%A6%E9%BA%A6%E7%A9%97%E6%A3%80%E6%B5%8B/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>Version1.0: 通过对 Faster R-CNN 网络进行微调来实现检测<br> Version2.0: 基于 EfficientDet 网络实现并结合 Pseudo-Labeling 策略进行改进</p>
<span id="more"></span>

<h1 id="分析数据"><a href="#分析数据" class="headerlink" title="分析数据"></a>分析数据</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="built_in">print</span>(torch.__version__)</span><br><span class="line"><span class="built_in">print</span>(torchvision.__version__)</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"></span><br><span class="line">train_label_df = pd.read_csv(<span class="string">&#x27;../input/global-wheat-detection/train.csv&#x27;</span>)</span><br><span class="line">sub_df = pd.read_csv(<span class="string">&#x27;../input/global-wheat-detection/sample_submission.csv&#x27;</span>)</span><br><span class="line"><span class="comment"># train_label_df.head()</span></span><br><span class="line"><span class="comment"># 删除不必要的列</span></span><br><span class="line"><span class="comment"># train_label_df = train_label_df.drop(columns=[&#x27;width&#x27;,&#x27;height&#x27;,&#x27;source&#x27;])</span></span><br><span class="line">train_label_df.head()</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 查看数据集以及标注情况</span></span><br><span class="line">train_path = <span class="string">&#x27;../input/global-wheat-detection/train&#x27;</span></span><br><span class="line">test_path = <span class="string">&#x27;../input/global-wheat-detection/test&#x27;</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;一共有%d 张图片&#x27;</span> % <span class="built_in">len</span>(os.listdir(train_path)))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;一共标注了%d张图片&#x27;</span> % train_label_df[<span class="string">&#x27;image_id&#x27;</span>].nunique() )</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;有 %d 张照片未被标注&#x27;</span> % (<span class="built_in">len</span>(os.listdir(train_path))-train_label_df[<span class="string">&#x27;image_id&#x27;</span>].nunique()))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;查看所有训练图片的大小 W * H：&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(train_label_df[<span class="string">&#x27;width&#x27;</span>].unique(),train_label_df[<span class="string">&#x27;height&#x27;</span>].unique())</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">一共有3422 张图片</span><br><span class="line">一共标注了3373张图片</span><br><span class="line">有 49 张照片未被标注</span><br><span class="line">查看所有训练图片的大小 W * H：</span><br><span class="line">[1024] [1024]</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 查看数据标注的分布情况</span></span><br><span class="line">counts = train_label_df[<span class="string">&#x27;image_id&#x27;</span>].value_counts()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;数据框数量的范围：<span class="subst">&#123;<span class="built_in">min</span>(counts)&#125;</span>-<span class="subst">&#123;<span class="built_in">max</span>(counts)&#125;</span>&#x27;</span>)</span><br><span class="line">sns.displot(counts)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;boxes&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;images&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;boxes vs. images&#x27;</span>)</span><br><span class="line">plt.show()</span><br><span class="line"><span class="comment"># 数据框数量的范围：1-116</span></span><br></pre></td></tr></table></figure>

<p><img src="/2021/10/02/Global-Wheat-Detection%E5%85%A8%E7%90%83%E5%B0%8F%E9%BA%A6%E9%BA%A6%E7%A9%97%E6%A3%80%E6%B5%8B/data_plt.png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 训练集和验证集划分 80%为训练集 20%为验证集</span></span><br><span class="line"></span><br><span class="line">image_ids = train_label_df[<span class="string">&#x27;image_id&#x27;</span>].unique()</span><br><span class="line"><span class="comment"># train_ids = image_ids[:-665]</span></span><br><span class="line"><span class="comment"># valid_ids = image_ids[-665:]</span></span><br><span class="line"></span><br><span class="line">split_num = <span class="built_in">int</span>(<span class="built_in">len</span>(image_ids) * <span class="number">0.8</span>)</span><br><span class="line">train_ids = image_ids[:split_num]</span><br><span class="line">valid_ids = image_ids[split_num:]</span><br><span class="line"></span><br><span class="line">train_df = train_label_df[train_label_df[<span class="string">&#x27;image_id&#x27;</span>].isin(train_ids)]</span><br><span class="line">valid_df = train_label_df[train_label_df[<span class="string">&#x27;image_id&#x27;</span>].isin(valid_ids)]</span><br><span class="line"><span class="built_in">print</span>(train_df.shape,valid_df.shape)</span><br><span class="line">train_df.head()</span><br></pre></td></tr></table></figure>

<h1 id="Version1-0-通过对-Faster-R-CNN-网络进行微调来实现检测"><a href="#Version1-0-通过对-Faster-R-CNN-网络进行微调来实现检测" class="headerlink" title="Version1.0: 通过对 Faster R-CNN 网络进行微调来实现检测"></a>Version1.0: 通过对 Faster R-CNN 网络进行微调来实现检测</h1><h2 id="自定义数据集类"><a href="#自定义数据集类" class="headerlink" title="自定义数据集类"></a>自定义数据集类</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">WheatDataset</span>(<span class="params">Dataset</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,dataframe,image_dir,transform=<span class="literal">None</span>,run_type=<span class="literal">True</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.image_ids = dataframe[<span class="string">&#x27;image_id&#x27;</span>].unique()</span><br><span class="line">        self.image_dir = image_dir</span><br><span class="line">        self.df = dataframe</span><br><span class="line">        self.transform = transform</span><br><span class="line">        self.run_type = run_type</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.image_ids.shape[<span class="number">0</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self,index</span>):</span></span><br><span class="line">        image_id = self.image_ids[index]</span><br><span class="line">        image = cv2.imread(<span class="string">f&#x27;<span class="subst">&#123;self.image_dir&#125;</span>/<span class="subst">&#123;image_id&#125;</span>.jpg&#x27;</span>,cv2.IMREAD_COLOR) <span class="comment"># 读取图片</span></span><br><span class="line">        image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB).astype(np.float32) <span class="comment"># 转RGB格式</span></span><br><span class="line">        image /= <span class="number">255.0</span> <span class="comment"># 图片归一化</span></span><br><span class="line">        <span class="keyword">if</span> self.transform <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> :</span><br><span class="line">            image = self.transform(image)</span><br><span class="line">        <span class="keyword">if</span>(self.run_type==<span class="literal">False</span>):  <span class="comment"># For test data</span></span><br><span class="line">            <span class="keyword">return</span> image, image_id</span><br><span class="line">        </span><br><span class="line">        records = self.df[self.df[<span class="string">&#x27;image_id&#x27;</span>]==image_id]</span><br><span class="line">        boxes = records[[<span class="string">&#x27;x&#x27;</span>,<span class="string">&#x27;y&#x27;</span>,<span class="string">&#x27;w&#x27;</span>,<span class="string">&#x27;h&#x27;</span>]].values</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 计算面积 </span></span><br><span class="line">        <span class="comment"># area (Tensor[N]): The area of the bounding box. This is used during evaluation with the COCO metric, </span></span><br><span class="line"><span class="comment">#         to separate the metric scores between small, medium and large boxes.</span></span><br><span class="line">        area = boxes[:,<span class="number">2</span>] * boxes[:,<span class="number">3</span>]</span><br><span class="line">        area = torch.as_tensor(area,dtype=torch.float32)</span><br><span class="line">        boxes[:,<span class="number">2</span>] = boxes[:,<span class="number">0</span>] + boxes[:,<span class="number">2</span>]</span><br><span class="line">        boxes[:,<span class="number">3</span>] = boxes[:,<span class="number">1</span>] + boxes[:,<span class="number">3</span>]</span><br><span class="line">        boxes = torch.as_tensor(boxes, dtype=torch.float32)</span><br><span class="line">        labels = torch.ones((records.shape[<span class="number">0</span>],), dtype=torch.int64)</span><br><span class="line"></span><br><span class="line">        iscrowd = torch.zeros((records.shape[<span class="number">0</span>],), dtype=torch.int64)</span><br><span class="line">        target = &#123;&#125;</span><br><span class="line">        target[<span class="string">&#x27;boxes&#x27;</span>] = boxes</span><br><span class="line">        target[<span class="string">&#x27;labels&#x27;</span>] = labels</span><br><span class="line">        target[<span class="string">&#x27;image_id&#x27;</span>] = torch.tensor([index])</span><br><span class="line">        target[<span class="string">&#x27;area&#x27;</span>] = area</span><br><span class="line">        target[<span class="string">&#x27;iscrowd&#x27;</span>] = iscrowd</span><br><span class="line">        <span class="keyword">return</span> image, target, image_id</span><br></pre></td></tr></table></figure>



<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Averager</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.current_total = <span class="number">0.0</span></span><br><span class="line">        self.iterations = <span class="number">0</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">send</span>(<span class="params">self,value</span>):</span></span><br><span class="line">        self.current_total += value</span><br><span class="line">        self.iterations += <span class="number">1</span></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">value</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">if</span> self.iterations == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">1.0</span>*self.current_total/self.iterations</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">reset</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.current_total = <span class="number">0.0</span></span><br><span class="line">        self.iterations = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过collate_fn函数可以对Dataset给出的样本做进一步的处理(任何你想要的处理)，原则上返回值应当是一个有结构的batch。而DataLoader每次迭代的返回值就是collate_fn的返回值。</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">collate_fn</span>(<span class="params">batch</span>):</span></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">tuple</span>(<span class="built_in">zip</span>(*batch))</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 数据加载</span></span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line">train_dir = <span class="string">&#x27;../input/global-wheat-detection/train&#x27;</span></span><br><span class="line">test_dir = <span class="string">&#x27;../input/global-wheat-detection/test&#x27;</span></span><br><span class="line">tans = transforms.Compose([transforms.ToTensor()])</span><br><span class="line">train_dataset = WheatDataset(train_df,train_dir,tans)</span><br><span class="line">valid_dataset = WheatDataset(valid_df,train_dir,tans)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 得到一个随机打乱的数字序列。</span></span><br><span class="line">indices = torch.randperm(<span class="built_in">len</span>(train_dataset)).tolist()</span><br><span class="line"></span><br><span class="line">train_data_loader = DataLoader(</span><br><span class="line">    train_dataset,</span><br><span class="line">    batch_size = <span class="number">16</span>,</span><br><span class="line">    shuffle = <span class="literal">False</span>,</span><br><span class="line">    num_workers = <span class="number">4</span>,</span><br><span class="line">    collate_fn = collate_fn</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">valid_data_loader = DataLoader(</span><br><span class="line">    valid_dataset,</span><br><span class="line">    batch_size = <span class="number">8</span>,</span><br><span class="line">    shuffle = <span class="literal">False</span>,</span><br><span class="line">    num_workers = <span class="number">4</span>,</span><br><span class="line">    collate_fn = collate_fn</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>模型定义</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(model.roi_heads.box_predictor)</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">FastRCNNPredictor(</span><br><span class="line">  (cls_score): Linear(in_features=1024, out_features=91, bias=True)</span><br><span class="line">  (bbox_pred): Linear(in_features=1024, out_features=364, bias=True)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>模型微调</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torchvision.models.detection.faster_rcnn <span class="keyword">import</span> FastRCNNPredictor</span><br><span class="line">num_classes = <span class="number">2</span>  <span class="comment"># wheat + background</span></span><br><span class="line"><span class="comment"># get number of input features for the classifier</span></span><br><span class="line">in_features = model.roi_heads.box_predictor.cls_score.in_features</span><br><span class="line">in_features</span><br><span class="line">model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)</span><br><span class="line"><span class="built_in">print</span>(model.roi_heads.box_predictor)</span><br></pre></td></tr></table></figure>

<p>微调之后的模型</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">FastRCNNPredictor(</span><br><span class="line">  (cls_score): Linear(in_features=1024, out_features=2, bias=True)</span><br><span class="line">  (bbox_pred): Linear(in_features=1024, out_features=8, bias=True)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>模型训练</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">device = torch.device(<span class="string">&#x27;cuda&#x27;</span>) <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> torch.device(<span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line">model.to(device)</span><br><span class="line"><span class="comment"># 创建优化器</span></span><br><span class="line">params = [p <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters() <span class="keyword">if</span> p.requires_grad]</span><br><span class="line">optimizer = torch.optim.SGD(params, lr=<span class="number">0.01</span>, momentum=<span class="number">0.9</span>, weight_decay=<span class="number">0.00001</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建学习率优化策略</span></span><br><span class="line">lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=<span class="number">1</span>, gamma=<span class="number">0.5</span>)</span><br><span class="line">num_epochs = <span class="number">5</span></span><br><span class="line">loss_hist = Averager()</span><br><span class="line">itr = <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    loss_hist.reset()</span><br><span class="line">    model.train()</span><br><span class="line">    <span class="keyword">for</span> images,targets,image_ids <span class="keyword">in</span> train_data_loader:</span><br><span class="line">        images = <span class="built_in">list</span>(image.to(device) <span class="keyword">for</span> image <span class="keyword">in</span> images)</span><br><span class="line">        targets = [&#123;k:v.to(device) <span class="keyword">for</span> k,v <span class="keyword">in</span> t.items()&#125; <span class="keyword">for</span> t <span class="keyword">in</span> targets]</span><br><span class="line">        loss_dict = model(images, targets)   <span class="comment"># 得到loss</span></span><br><span class="line">        losses = <span class="built_in">sum</span>(loss <span class="keyword">for</span> loss <span class="keyword">in</span> loss_dict.values())</span><br><span class="line">        loss_value = losses.item()</span><br><span class="line">        loss_hist.send(loss_value)  <span class="comment">#以每一batch的平均loss作为 </span></span><br><span class="line">        <span class="comment"># 梯度置零 反向传播 梯度计算</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        losses.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">      </span><br><span class="line">        <span class="keyword">if</span> itr % <span class="number">50</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&#x27;迭代了 #<span class="subst">&#123;itr&#125;</span> 次，当前损失为：<span class="subst">&#123;loss_value&#125;</span>&#x27;</span>)</span><br><span class="line">        itr += <span class="number">1</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">if</span> lr_scheduler <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        lr_scheduler.step()</span><br><span class="line">        </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;Epoch #<span class="subst">&#123;epoch&#125;</span> loss:<span class="subst">&#123;loss_hist.value&#125;</span>&#x27;</span>)</span><br><span class="line">torch.save(model.state_dict(), <span class="string">&#x27;./my_fasterrcnn_resnet50_fpn.pth&#x27;</span>)</span><br></pre></td></tr></table></figure>



<h2 id="预测阶段"><a href="#预测阶段" class="headerlink" title="预测阶段"></a>预测阶段</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 加载模型</span></span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torchvision.models.detection.faster_rcnn <span class="keyword">import</span> FastRCNNPredictor</span><br><span class="line"><span class="keyword">from</span> torchvision.models.detection <span class="keyword">import</span> FasterRCNN</span><br><span class="line"><span class="keyword">from</span> torchvision.models.detection.rpn <span class="keyword">import</span> AnchorGenerator</span><br><span class="line"></span><br><span class="line">device = torch.device(<span class="string">&#x27;cuda&#x27;</span>) <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> torch.device(<span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建没有预训练的Faster R-CNN model</span></span><br><span class="line">model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=<span class="literal">False</span>, pretrained_backbone=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">num_classes = <span class="number">2</span> <span class="comment"># wheat or not(background)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 输入类别数</span></span><br><span class="line">in_features = model.roi_heads.box_predictor.cls_score.in_features</span><br><span class="line"><span class="comment"># 替换预训练模型的头部</span></span><br><span class="line">model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)</span><br><span class="line"><span class="comment"># 加载权值文件</span></span><br><span class="line">WEIGHTS_FILE = <span class="string">&#x27;my_fasterrcnn_resnet50_fpn.pth&#x27;</span></span><br><span class="line">model.load_state_dict(torch.load(WEIGHTS_FILE, map_location=device))</span><br><span class="line"></span><br><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line">_ = model.to(device)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 加载预测数据并进行预测</span></span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line">submit = pd.read_csv(<span class="string">&quot;../input/global-wheat-detection/sample_submission.csv&quot;</span>)</span><br><span class="line">test_dataset = WheatDataset(submit,test_dir, tans,<span class="literal">False</span>)</span><br><span class="line">test_data_loader = DataLoader(test_dataset, batch_size=<span class="number">8</span>, shuffle=<span class="literal">False</span>)</span><br><span class="line">detection_threshold = <span class="number">0.45</span></span><br><span class="line">image_outputs = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> images, image_ids <span class="keyword">in</span> test_data_loader:</span><br><span class="line">  images = <span class="built_in">list</span>(image.to(device) <span class="keyword">for</span> image <span class="keyword">in</span> images)</span><br><span class="line">  outputs = model(images)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> image_id, output <span class="keyword">in</span> <span class="built_in">zip</span>(image_ids, outputs):</span><br><span class="line">    boxes = output[<span class="string">&#x27;boxes&#x27;</span>].data.cpu().numpy()</span><br><span class="line">    scores = output[<span class="string">&#x27;scores&#x27;</span>].data.cpu().numpy()</span><br><span class="line"></span><br><span class="line">    mask = scores &gt;= detection_threshold</span><br><span class="line">    boxes = boxes[mask].astype(np.int32)</span><br><span class="line">    scores = scores[mask]</span><br><span class="line"></span><br><span class="line">    image_outputs.append((image_id, boxes, scores))</span><br><span class="line">  </span><br><span class="line"><span class="comment"># 保存预测结果</span></span><br><span class="line">results = []</span><br><span class="line"><span class="keyword">for</span> image_id, boxes, scores <span class="keyword">in</span> image_outputs:</span><br><span class="line">  <span class="comment">#boxes = boxes_.copy()</span></span><br><span class="line">  boxes[:, <span class="number">2</span>] = boxes[:, <span class="number">2</span>] - boxes[:, <span class="number">0</span>]</span><br><span class="line">  boxes[:, <span class="number">3</span>] = boxes[:, <span class="number">3</span>] - boxes[:, <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">  result = &#123;</span><br><span class="line">    <span class="string">&#x27;image_id&#x27;</span>: image_id,</span><br><span class="line">    <span class="string">&#x27;PredictionString&#x27;</span>: format_prediction_string(boxes, scores)</span><br><span class="line">  &#125;</span><br><span class="line">  results.append(result)</span><br><span class="line">test_df = pd.DataFrame(results, columns=[<span class="string">&#x27;image_id&#x27;</span>, <span class="string">&#x27;PredictionString&#x27;</span>])</span><br><span class="line"><span class="comment"># print(test_df)</span></span><br><span class="line">test_df.to_csv(<span class="string">&#x27;submission.csv&#x27;</span>, index=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>

<h1 id="结合-Pseudo-Labeling-策略进行改进"><a href="#结合-Pseudo-Labeling-策略进行改进" class="headerlink" title="结合 Pseudo-Labeling 策略进行改进"></a>结合 Pseudo-Labeling 策略进行改进</h1><p><strong>具体思路</strong>如下：首先利用现有的标注数据，训练得到一个模型；利用训练得到的模型对无标注数据进行预测；然后将无标注数据的预测标签和数据加入训练集一起训练；</p>
<p>第一步：使用标签数据训练模型</p>
<p>第二步：使用训练的模型为不加标签的数据预测标签</p>
<p>第三步：同时使用pseudo和标签数据集重新训练模型</p>
<p>在第三步中训练的最终模型用于对测试数据的最终预测</p>
<p><img src="/2021/10/02/Global-Wheat-Detection%E5%85%A8%E7%90%83%E5%B0%8F%E9%BA%A6%E9%BA%A6%E7%A9%97%E6%A3%80%E6%B5%8B/pseudo.png"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Albumentations</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_train_transform</span>():</span></span><br><span class="line">    <span class="keyword">return</span> A.Compose([</span><br><span class="line">        A.Flip(<span class="number">0.5</span>),</span><br><span class="line">        ToTensorV2(p=<span class="number">1.0</span>)</span><br><span class="line">    ], bbox_params=&#123;<span class="string">&#x27;format&#x27;</span>: <span class="string">&#x27;pascal_voc&#x27;</span>, <span class="string">&#x27;label_fields&#x27;</span>: [<span class="string">&#x27;labels&#x27;</span>]&#125;)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_valid_transform</span>():</span></span><br><span class="line">    <span class="keyword">return</span> A.Compose([</span><br><span class="line">        ToTensorV2(p=<span class="number">1.0</span>)</span><br><span class="line">    ], bbox_params=&#123;<span class="string">&#x27;format&#x27;</span>: <span class="string">&#x27;pascal_voc&#x27;</span>, <span class="string">&#x27;label_fields&#x27;</span>: [<span class="string">&#x27;labels&#x27;</span>]&#125;)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_test_transform</span>():</span></span><br><span class="line">    <span class="keyword">return</span> A.Compose([</span><br><span class="line">        <span class="comment"># A.Resize(512, 512),</span></span><br><span class="line">        ToTensorV2(p=<span class="number">1.0</span>)</span><br><span class="line">    ])</span><br></pre></td></tr></table></figure>





]]></content>
  </entry>
  <entry>
    <title>OpenCV图像处理基础（一）</title>
    <url>/2021/09/29/OpenCV%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E5%9F%BA%E7%A1%80%EF%BC%88%E4%B8%80%EF%BC%89/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>数字图像基础</p>
<p>读、写、显示（图片、视频）</p>
<p>缩放、补边、裁剪</p>
<p>绘制形状</p>
<p>书写文字</p>
<span id="more"></span>

<h1 id="图像的数字表示"><a href="#图像的数字表示" class="headerlink" title="图像的数字表示"></a>图像的数字表示</h1><p>在计算机系统中存储的是每张图像每个像素点的值</p>
<p>灰度图，单通道图像也就是8位图像，每个像素点的值占8字节，00000000-11111111也就是0-255</p>
<p>RGB/BGR图像，是三通道图像，也就是24位图像，每个像素点的值占24字节，也就是三个通道每个通道的值占8字节0-255</p>
<p>灰度图也可以表示成三通道，不过三通道上的值是相等的</p>
<p>像素值为0表示黑色，255表示白色</p>
<p>32位图像在24位的基础上增加了一个alpha分量，该分量用于记录图像的透明度信息。</p>
<h1 id="读、写、显示"><a href="#读、写、显示" class="headerlink" title="读、写、显示"></a>读、写、显示</h1><h2 id="图片读取"><a href="#图片读取" class="headerlink" title="图片读取"></a>图片读取</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="comment"># 读取函数imread()</span></span><br><span class="line">img = cv2.imread(<span class="string">&quot;Path.jpg/png&quot;</span>)</span><br><span class="line"><span class="comment"># 显示函数 imshow()</span></span><br><span class="line">cv2.imshow(<span class="string">&#x27;窗口名称&#x27;</span>,img)</span><br><span class="line"><span class="comment"># 写入函数 imwrite()</span></span><br><span class="line">cv2.imwrite(<span class="string">&#x27;写入的文件名.jpg&#x27;</span>,img)</span><br><span class="line"><span class="comment"># cv2.IMWRITE_JPEG_QUALITY指定jpg质量，范围0到100，默认95，越高画质越好，文件越大</span></span><br><span class="line">cv2.imwrite(<span class="string">&#x27;写入的文件名.jpg&#x27;</span>, img, (cv2.IMWRITE_JPEG_QUALITY, <span class="number">80</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># cv2.IMWRITE_PNG_COMPRESSION指定png质量，范围0到9，默认3，越高文件越小，画质越差</span></span><br><span class="line">cv2.imwrite(<span class="string">&#x27;写入的文件名.png&#x27;</span>, img, (cv2.IMWRITE_PNG_COMPRESSION, <span class="number">5</span>))</span><br></pre></td></tr></table></figure>

<h2 id="显示图片需要对窗口进行设置"><a href="#显示图片需要对窗口进行设置" class="headerlink" title="显示图片需要对窗口进行设置"></a>显示图片需要对窗口进行设置</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">cv2.imshow(<span class="string">&#x27;窗口名称&#x27;</span>,img)</span><br><span class="line">cv2.waitkey(<span class="number">0</span>)</span><br><span class="line">cv2.destoryAllWindows()</span><br></pre></td></tr></table></figure>

<p><code>cv2.waitKey()</code> 是键盘绑定函数，可以设置毫秒级数值，如果是0会一直等待，也可以设置为指定字母，比如a</p>
<p><code>cv2.destoryAllWindows()</code>销毁所有创建的窗口</p>
<h2 id="视频读取，显示"><a href="#视频读取，显示" class="headerlink" title="视频读取，显示"></a>视频读取，显示</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">cap = cv2.VideoCapture(<span class="string">&#x27;视频名称.mp4/avi&#x27;</span>) <span class="comment"># 获取视频</span></span><br><span class="line"><span class="comment"># cap = cv2.VideoCapture(0) # 获取摄像头 0内置 1外置摄像头</span></span><br><span class="line"><span class="comment"># 视频显示：while循环显示视频的每一帧</span></span><br><span class="line"><span class="comment"># 使用read()获取的视频帧，将每一帧显示100ms,如果期间检测到‘q‘则退出，关闭窗口</span></span><br><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">  reg,frame = cap.read()</span><br><span class="line">  cv2.imshow(<span class="string">&#x27;caputre&#x27;</span>,frame)</span><br><span class="line">  <span class="keyword">if</span> cv2.waitkey(<span class="number">100</span>) <span class="keyword">and</span> <span class="number">0xFF</span> == <span class="built_in">ord</span>(<span class="string">&#x27;q&#x27;</span>):</span><br><span class="line">    <span class="keyword">break</span></span><br></pre></td></tr></table></figure>

<h1 id="缩放，补边，裁剪"><a href="#缩放，补边，裁剪" class="headerlink" title="缩放，补边，裁剪"></a>缩放，补边，裁剪</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 缩放成200x200的方形图像</span></span><br><span class="line">img_200x200 = cv2.resize(img, (<span class="number">200</span>, <span class="number">200</span>))</span><br><span class="line"><span class="comment"># 根据缩放比例来</span></span><br><span class="line"><span class="comment"># 默认线性插值 cv2.INTER_LINEAR</span></span><br><span class="line">img = cv2.resize(img,(<span class="number">0</span>,<span class="number">0</span>),fx=<span class="number">0.5</span>,fy=<span class="number">0.5</span>,interpolation=cv2.INTER_NEAREST) <span class="comment"># 邻近插值</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 上下左右分布补边50，40，30，20宽，常数填充 都是黑边</span></span><br><span class="line">img = cv2.copyMakeBorder(img, <span class="number">50</span>, <span class="number">40</span>, <span class="number">30</span>, <span class="number">20</span>,cv2.BORDER_CONSTANT, value=(<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>,<span class="number">0</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 根据切片来做裁剪</span></span><br><span class="line">imgCropped = img[<span class="number">50</span>:<span class="number">250</span>,<span class="number">120</span>:<span class="number">330</span>]</span><br></pre></td></tr></table></figure>



<h1 id="绘制形状"><a href="#绘制形状" class="headerlink" title="绘制形状"></a>绘制形状</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cv2.line() 图像，起点，终点，画笔颜色，线宽</span></span><br><span class="line">img = cv2.line(img,(<span class="number">0</span>,<span class="number">0</span>),(<span class="number">511</span>,<span class="number">511</span>),(<span class="number">255</span>,<span class="number">0</span>,<span class="number">0</span>),<span class="number">5</span>)  </span><br><span class="line"><span class="comment"># cv2.circle() 图像，圆心，半径，颜色，-1是表示一个封闭的图形</span></span><br><span class="line">img = cv2.circle(img,(<span class="number">447</span>,<span class="number">63</span>),<span class="number">63</span>,(<span class="number">0</span>,<span class="number">0</span>,<span class="number">255</span>),-<span class="number">1</span>)  </span><br><span class="line"><span class="comment"># cv2.rectangle() 图像，左上角，右下角，颜色，线宽</span></span><br><span class="line">img = cv2.rectangle(img,(<span class="number">384</span>,<span class="number">0</span>),(<span class="number">510</span>,<span class="number">128</span>),(<span class="number">0</span>,<span class="number">255</span>,<span class="number">0</span>),<span class="number">3</span>) </span><br><span class="line"><span class="comment"># cv2.ellipse()</span></span><br><span class="line">img = cv2.ellipse(img,(<span class="number">256</span>,<span class="number">256</span>),(<span class="number">100</span>,<span class="number">50</span>),<span class="number">0</span>,<span class="number">0</span>,<span class="number">180</span>,<span class="number">255</span>,-<span class="number">1</span>) </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h1 id="书写文字"><a href="#书写文字" class="headerlink" title="书写文字"></a>书写文字</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cv2.putText()</span></span><br><span class="line"><span class="comment"># 各参数依次是：图片，添加的文字，左上角坐标，字体，字体大小，颜色，字体粗细</span></span><br><span class="line">img = cv2.putText(img, <span class="built_in">str</span>, (<span class="number">123</span>,<span class="number">456</span>)), font, <span class="number">2</span>, (<span class="number">0</span>,<span class="number">255</span>,<span class="number">0</span>), <span class="number">3</span>)</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>学习</category>
        <category>cv算法</category>
      </categories>
      <tags>
        <tag>OpenCV</tag>
        <tag>基础操作</tag>
        <tag>噪音</tag>
        <tag>编程</tag>
      </tags>
  </entry>
  <entry>
    <title>OpenCV图像处理基础（三）</title>
    <url>/2021/09/29/OpenCV%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E5%9F%BA%E7%A1%80%EF%BC%88%E4%B8%89%EF%BC%89/</url>
    <content><![CDATA[<h1 id="灰度图"><a href="#灰度图" class="headerlink" title="灰度图"></a>灰度图</h1><h1 id="灰度直方图等"><a href="#灰度直方图等" class="headerlink" title="灰度直方图等"></a>灰度直方图等</h1><h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>背景问题：</p>
<p>方法简述：</p>
<p>优点：</p>
<p>缺点：</p>
<span id="more"></span>
]]></content>
      <categories>
        <category>学习</category>
        <category>cv算法</category>
      </categories>
      <tags>
        <tag>OpenCV</tag>
        <tag>基础操作</tag>
        <tag>噪音</tag>
        <tag>编程</tag>
        <tag>灰度图</tag>
      </tags>
  </entry>
  <entry>
    <title>Python中的*args和**kwargs</title>
    <url>/2021/10/28/Python%E4%B8%AD%E7%9A%84-args%E5%92%8C-kwargs/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>*和**的用法</p>
<p>*args和**kwargs的用法</p>
<span id="more"></span>



<h2 id="和-的用法"><a href="#和-的用法" class="headerlink" title="* 和 ** 的用法"></a>* 和 ** 的用法</h2><p>*和**还可用于列表、元组、字典的解包，但是*后的参数必须是一个迭代器。例如:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = (<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"><span class="comment"># (1, 2, 3)</span></span><br><span class="line"><span class="built_in">print</span>(*a)</span><br><span class="line"><span class="comment"># 1 2 3</span></span><br><span class="line">b = &#123;<span class="string">&#x27;a&#x27;</span>:<span class="number">1</span>,<span class="string">&#x27;n&#x27;</span>:<span class="number">2</span>&#125;</span><br><span class="line"><span class="built_in">print</span>(b)</span><br><span class="line"><span class="comment"># &#123;&#x27;a&#x27;: 1, &#x27;n&#x27;: 2&#125;</span></span><br><span class="line"><span class="built_in">print</span>(*b)</span><br><span class="line"><span class="comment"># a n</span></span><br></pre></td></tr></table></figure>

<h2 id="args和-kwargs的用法"><a href="#args和-kwargs的用法" class="headerlink" title="*args和**kwargs的用法"></a>*args和**kwargs的用法</h2><p>定义一个函数时，当输入的参数个数不确定的时候，不能够使用基于位置的参数表示方式，我们会使用(*args, **kwargs)来自动适应变参数和命名参数。这意味着，我们想往函数中输入多少个参数都行，分别以<strong>元组</strong>形式，<strong>字典</strong>形式传递参数。</p>
<p>**kwargs传入的参数必须形式上有key-value才可以</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span>(<span class="params">*args,**kwargs</span>):</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">type</span>(args),args)</span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">type</span>(kwargs),kwargs)</span><br><span class="line"></span><br><span class="line">test(<span class="string">&quot;test&quot;</span>,y=<span class="string">&#x27;test2&#x27;</span>)</span><br><span class="line"><span class="comment"># &lt;class &#x27;tuple&#x27;&gt; (&#x27;test&#x27;,)</span></span><br><span class="line"><span class="comment"># &lt;class &#x27;dict&#x27;&gt; &#123;&#x27;y&#x27;: &#x27;test2&#x27;&#125;</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a=(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">b=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]</span><br><span class="line">c=&#123;<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>&#125;</span><br><span class="line">d=&#123;<span class="number">1</span>:<span class="number">1</span>,<span class="number">2</span>:<span class="number">2</span>,<span class="number">3</span>:<span class="number">3</span>&#125;</span><br><span class="line"></span><br><span class="line">test(a)</span><br><span class="line"><span class="comment"># &lt;class &#x27;tuple&#x27;&gt; ((1, 2, 3),)  # 输出元组的元素为元组</span></span><br><span class="line"><span class="comment"># &lt;class &#x27;dict&#x27;&gt; &#123;&#125;</span></span><br><span class="line"></span><br><span class="line">test(b)</span><br><span class="line"><span class="comment"># &lt;class &#x27;tuple&#x27;&gt; ([1, 2, 3],)  # 输出元组的元素为列表</span></span><br><span class="line"><span class="comment"># &lt;class &#x27;dict&#x27;&gt; &#123;&#125;</span></span><br><span class="line"></span><br><span class="line">test(c)</span><br><span class="line"><span class="comment"># &lt;class &#x27;tuple&#x27;&gt; (&#123;1, 2, 3&#125;,)  # 输出元组的元素为集合</span></span><br><span class="line"><span class="comment"># &lt;class &#x27;dict&#x27;&gt; &#123;&#125;</span></span><br><span class="line"></span><br><span class="line">test(d)</span><br><span class="line"><span class="comment"># &lt;class &#x27;tuple&#x27;&gt; (&#123;1: 1, 2: 2, 3: 3&#125;,)  # 输出元组的元素为字典</span></span><br><span class="line"><span class="comment"># &lt;class &#x27;dict&#x27;&gt; &#123;&#125;</span></span><br><span class="line"></span><br><span class="line">test(a,b,c,d)</span><br><span class="line"><span class="comment"># &lt;class &#x27;tuple&#x27;&gt; ((1, 2, 3), [1, 2, 3], &#123;1, 2, 3&#125;, &#123;1: 1, 2: 2, 3: 3&#125;)  # 输出元组的元素可以为不同的类型</span></span><br><span class="line"><span class="comment"># &lt;class &#x27;dict&#x27;&gt; &#123;&#125;</span></span><br><span class="line"></span><br><span class="line">test(a,y=b,z=c)  <span class="comment"># 只有当以关键字参数传入时，**kwargs才能接受</span></span><br><span class="line"><span class="comment"># &lt;class &#x27;tuple&#x27;&gt; ((1, 2, 3),)</span></span><br><span class="line"><span class="comment"># &lt;class &#x27;dict&#x27;&gt; &#123;&#x27;y&#x27;: [1, 2, 3], &#x27;z&#x27;: &#123;1, 2, 3&#125;&#125;  # 输出字典</span></span><br></pre></td></tr></table></figure>



]]></content>
      <categories>
        <category>学习</category>
        <category>Python</category>
      </categories>
      <tags>
        <tag>基础操作</tag>
        <tag>编程</tag>
      </tags>
  </entry>
  <entry>
    <title>Python内置模块盘点</title>
    <url>/2021/11/02/Python%E5%9F%BA%E7%A1%80%E8%A1%A5%E5%85%85/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><h1 id="os模块"><a href="#os模块" class="headerlink" title="os模块"></a>os模块</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br></pre></td></tr></table></figure>





<h1 id="shutil模块"><a href="#shutil模块" class="headerlink" title="shutil模块"></a>shutil模块</h1><p>完成文件的复制，删除，移动，压缩，解压</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> shutil</span><br><span class="line">shutil.copy(src,dst)</span><br><span class="line">shutil.copy2()</span><br><span class="line">shutil.copyfile(src,dst)</span><br><span class="line">shutil.copymode()</span><br><span class="line">shutil.copystat()</span><br><span class="line">shutil.copytree()</span><br><span class="line">shutil.move()</span><br><span class="line">shutil.rmtree()</span><br></pre></td></tr></table></figure>

]]></content>
  </entry>
  <entry>
    <title>Pytorch框架基础（一）</title>
    <url>/2021/09/29/Pytorch%E6%A1%86%E6%9E%B6%E5%9F%BA%E7%A1%80%EF%BC%88%E4%B8%80%EF%BC%89/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>Tensor()创建</p>
<p>view()和reshape()</p>
<p>Broadcasting 广播机制</p>
<p>Numpy 和 Tensor互转</p>
<p>GPU/CPU计算</p>
<span id="more"></span>

<h1 id="Tensor-创建"><a href="#Tensor-创建" class="headerlink" title="Tensor() 创建"></a>Tensor() 创建</h1><p><a href="https://pytorch.zhangxiann.com/1-ji-ben-gai-nian/1.2-tensor-zhang-liang-jie-shao">https://pytorch.zhangxiann.com/1-ji-ben-gai-nian/1.2-tensor-zhang-liang-jie-shao</a></p>
<h2 id="直接创建"><a href="#直接创建" class="headerlink" title="直接创建"></a>直接创建</h2><h3 id="torch-tensor"><a href="#torch-tensor" class="headerlink" title="torch.tensor()"></a>torch.tensor()</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># torch.tensor(data,dtype=None,device=None,requires_grad=False)</span></span><br><span class="line"><span class="comment"># 复制data数据进行</span></span><br><span class="line"><span class="comment"># data 可以是list, tuple, numpy, scalar</span></span><br><span class="line"><span class="comment"># dtype 返回tensor的类型</span></span><br><span class="line"><span class="comment"># device 指定返回设备</span></span><br><span class="line"><span class="comment"># requires_grad 是否需要在计算中保留梯度信息</span></span><br><span class="line">torch.tensor([<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>])</span><br><span class="line"><span class="comment"># Out[24]: tensor([2, 3, 4])</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">torch.Tensor(<span class="number">2</span>,<span class="number">3</span>) <span class="comment">#给定shape</span></span><br><span class="line"><span class="comment"># Out[25]: </span></span><br><span class="line"><span class="comment"># tensor([[0.0000e+00, 1.5846e+29, 0.0000e+00],</span></span><br><span class="line"><span class="comment">#         [1.5846e+29, 1.2296e-09, 1.4013e-45]])</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#未初始化的tensor</span></span><br><span class="line">torch.empty(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line"><span class="comment"># Out[29]: </span></span><br><span class="line"><span class="comment"># tensor([[9.8091e-45, 0.0000e+00, 0.0000e+00],</span></span><br><span class="line"><span class="comment">#         [0.0000e+00, 0.0000e+00, 0.0000e+00]])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置默认的tensor的类型</span></span><br><span class="line">torch.set_default_tensor_type(torch.DoubleTensor)</span><br></pre></td></tr></table></figure>

<h3 id="torch-from-numpy-ndarry"><a href="#torch-from-numpy-ndarry" class="headerlink" title="torch.from_numpy(ndarry)"></a>torch.from_numpy(ndarry)</h3><p>​    共享内存地址</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = np.array([<span class="number">2</span>,<span class="number">3.3</span>])</span><br><span class="line">torch.from_numpy(a)</span><br><span class="line"><span class="comment"># Out[21]: tensor([2.0000, 3.3000], dtype=torch.float64)</span></span><br><span class="line"></span><br><span class="line">b = np.ones([<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">torch.from_numpy(b)</span><br><span class="line"><span class="comment"># Out[23]: </span></span><br><span class="line"><span class="comment"># tensor([[1., 1., 1.],</span></span><br><span class="line"><span class="comment">#         [1., 1., 1.]], dtype=torch.float64)</span></span><br></pre></td></tr></table></figure>

<h2 id="根据数值创建"><a href="#根据数值创建" class="headerlink" title="根据数值创建"></a>根据数值创建</h2><h3 id="torch-zeros-amp-torch-zeros-like"><a href="#torch-zeros-amp-torch-zeros-like" class="headerlink" title="torch.zeros()&amp; torch.zeros_like()"></a>torch.zeros()&amp; torch.zeros_like()</h3><h3 id="torch-ones-amp-torch-ones-like"><a href="#torch-ones-amp-torch-ones-like" class="headerlink" title="torch.ones() &amp; torch.ones_like()"></a>torch.ones() &amp; torch.ones_like()</h3><h3 id="torch-full-amp-torch-full-like"><a href="#torch-full-amp-torch-full-like" class="headerlink" title="torch.full() &amp; torch.full_like()"></a>torch.full() &amp; torch.full_like()</h3><h3 id="torch-arange"><a href="#torch-arange" class="headerlink" title="torch.arange()"></a>torch.arange()</h3><h3 id="torch-linspace"><a href="#torch-linspace" class="headerlink" title="torch.linspace()"></a>torch.linspace()</h3><h3 id="torch-logspace"><a href="#torch-logspace" class="headerlink" title="torch.logspace()"></a>torch.logspace()</h3><h3 id="torch-eye"><a href="#torch-eye" class="headerlink" title="torch.eye()"></a>torch.eye()</h3><h2 id="根据概率创建"><a href="#根据概率创建" class="headerlink" title="根据概率创建"></a>根据概率创建</h2><h3 id="torch-normal"><a href="#torch-normal" class="headerlink" title="torch.normal()"></a>torch.normal()</h3><h3 id="torch-randn-amp-torch-randn-like"><a href="#torch-randn-amp-torch-randn-like" class="headerlink" title="torch.randn() &amp; torch.randn_like()"></a>torch.randn() &amp; torch.randn_like()</h3><h3 id="torch-rand-amp-torch-rand-like"><a href="#torch-rand-amp-torch-rand-like" class="headerlink" title="torch.rand() &amp; torch.rand_like()"></a>torch.rand() &amp; torch.rand_like()</h3><h3 id="torch-randint-amp-torch-randint-like"><a href="#torch-randint-amp-torch-randint-like" class="headerlink" title="torch.randint() &amp; torch.randint_like()"></a>torch.randint() &amp; torch.randint_like()</h3><h3 id="torch-perm"><a href="#torch-perm" class="headerlink" title="torch.perm()"></a>torch.perm()</h3><h3 id="torch-bernoulli"><a href="#torch-bernoulli" class="headerlink" title="torch.bernoulli()"></a>torch.bernoulli()</h3><h1 id="Tensor的操作"><a href="#Tensor的操作" class="headerlink" title="Tensor的操作"></a>Tensor的操作</h1><h2 id="拼接"><a href="#拼接" class="headerlink" title="拼接"></a>拼接</h2><h3 id="torch-cat"><a href="#torch-cat" class="headerlink" title="torch.cat()"></a>torch.cat()</h3><p>在现有维度上做续接</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>], [<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]])</span><br><span class="line">b = torch.tensor([[<span class="number">10</span>, <span class="number">20</span>, <span class="number">30</span>], [<span class="number">40</span>, <span class="number">50</span>, <span class="number">60</span>], [<span class="number">70</span>, <span class="number">80</span>, <span class="number">90</span>]])</span><br><span class="line">c = torch.tensor([[<span class="number">100</span>, <span class="number">200</span>, <span class="number">300</span>], [<span class="number">400</span>, <span class="number">500</span>, <span class="number">600</span>], [<span class="number">700</span>, <span class="number">800</span>, <span class="number">900</span>]])</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.cat((a,b,c),<span class="number">0</span>)</span><br><span class="line"><span class="comment"># Out[45]: </span></span><br><span class="line"><span class="comment"># tensor([[  1,   2,   3],</span></span><br><span class="line"><span class="comment">#         [  4,   5,   6],</span></span><br><span class="line"><span class="comment">#         [  7,   8,   9],</span></span><br><span class="line"><span class="comment">#         [ 10,  20,  30],</span></span><br><span class="line"><span class="comment">#         [ 40,  50,  60],</span></span><br><span class="line"><span class="comment">#         [ 70,  80,  90],</span></span><br><span class="line"><span class="comment">#         [100, 200, 300],</span></span><br><span class="line"><span class="comment">#         [400, 500, 600],</span></span><br><span class="line"><span class="comment">#         [700, 800, 900]])</span></span><br><span class="line"></span><br><span class="line">torch.cat((a,b,c),<span class="number">1</span>)</span><br><span class="line"><span class="comment"># Out[46]:</span></span><br><span class="line"><span class="comment"># tensor([[  1,   2,   3,  10,  20,  30, 100, 200, 300],</span></span><br><span class="line"><span class="comment">#         [  4,   5,   6,  40,  50,  60, 400, 500, 600],</span></span><br><span class="line"><span class="comment">#         [  7,   8,   9,  70,  80,  90, 700, 800, 900]])</span></span><br></pre></td></tr></table></figure>

<h3 id="torch-stack"><a href="#torch-stack" class="headerlink" title="torch.stack()"></a>torch.stack()</h3><p>增加新的维度</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.stack((a,b,c),<span class="number">0</span>)</span><br><span class="line"><span class="comment"># Out[47]: </span></span><br><span class="line"><span class="comment"># tensor([[[  1,   2,   3],</span></span><br><span class="line"><span class="comment">#          [  4,   5,   6],</span></span><br><span class="line"><span class="comment">#          [  7,   8,   9]],</span></span><br><span class="line"><span class="comment">#         [[ 10,  20,  30],</span></span><br><span class="line"><span class="comment">#          [ 40,  50,  60],</span></span><br><span class="line"><span class="comment">#          [ 70,  80,  90]],</span></span><br><span class="line"><span class="comment">#         [[100, 200, 300],</span></span><br><span class="line"><span class="comment">#          [400, 500, 600],</span></span><br><span class="line"><span class="comment">#          [700, 800, 900]]])</span></span><br><span class="line"></span><br><span class="line">torch.stack((a,b,c),<span class="number">1</span>)</span><br><span class="line"><span class="comment"># Out[48]: </span></span><br><span class="line"><span class="comment"># tensor([[[  1,   2,   3],</span></span><br><span class="line"><span class="comment">#          [ 10,  20,  30],</span></span><br><span class="line"><span class="comment">#          [100, 200, 300]],</span></span><br><span class="line"><span class="comment">#         [[  4,   5,   6],</span></span><br><span class="line"><span class="comment">#          [ 40,  50,  60],</span></span><br><span class="line"><span class="comment">#          [400, 500, 600]],</span></span><br><span class="line"><span class="comment">#         [[  7,   8,   9],</span></span><br><span class="line"><span class="comment">#          [ 70,  80,  90],</span></span><br><span class="line"><span class="comment">#          [700, 800, 900]]])</span></span><br><span class="line"></span><br><span class="line">torch.stack((a,b,c),<span class="number">2</span>)</span><br><span class="line"><span class="comment"># Out[49]: </span></span><br><span class="line"><span class="comment"># tensor([[[  1,  10, 100],</span></span><br><span class="line"><span class="comment">#          [  2,  20, 200],</span></span><br><span class="line"><span class="comment">#          [  3,  30, 300]],</span></span><br><span class="line"><span class="comment">#         [[  4,  40, 400],</span></span><br><span class="line"><span class="comment">#          [  5,  50, 500],</span></span><br><span class="line"><span class="comment">#          [  6,  60, 600]],</span></span><br><span class="line"><span class="comment">#         [[  7,  70, 700],</span></span><br><span class="line"><span class="comment">#          [  8,  80, 800],</span></span><br><span class="line"><span class="comment">#          [  9,  90, 900]]])</span></span><br></pre></td></tr></table></figure>

<h2 id="分割"><a href="#分割" class="headerlink" title="分割"></a>分割</h2><h3 id="torch-chunk"><a href="#torch-chunk" class="headerlink" title="torch.chunk()"></a>torch.chunk()</h3><h3 id="torch-split"><a href="#torch-split" class="headerlink" title="torch.split()"></a>torch.split()</h3><h2 id="索引"><a href="#索引" class="headerlink" title="索引"></a>索引</h2><h3 id="torch-index-select"><a href="#torch-index-select" class="headerlink" title="torch.index_select()"></a>torch.index_select()</h3><h3 id="torch-mask-select"><a href="#torch-mask-select" class="headerlink" title="torch.mask_select()"></a>torch.mask_select()</h3><h2 id="变换"><a href="#变换" class="headerlink" title="变换"></a>变换</h2><h3 id="view-和reshape"><a href="#view-和reshape" class="headerlink" title="view()和reshape()"></a>view()和reshape()</h3><p> <code>view()</code>是内存共享的，改变了tensor的观察视角而已</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">a=torch.Tensor([[[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]]])</span><br><span class="line"></span><br><span class="line">b = a.view(<span class="number">1</span>,<span class="number">6</span>)</span><br><span class="line">c = a.view(<span class="number">3</span>,<span class="number">2</span>)</span><br><span class="line">d = a.view(<span class="number">6</span>,-<span class="number">1</span>)<span class="comment"># 不确定维数时用-1</span></span><br><span class="line"><span class="built_in">print</span>(b)</span><br><span class="line"><span class="built_in">print</span>(c)</span><br><span class="line"><span class="built_in">print</span>(d)</span><br><span class="line">d[<span class="number">0</span>] = <span class="number">9</span> <span class="comment"># 共享地址只要改变 其他都会改变</span></span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"><span class="built_in">print</span>(b)</span><br><span class="line"><span class="built_in">print</span>(c)</span><br><span class="line"><span class="built_in">print</span>(d)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>输出分别是:(不改变原有的tensor的shape)</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tensor([[<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>, <span class="number">4.</span>, <span class="number">5.</span>, <span class="number">6.</span>]])</span><br><span class="line">tensor([[<span class="number">1.</span>, <span class="number">2.</span>],</span><br><span class="line">        [<span class="number">3.</span>, <span class="number">4.</span>],</span><br><span class="line">        [<span class="number">5.</span>, <span class="number">6.</span>]])</span><br><span class="line">tensor([[<span class="number">1.</span>],</span><br><span class="line">        [<span class="number">2.</span>],</span><br><span class="line">        [<span class="number">3.</span>],</span><br><span class="line">        [<span class="number">4.</span>],</span><br><span class="line">        [<span class="number">5.</span>],</span><br><span class="line">        [<span class="number">6.</span>]])</span><br><span class="line"></span><br><span class="line">tensor([[[<span class="number">9.</span>, <span class="number">2.</span>, <span class="number">3.</span>],</span><br><span class="line">         [<span class="number">4.</span>, <span class="number">5.</span>, <span class="number">6.</span>]]])</span><br><span class="line">tensor([[<span class="number">9.</span>, <span class="number">2.</span>, <span class="number">3.</span>, <span class="number">4.</span>, <span class="number">5.</span>, <span class="number">6.</span>]])</span><br><span class="line">tensor([[<span class="number">9.</span>, <span class="number">2.</span>],</span><br><span class="line">        [<span class="number">3.</span>, <span class="number">4.</span>],</span><br><span class="line">        [<span class="number">5.</span>, <span class="number">6.</span>]])</span><br><span class="line">tensor([[<span class="number">9.</span>],</span><br><span class="line">        [<span class="number">2.</span>],</span><br><span class="line">        [<span class="number">3.</span>],</span><br><span class="line">        [<span class="number">4.</span>],</span><br><span class="line">        [<span class="number">5.</span>],</span><br><span class="line">        [<span class="number">6.</span>]])</span><br></pre></td></tr></table></figure>

<p><code>reshape()</code>不是共享的，可以理解为**reshape获取的是tensor克隆真实副本，<code>reshape()</code>可以改变该tensor结构，但是view()不可以，因为<code>reshape</code>等价于先<code>contiguous()</code>再<code>view()</code>,也就是数据内存给你撸顺了，再改变观察视角。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = torch.rand(<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">x = x.transpose(<span class="number">0</span>,<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(x.is_contiguous()) <span class="comment">#  x.permute(1,0)</span></span><br><span class="line"><span class="comment"># False 输出</span></span><br><span class="line"><span class="comment"># x.view(3,4) 地址不连续会报错</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">RuntimeError: invalid argument 2: view size is not compatible with input tensor&#x27;s....</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="comment"># 但是这样是可以的。</span></span><br><span class="line">x = x.contiguous()</span><br><span class="line">x.view(<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">x.reshape(<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">x.reshape(3,4) 这个操作</span></span><br><span class="line"><span class="string">等于x = x.contiguous().view()</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>

<h3 id="torch-transpose"><a href="#torch-transpose" class="headerlink" title="torch.transpose()"></a>torch.transpose()</h3><p>交换维度</p>
<h3 id="torch-t"><a href="#torch-t" class="headerlink" title="torch.t()"></a>torch.t()</h3><p>转置</p>
<h3 id="torch-squeeze"><a href="#torch-squeeze" class="headerlink" title="torch.squeeze()"></a>torch.squeeze()</h3><p>压缩长度为一维</p>
<h3 id="torch-unsqueeze"><a href="#torch-unsqueeze" class="headerlink" title="torch.unsqueeze()"></a>torch.unsqueeze()</h3><p>根据dim扩展维度</p>
<h1 id="Tensor的数学计算"><a href="#Tensor的数学计算" class="headerlink" title="Tensor的数学计算"></a>Tensor的数学计算</h1><h3 id="torch-add"><a href="#torch-add" class="headerlink" title="torch.add()"></a>torch.add()</h3><h3 id="torch-addcdiv"><a href="#torch-addcdiv" class="headerlink" title="torch.addcdiv()"></a>torch.addcdiv()</h3><h3 id="torch-addcmul"><a href="#torch-addcmul" class="headerlink" title="torch.addcmul()"></a>torch.addcmul()</h3><p>torch.lerp(star, end, weight) : 返回结果是out= star t+ (end-start) * weight<br>torch.rsqrt(input) : 返回平方根的倒数<br>torch.mean(input) : 返回平均值<br>torch.std(input) : 返回标准偏差<br>torch.prod(input) : 返回所有元素的乘积<br>torch.sum(input) : 返回所有元素的之和<br>torch.var(input) : 返回所有元素的方差<br>torch.tanh(input) ：返回元素双正切的结果<br>torch.equal(torch.Tensor(a), torch.Tensor(b)) ：两个张量进行比较，如果相等返回true<br>torch.max(input)： 返回输入元素的最大值<br>torch.min(input) ： 返回输入元素的最小值<br>element_size() ：返回单个元素的字节<br>torch.from_numpy(obj)，利用一个numpy的array创建Tensor。注意，若obj原来是1列或者1行，无论obj是否为2维，所生成的Tensor都是一阶的，若需要2阶的Tensor，需要利用view()函数进行转换。<br>torch.numel(obj)，返回Tensor对象中的元素总数。<br>torch.ones_like(input)，返回一个全1的Tensor，其维度与input相一致<br>torch.cat(seq, dim)，在给定维度上对输入的张量序列进行连接操作<br>torch.chunk（input, chunks, dim）在给定维度(轴)上将输入张量进行分块<br>torch.squeeze(input)，将input中维度数值为1的维度去除。可以指定某一维度。共享input的内存<br>torch.unsqeeze(input, dim)，在input目前的dim维度上增加一维<br>torch.clamp(input, min, max)，将input的值约束在min和max之间<br>torch.trunc(input)，将input的小数部分舍去 <a href="https://blog.csdn.net/lijiaming_99/article/details/114642093">https://blog.csdn.net/lijiaming_99/article/details/114642093</a></p>
<h1 id="Broadcasting-广播机制"><a href="#Broadcasting-广播机制" class="headerlink" title="Broadcasting 广播机制"></a>Broadcasting 广播机制</h1><p>当对两个形状不同的 Tensor 按元素运算 时，可能会触发⼴播（broadcasting）机制：<strong>先适当复制元素使这两个 Tensor 形状相同后再按元素运算</strong>。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">x = torch.arange(<span class="number">1</span>,<span class="number">3</span>).view(<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line">y = torch.arange(<span class="number">2</span>,<span class="number">5</span>).view(<span class="number">3</span>,<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line"><span class="built_in">print</span>(y)</span><br><span class="line"><span class="built_in">print</span>(x+y)</span><br></pre></td></tr></table></figure>

<p>结果：(x 中第⼀⾏的2个元素被⼴播 （复制）到了第⼆⾏和第三⾏，⽽ y 中第⼀列的3个元素被⼴播（复制）到了第⼆列)</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tensor([[<span class="number">1</span>, <span class="number">2</span>]])</span><br><span class="line">tensor([[<span class="number">2</span>],</span><br><span class="line">        [<span class="number">3</span>],</span><br><span class="line">        [<span class="number">4</span>]])</span><br><span class="line">tensor([[<span class="number">3</span>, <span class="number">4</span>],</span><br><span class="line">        [<span class="number">4</span>, <span class="number">5</span>],</span><br><span class="line">        [<span class="number">5</span>, <span class="number">6</span>]])</span><br></pre></td></tr></table></figure>

<h1 id="Numpy-和-Tensor互转"><a href="#Numpy-和-Tensor互转" class="headerlink" title="Numpy 和 Tensor互转"></a>Numpy 和 Tensor互转</h1><h2 id="Tensor转numpy"><a href="#Tensor转numpy" class="headerlink" title="Tensor转numpy"></a>Tensor转numpy</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">x = torch.ones(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">y = x.numpy()</span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line"><span class="built_in">print</span>(y)</span><br><span class="line">y += <span class="number">1</span></span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line"><span class="built_in">print</span>(y)</span><br></pre></td></tr></table></figure>

<p>输出： 地址共享</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tensor([[<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>]])</span><br><span class="line"></span><br><span class="line">[[<span class="number">1.</span> <span class="number">1.</span> <span class="number">1.</span>]</span><br><span class="line"> [<span class="number">1.</span> <span class="number">1.</span> <span class="number">1.</span>]]</span><br><span class="line"> </span><br><span class="line">tensor([[<span class="number">2.</span>, <span class="number">2.</span>, <span class="number">2.</span>],</span><br><span class="line">        [<span class="number">2.</span>, <span class="number">2.</span>, <span class="number">2.</span>]])</span><br><span class="line"></span><br><span class="line">[[<span class="number">2.</span> <span class="number">2.</span> <span class="number">2.</span>]</span><br><span class="line"> [<span class="number">2.</span> <span class="number">2.</span> <span class="number">2.</span>]]</span><br></pre></td></tr></table></figure>

<h2 id="Numpy转Tensor"><a href="#Numpy转Tensor" class="headerlink" title="Numpy转Tensor"></a>Numpy转Tensor</h2><p>torch.from_numpy(y) 和torch.tensor(y) 方法</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">z = torch.from_numpy(y)</span><br><span class="line">z2 = torch.tensor(y)</span><br><span class="line">y += <span class="number">2</span></span><br><span class="line"><span class="built_in">print</span>(z)</span><br><span class="line"><span class="built_in">print</span>(z2)</span><br></pre></td></tr></table></figure>

<p>输出：torch.from_numpy()<strong>共享相同的内存</strong>   torch.tensor(y) 方法会进⾏数据拷⻉</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tensor([[4., 4., 4.],</span><br><span class="line">        [4., 4., 4.]])</span><br><span class="line">        </span><br><span class="line">tensor([[2., 2., 2.],</span><br><span class="line">        [2., 2., 2.]])</span><br></pre></td></tr></table></figure>

<h1 id="GPU-CPU计算"><a href="#GPU-CPU计算" class="headerlink" title="GPU/CPU计算"></a>GPU/CPU计算</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    x = torch.ones(<span class="number">2</span>, <span class="number">3</span>,device=torch.device(<span class="string">&#x27;cuda&#x27;</span>))</span><br><span class="line">    y = y.to(<span class="string">&#x27;cuda&#x27;</span>)</span><br></pre></td></tr></table></figure>





]]></content>
      <categories>
        <category>学习</category>
        <category>Pytorch</category>
      </categories>
      <tags>
        <tag>基础操作</tag>
        <tag>编程</tag>
      </tags>
  </entry>
  <entry>
    <title>Pytorch框架基础（二）</title>
    <url>/2021/09/29/Pytorch%E6%A1%86%E6%9E%B6%E5%9F%BA%E7%A1%80%EF%BC%88%E4%BA%8C%EF%BC%89/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p><a href="https://www.cnblogs.com/Jason66661010/category/1831834.html?page=1">https://www.cnblogs.com/Jason66661010/category/1831834.html?page=1</a></p>
]]></content>
      <categories>
        <category>学习</category>
        <category>Pytorch</category>
      </categories>
      <tags>
        <tag>基础操作</tag>
        <tag>编程</tag>
      </tags>
  </entry>
  <entry>
    <title>cv算法面试问题总结（七）</title>
    <url>/2021/10/06/cv%E7%AE%97%E6%B3%95%E9%9D%A2%E8%AF%95%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93%EF%BC%88%E4%B8%83%EF%BC%89/</url>
    <content><![CDATA[<p>如何解决深度学习中模型训练效果不佳的情况？<br>神经网络中，是否隐藏层如果具有足够数量的单位，它就可以近似任何连续函数？<br>为什么更深的网络更好？<br>更多的数据是否有利于更深的神经网络？<br>不平衡数据是否会影响神经网络的分类效果？<br>无监督降维提供的是帮助还是摧毁？<br>是否可以将任何非线性作为激活函数?<br>批大小如何影响测试正确率？<br>初始化如何影响训练?<br>不同层的权重是否以不同的速度收敛？ </p>
<p>正则化如何影响权重？<br>什么是fine-tuning？<br>什么是边框回归Bounding-Box regression，以及为什么要做、怎么做<br>请阐述下Selective Search的主要思想<br>什么是非极大值抑制（NMS）？<br>什么是深度学习中的anchor？<br>CNN的特点以及优势<br>深度学习中有什么加快收敛/降低训练难度的方法？<br>请写出链式法则并证明<br>请写出Batch Normalization的计算方法及其应用</p>
<span id="more"></span>

]]></content>
  </entry>
  <entry>
    <title>cv算法面试问题总结（一）</title>
    <url>/2021/09/29/cv%E7%AE%97%E6%B3%95%E9%9D%A2%E8%AF%95%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93%EF%BC%88%E4%B8%80%EF%BC%89/</url>
    <content><![CDATA[<h1 id="问题列表："><a href="#问题列表：" class="headerlink" title="问题列表："></a>问题列表：</h1><ol>
<li>CNN的特点和优势</li>
<li>decent 反卷积的作用</li>
<li>dropout 的作用和实现机制</li>
<li>深度学习中有哪些可加快收敛（降低训练难度）的方法</li>
<li>什么导致过拟合，如何防止过拟合</li>
<li>**LSTM防止梯度弥散和爆炸 **</li>
<li>Local Connected Conv</li>
<li>神经网络权值初始化的方式和区别</li>
<li>简述Convolution,pooling,和Normalization在卷积中的作用</li>
<li>Dilated conv(空洞卷积)优缺点和应用场景</li>
</ol>
<span id="more"></span>

<h1 id="常见问题及回答总结"><a href="#常见问题及回答总结" class="headerlink" title="常见问题及回答总结"></a>常见问题及回答总结</h1><h2 id="CNN的特点和优势"><a href="#CNN的特点和优势" class="headerlink" title="CNN的特点和优势"></a>CNN的特点和优势</h2><ol>
<li>改变全链接为局部连接，可以提取局部特征</li>
<li>权值共享，减少参数数量，降低训练难度（空间和时间都降低）</li>
<li>降维， 通过池化或者卷积stride实现</li>
<li>多层次结构：降低层次的局部特征组合为较高层次的特征。不同层次的特征应对不同任务</li>
<li>可以完全共享也可以局部共享 （比如眼睛鼻子嘴巴等位置样式固定的可以用和脸部不一样的卷积核）</li>
</ol>
<h2 id="decent的作用"><a href="#decent的作用" class="headerlink" title="decent的作用"></a>decent的作用</h2><ol>
<li>CNN可视化，将conv中得到的feature map还原到像素空间，观察特定的feature map对哪些图案比较敏感</li>
<li>Upsampling 上采样</li>
<li>Unsupervised learning 重构图像</li>
</ol>
<h2 id="dropout-的作用和实现机制"><a href="#dropout-的作用和实现机制" class="headerlink" title="dropout 的作用和实现机制"></a>dropout 的作用和实现机制</h2><ol>
<li>原理是 在深度学习网络训练中对于输入层和隐藏层的神经网络单元 按照一定的概率P（伯努利分布） 暂时性！！！的丢弃。对于随机梯度下降，由于是随机丢弃，所以对于每一个mini-batch都在训练不同的网络</li>
<li>作用是防止过拟合，提高效果</li>
<li>缺点是 收敛速度减慢，由于每一次迭代只有一部分参数更新，导致梯度下降的速度减慢</li>
<li>测试时，每个权重值需要乘概率p <a href="https://zhuanlan.zhihu.com/p/38200980">https://zhuanlan.zhihu.com/p/38200980</a> dropout 必读</li>
</ol>
<h2 id="深度学习中有哪些可加快收敛（降低训练难度）的方法"><a href="#深度学习中有哪些可加快收敛（降低训练难度）的方法" class="headerlink" title="深度学习中有哪些可加快收敛（降低训练难度）的方法"></a>深度学习中有哪些可加快收敛（降低训练难度）的方法</h2><ol>
<li>bottleneck瓶颈结构： 在计算比较大的卷积层的之前使用一个1<em>1的卷积来压缩大卷积层输入特征图的通道数，用来减少计算量。大卷积层完成之后按照实际情况，有时候需要1</em>1的卷积来将大卷积层的输出特征图的通道数复原</li>
<li>残差 （还不是很明白）</li>
<li>学习率，步长，动量</li>
<li>优化方法</li>
<li>预训练</li>
</ol>
<h2 id="什么导致过拟合，如何防止过拟合"><a href="#什么导致过拟合，如何防止过拟合" class="headerlink" title="什么导致过拟合，如何防止过拟合"></a>什么导致过拟合，如何防止过拟合</h2><ol>
<li>过拟合的原因： 样本量过小，样本抽取不均衡噪音过多，参数太多模型复杂度高，权值迭代次数足够多拟合了训练样本中的噪音和不具代表性的特征</li>
<li>防止方法：<ol>
<li>数据增强 data argumentation</li>
<li>early stop</li>
<li>dropout</li>
<li>Batch Normalization</li>
<li>使用更简单的模型</li>
<li>参数正则化： 通过一定方法使神经网络中的部分神经元关闭，降低模型的复杂程度。正则化就是为了减小测试误差的，虽然有的时候可能会以增大训练误差为代价。正则化是为了显著的减小方差而较小的增大偏差。也就是提升泛化能力</li>
<li>加噪音（输入数噪声，权重噪声，响应结果里面加噪声）</li>
<li>freeze预训练网络中的某几层 #代码 <a href="https://blog.csdn.net/weixin_41712499/article/details/111295683">https://blog.csdn.net/weixin_41712499/article/details/111295683</a></li>
<li>结合多个模型 <ol>
<li>Bagging: 可以将其理解为一个分段函数，使用不同的模型拟合不同部分的训练集。如随机森林就是训练了一堆互不关联的决策树</li>
<li>Boosting：使用多个模型最后将模型的输出加权平均</li>
</ol>
</li>
</ol>
</li>
</ol>
<h2 id="LSTM防止梯度弥散和爆炸"><a href="#LSTM防止梯度弥散和爆炸" class="headerlink" title="LSTM防止梯度弥散和爆炸"></a>LSTM防止梯度弥散和爆炸</h2><h2 id="Local-Connected-Conv"><a href="#Local-Connected-Conv" class="headerlink" title="Local Connected Conv"></a>Local Connected Conv</h2><ol>
<li>人脸在不同的区域存在不同的特征（眼睛／鼻子／嘴的分布位置相对固定），当不存在全局的局部特征分布时，Local-Conv更适合特征的提取</li>
</ol>
<h2 id="神经网络权值初始化的方式和区别"><a href="#神经网络权值初始化的方式和区别" class="headerlink" title="神经网络权值初始化的方式和区别"></a>神经网络权值初始化的方式和区别</h2><p>​    详见权值初始化blog</p>
<ol>
<li>常量初始化</li>
<li>随机高斯初始化： 将权重初始化为固定的均值和方差（例如均值取0，方差取0.01）如果初始的方差小，如0.1，就会导致在前向传播过程中，不同的层的输入不断减小。会导致权重的更新速度很慢很慢。初始化的方差如果太大，就会使得每一层的输出越来越大。形如tanh激活函数，就会容易导致梯度饱和的现象</li>
<li>均匀分布初始化</li>
<li>Xavier初始化</li>
<li>双线性初始化</li>
<li>msra初始化</li>
</ol>
<h2 id="简述Convolution-pooling-和Normalization在卷积中的作用"><a href="#简述Convolution-pooling-和Normalization在卷积中的作用" class="headerlink" title="简述Convolution,pooling,和Normalization在卷积中的作用"></a>简述Convolution,pooling,和Normalization在卷积中的作用</h2><ol>
<li>Convolution：通过卷积核运算，提取卷积核希望提取的特征</li>
<li>pooling层 减小图像大小，加速计算，使其检测出的特征更加健壮</li>
<li>fully connected： 用来做分类</li>
<li>激活层 使得函数更加复杂</li>
<li>Batch Normalization：<ol>
<li>背景：深度学习中数据分布在某一层开始有明显的偏移，网络加深会加剧，导致模型优化的难度增加，也就是梯度弥散。</li>
<li>方法：在每个卷积层之后重新调整数据分布，解决梯度问题。在网络每一层输入前，先做归一化处理，加一个归一化层。但并不是盲目加。变换重构，引入可学习的参数r,B, 从而控制归一化尽可能不影响特征提取</li>
<li>好处：降低对参数初始化的依赖。可使用更高的学习率，加速训练。一定程度上增加了泛化能力，可以取代dropout</li>
</ol>
</li>
</ol>
<h2 id="Dilated-conv-空洞卷积-优缺点和应用场景"><a href="#Dilated-conv-空洞卷积-优缺点和应用场景" class="headerlink" title="Dilated conv(空洞卷积)优缺点和应用场景"></a>Dilated conv(空洞卷积)优缺点和应用场景</h2><ol>
<li>背景： FCN全卷积的语义分割问题中，需要输入图像和输出图像的size保持一致。若使用池化层，会降低size之后需要上采样，导致特征信息丢失精度降低。若使用较小的卷积核，可保持size一致，但需要增大特征图通道数会导致计算量较大。</li>
<li>空洞卷积，在特征图上进行0填充扩大特征图size，既能保持感受野又能保持计算点不变</li>
</ol>
]]></content>
      <categories>
        <category>学习</category>
        <category>cv算法</category>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>面试</tag>
        <tag>总结</tag>
        <tag>技巧</tag>
      </tags>
  </entry>
  <entry>
    <title>cv算法面试问题总结（三）</title>
    <url>/2021/09/29/cv%E7%AE%97%E6%B3%95%E9%9D%A2%E8%AF%95%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93%EF%BC%88%E4%B8%89%EF%BC%89/</url>
    <content><![CDATA[<h1 id="问题列表"><a href="#问题列表" class="headerlink" title="问题列表"></a>问题列表</h1><ol>
<li>mAP计算</li>
<li>为什么要做特征归一化，标准化</li>
<li>常用的归一化和标准化方法</li>
<li>为什么线性回归使用mse作文损失函数</li>
<li>神经网络的深度和宽度分别是什么</li>
<li>下采样的作用？下采样的方法</li>
<li>上采样的原理和常用的方法</li>
<li>模型的FLOPs（计算量）是什么？怎么计算？</li>
<li>什么是深度可分离卷积？作用？</li>
<li>转置卷积的原理</li>
</ol>
<span id="more"></span>

<h1 id="问题及解答"><a href="#问题及解答" class="headerlink" title="问题及解答"></a>问题及解答</h1><h2 id="mAP计算"><a href="#mAP计算" class="headerlink" title="mAP计算"></a>mAP计算</h2><h2 id="为什么要做特征归一化，标准化"><a href="#为什么要做特征归一化，标准化" class="headerlink" title="为什么要做特征归一化，标准化"></a>为什么要做特征归一化，标准化</h2><h2 id="常用的归一化和标准化方法"><a href="#常用的归一化和标准化方法" class="headerlink" title="常用的归一化和标准化方法"></a>常用的归一化和标准化方法</h2><h2 id="为什么线性回归使用mse作文损失函数"><a href="#为什么线性回归使用mse作文损失函数" class="headerlink" title="为什么线性回归使用mse作文损失函数"></a>为什么线性回归使用mse作文损失函数</h2><h2 id="神经网络的深度和宽度分别是什么"><a href="#神经网络的深度和宽度分别是什么" class="headerlink" title="神经网络的深度和宽度分别是什么"></a>神经网络的深度和宽度分别是什么</h2><h2 id="下采样的作用？下采样的方法"><a href="#下采样的作用？下采样的方法" class="headerlink" title="下采样的作用？下采样的方法"></a>下采样的作用？下采样的方法</h2><h2 id="上采样的原理和常用的方法"><a href="#上采样的原理和常用的方法" class="headerlink" title="上采样的原理和常用的方法"></a>上采样的原理和常用的方法</h2><h2 id="模型的FLOPs（计算量）是什么？怎么计算？"><a href="#模型的FLOPs（计算量）是什么？怎么计算？" class="headerlink" title="模型的FLOPs（计算量）是什么？怎么计算？"></a>模型的FLOPs（计算量）是什么？怎么计算？</h2><h2 id="什么是深度可分离卷积？作用？"><a href="#什么是深度可分离卷积？作用？" class="headerlink" title="什么是深度可分离卷积？作用？"></a>什么是深度可分离卷积？作用？</h2><h2 id="转置卷积的原理"><a href="#转置卷积的原理" class="headerlink" title="转置卷积的原理"></a>转置卷积的原理</h2>]]></content>
      <categories>
        <category>学习</category>
        <category>cv算法</category>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>面试</tag>
        <tag>总结</tag>
        <tag>技巧</tag>
      </tags>
  </entry>
  <entry>
    <title>cv算法面试问题总结（九）</title>
    <url>/2021/10/06/cv%E7%AE%97%E6%B3%95%E9%9D%A2%E8%AF%95%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93%EF%BC%88%E4%B9%9D%EF%BC%89/</url>
    <content><![CDATA[<p>什么是强化学习？<br>强化学习和监督学习、无监督学习的区别是什么？<br>强化学习适合解决什么样子的问题？<br>使用tensorflow进行深度学习算法实验时，如何调节超参数？<br>深度学习中的batch的大小对学习效果有何影响？<br>用梯度下降训练神经网络的参数，为什么参数有时会被训练为nan值？<br>卷积神经网络CNN中池化层有什么作用？<br>请列举几种常见的激活函数。激活函数有什么作用？<br>神经网络中Dropout的作用？具体是怎么实现的？<br>利用梯度下降法训练神经网络，发现模型loss不变，可能有哪些问题？怎么解决？</p>
<p>如何解决不平衡数据集的分类问题？<br>残差网络为什么能做到很深层？<br>相比sigmoid激活函数ReLU激活函数有什么优势？<br>卷积神经网络中空洞卷积的作用是什么？<br>适用于移动端部署的网络结构都有哪些？<br>深度学习模型参数初始化都有哪些方法？<br>卷积神经网络为什么会具有平移等不变性？</p>
<span id="more"></span>

]]></content>
  </entry>
  <entry>
    <title>cv算法面试问题总结（二）</title>
    <url>/2021/09/29/cv%E7%AE%97%E6%B3%95%E9%9D%A2%E8%AF%95%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93%EF%BC%88%E4%BA%8C%EF%BC%89/</url>
    <content><![CDATA[<h1 id="问题列表"><a href="#问题列表" class="headerlink" title="问题列表"></a>问题列表</h1><ol>
<li>？？？ 判别模型和生成模型</li>
<li>如何决定算法是否收敛</li>
<li>正则的方法及特点</li>
<li>1*1卷积的作用</li>
<li>？？？ 无监督学习的方法有哪些</li>
<li>什么是感受野，增大感受野的方法</li>
<li>反卷积的棋盘效应和解决方法</li>
<li>神经网络参数量计算</li>
<li>？？？ 空洞卷积的原理和作用</li>
<li>？？？ 空洞卷积的感受野计算</li>
</ol>
<span id="more"></span>

<h1 id="问题及解答"><a href="#问题及解答" class="headerlink" title="问题及解答"></a>问题及解答</h1><h2 id="判别模型和生成模型"><a href="#判别模型和生成模型" class="headerlink" title="判别模型和生成模型"></a>判别模型和生成模型</h2><p>？？？</p>
<h2 id="如何决定算法是否收敛"><a href="#如何决定算法是否收敛" class="headerlink" title="如何决定算法是否收敛"></a>如何决定算法是否收敛</h2><ol>
<li><p>Loss 小于某个阈值</p>
</li>
<li><p>Loss趋于稳定，在某个之附近徘徊</p>
</li>
<li><p>迭代到一定的次数</p>
</li>
<li><p>看权值矩阵的变化，两次迭代权值变化很小</p>
</li>
</ol>
<h2 id="正则的方法及特点"><a href="#正则的方法及特点" class="headerlink" title="正则的方法及特点"></a>正则的方法及特点</h2><ol>
<li>正则的目的是防止过拟合，提高泛化能力</li>
<li>L1正则</li>
<li>L2正则</li>
<li>数据集扩增</li>
<li>dropout</li>
</ol>
<h2 id="1-1卷积的作用"><a href="#1-1卷积的作用" class="headerlink" title="1*1卷积的作用"></a>1*1卷积的作用</h2><ol>
<li>实现跨通道信息融合；不同通道上的一个线性组合，实际上就是加起来乘一个系数</li>
<li>feature map 通道数上的降维：<ol>
<li>假设输入特征维度是 100<em>100</em>128，卷积核的大小是5<em>5（stride=1，padding=2）通道数是256，经过卷积后输出的特征维度是（100-5+2</em>2）/1+1 = 100-》100<em>100</em>256 卷积参数量是128<em>5</em>5<em>256=819200 如果在5</em>5卷积之前使用一个64通道1<em>1的卷积，参数量是 128</em>1<em>1</em>64 + 64<em>5</em>5*256 = 417792</li>
</ol>
</li>
<li>增加非线性映射次数 1*1卷积之后会加一个非线性激活函数，使网络更深，也是网络更加具有判别信息的特征</li>
</ol>
<h2 id="无监督学习的方法有哪些"><a href="#无监督学习的方法有哪些" class="headerlink" title="无监督学习的方法有哪些"></a>无监督学习的方法有哪些</h2><h2 id="什么是感受野，增大感受野的方法"><a href="#什么是感受野，增大感受野的方法" class="headerlink" title="什么是感受野，增大感受野的方法"></a>什么是感受野，增大感受野的方法</h2><ol>
<li>感受野是卷积神经网络每一层输出的的feature map上的每个feature 在原图上映射的区域大小（原图：预处理resize，crop，wrap之后的图）</li>
<li>感受野越大，映射的原始图像的范围越广，可以蕴含更为全局，语义层次更高的信息</li>
<li>计算公式：一开始RF=1，然后RF= （RF-1）* stride + kernelsize （不管conv层还是pooling层，依次替换stride和kernelsize至最后）</li>
<li>增加感受野的方法：<ol>
<li>空洞卷积</li>
<li>增加pooling层，但会降低准确度 pooling会丢失信息</li>
<li>增大kernelsize， 会增加参数</li>
<li>增加卷积层的个数， 会面临梯度消失的问题。CPM中作者采用多阶段训练，并引入中间层监督来解决梯度消失的问题</li>
</ol>
</li>
</ol>
<h2 id="反卷积的棋盘效应和解决方法"><a href="#反卷积的棋盘效应和解决方法" class="headerlink" title="反卷积的棋盘效应和解决方法"></a>反卷积的棋盘效应和解决方法</h2><ol>
<li>反卷积是允许小图像中的点来绘制更大图像中的方块，很容易出现不均匀重叠，使得图像中的某个部分颜色比其他部分更深，也就是伪影。尤其是kernelsize不能被stride整除的时候。虽然网络可以通过学习权重来避免这种情况，但实践中很难完全避免</li>
<li>解决方法：<ol>
<li>修改反卷积形式<ol>
<li>确保反卷积核的大小可以被步长整除</li>
<li>网络末尾使用1*1的反卷积</li>
<li>调整卷积核权重</li>
</ol>
</li>
<li>修改上采样形式，采用插值方法代替反卷积进行上采样，如邻近差值和双线性差值</li>
<li>通过损失函数修正输出，在损失函数中加入total variation loss等损失函数，平滑输出图像，但图像边缘会模糊</li>
</ol>
</li>
</ol>
<h2 id="神经网络参数量计算"><a href="#神经网络参数量计算" class="headerlink" title="神经网络参数量计算"></a>神经网络参数量计算</h2><ol>
<li>带参数的层是：卷积层，BN层和全连接层。激活函数层，pooling层和Upsample层是没有参数的</li>
<li>卷积层参数计算：卷积核体积(Kw X Kh X Cin) X 卷积核个数Cout(也就是输出channel数) + 偏置项个数Cout (每个卷积核带有一个偏置项，但是不影响参数的数量级有时候会省略)</li>
<li>BN层的参数计算：2 X Cin(输入通道数) 有两个需要学习缩放因子和平移因子</li>
<li>FC层的参数计算：输入向量长度(和feature map相同size的一个卷积核)Ti X 输出向量长度To + 偏置量To  （全连接层逐渐被global average pooling取代 将最后一层的特征图 整张图进行一个均值化形成特征点，将这些特征点组合成最后的特征向量，通过softmax 进行计算）</li>
</ol>
<h2 id="空洞卷积的原理和作用"><a href="#空洞卷积的原理和作用" class="headerlink" title="空洞卷积的原理和作用"></a>空洞卷积的原理和作用</h2><h2 id="空洞卷积的感受野计算"><a href="#空洞卷积的感受野计算" class="headerlink" title="空洞卷积的感受野计算"></a>空洞卷积的感受野计算</h2>]]></content>
      <categories>
        <category>学习</category>
        <category>cv算法</category>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>面试</tag>
        <tag>总结</tag>
        <tag>技巧</tag>
      </tags>
  </entry>
  <entry>
    <title>cv算法面试问题总结（八）</title>
    <url>/2021/10/06/cv%E7%AE%97%E6%B3%95%E9%9D%A2%E8%AF%95%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93%EF%BC%88%E5%85%AB%EF%BC%89/</url>
    <content><![CDATA[<p>神经网络中会用到批量梯度下降（BGD）吗？为什么用随机梯度下降（SGD）?<br>当神经网络的调参效果不好时，从哪些角度思考？（不要首先归结于overfiting）<br>请阐述下卷积神经网络CNN的基本原理(全网最通俗版)<br>神经网络输出层为什么通常使用softmax?<br>了解无人驾驶的核心技术么？<br>如何形象的理解LSTM的三个门<br>通过一张张动图形象的理解LSTM<br>如何理解反向传播算法BackPropagation<br>请问什么是softmax函数？<br>通俗理解BN(Batch Normalization)<br>批量归一化BN到底解决了什么问题？<br>如何理解随机梯度下降，以及为什么SGD能够收敛？<br>模拟退火算法能解决陷入局部最优的问题么<br>请说下常见优化方法各自的优缺点（BGD、SGD、MBGD、Momentum、NAG、Adagrad、Adadelta、RMSprop、Adam）<br>Adam算法的原理机制是怎么样的？它与相关的AdaGrad和RMSProp方法有什么区别</p>
<span id="more"></span>
]]></content>
  </entry>
  <entry>
    <title>cv算法面试问题总结（五）</title>
    <url>/2021/10/01/cv%E7%AE%97%E6%B3%95%E9%9D%A2%E8%AF%95%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93%EF%BC%88%E4%BA%94%EF%BC%89/</url>
    <content><![CDATA[<p>如何确定CNN的卷积核通道数和卷积输出层的通道数？<br>什么是卷积？<br>什么是CNN的池化pool层？<br>简述下什么是生成对抗网络<br>学梵高作画的原理是什么？<br>请简要介绍下tensorflow的计算图<br>你有哪些深度学习（rnn、cnn）调参的经验？<br>为什么不同的机器学习领域都可以使用CNN，CNN解决了这些领域的哪些共性问题？他是如何解决的？<br>LSTM结构推导，为什么比RNN好？<br>Sigmoid、Tanh、ReLu这三个激活函数有什么缺点或不足，有没改进的激活函数。</p>
<p>为什么引入非线性激励函数？<br>请问人工神经网络中为什么ReLu要好过于tanh和sigmoid function？<br>为什么LSTM模型中既存在sigmoid又存在tanh两种激活函数，而不是选择统一一种sigmoid或者tanh？这样做的目的是什么？<br>如何解决RNN梯度爆炸和弥散的问题？<br>什么样的数据集不适合用深度学习？<br>广义线性模型是怎被应用在深度学习中？<br>如何缓解梯度消失和梯度膨胀（微调、梯度截断、改良激活函数等）<br>简述神经网络的发展历史<br>深度学习常用方法<br>请简述神经网络的发展史。</p>
<span id="more"></span>

]]></content>
      <categories>
        <category>学习</category>
        <category>cv算法</category>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>面试</tag>
        <tag>总结</tag>
        <tag>技巧</tag>
      </tags>
  </entry>
  <entry>
    <title>cv算法面试问题总结（六）</title>
    <url>/2021/10/01/cv%E7%AE%97%E6%B3%95%E9%9D%A2%E8%AF%95%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93%EF%BC%88%E5%85%AD%EF%BC%89/</url>
    <content><![CDATA[<p>神经网络中激活函数的真正意义？一个激活函数需要具有哪些必要的属性？还有哪些属性是好的属性但不必要的？<br>梯度下降法的神经网络容易收敛到局部最优，为什么应用广泛？<br>简单说说CNN常用的几个模型<br>为什么很多做人脸的Paper会最后加入一个Local Connected Conv？<br>什么是梯度爆炸？<br>梯度爆炸会引发什么问题？<br>如何确定是否出现梯度爆炸？<br>如何修复梯度爆炸问题？</p>
<p>LSTM神经网络输入输出究竟是怎样的？<br>什么是RNN？<br>请详细介绍一下RNN模型的几种经典结构<br>简单说下sigmoid激活函数<br>如何从RNN起步，一步一步通俗理解LSTM（全网最通俗的LSTM详解）<br>CNN究竟是怎样一步一步工作的？<br>rcnn、fast-rcnn和faster-rcnn三者的区别是什么<br>在神经网络中，有哪些办法防止过拟合？<br>CNN是什么，CNN关键的层有哪些？<br>GRU是什么？GRU对LSTM做了哪些改动？</p>
<span id="more"></span>
]]></content>
      <categories>
        <category>学习</category>
        <category>cv算法</category>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>面试</tag>
        <tag>总结</tag>
        <tag>技巧</tag>
      </tags>
  </entry>
  <entry>
    <title>cv算法面试问题总结（十）</title>
    <url>/2021/10/06/cv%E7%AE%97%E6%B3%95%E9%9D%A2%E8%AF%95%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93%EF%BC%88%E5%8D%81%EF%BC%89/</url>
    <content><![CDATA[<p>基于深度学习的目标检测技术演进：R-CNN、Fast R-CNN、Faster R-CNN、YOLO、SSD<br>请简单解释下目标检测中的这个IOU评价函数（intersection-over-union）<br>KNN与K-means区别？<br>K-means选择初始点的方法有哪些,优缺点是什么?(列出两种以上)<br>简述线性分类器的原理(要求对权重矩阵进行剖析)<br>请简述下log对数、Hinge Loss(折页)、Cross-Entropy Loss(交叉熵)这三个损失函数<br>简述正则化与奥卡姆剃刀原则<br>图像尺寸为 7<em>7, 卷积窗口大小为3</em>3, 步长为3, 是否能输出图像?如果能,输出图像大小为多少?如果不能,说明原因?<br>如果最后一个卷积层和第一个全连接层参数量太大怎么办?<br>为什么说神经网络是端到端的网络?<br>当参数量 &gt;&gt; 样本量时候, 神经网络是如何预防过拟合?<br>什么是感受野？</p>
<p>简述你对CBIR(Content-based Image Retrieval基于内容的图像检索)的理解<br>什么是计算机视觉单词模型？<br>简述什么是Local Feature(局部特征算子)？<br>KD-Tree相比KNN来进行快速图像特征比对的好处在哪里?<br>简述encode和decode思想<br>输入图片尺寸不匹配CNN网络input时候的解决方式？（三种以上）<br>FCN与CNN最大的区别？<br>遇到class-imbalanced data（数据类目不平衡）问题怎么办？<br>简述孪生随机网络（Siamese Network）<br>DPM（Deformable Parts Model）算法流程<br>什么是NMS（Non-maximum suppression 非极大值抑制）?<br>列举出常见的损失函数(三个以上)?<br>做过目标检测项目么？比如Mask R-CNN和Python做一个抢车位神器<br>如何理解Faster RCNN<br>one-stage和two-stage目标检测方法的区别和优缺点？<br>请画下YOLOv3的网络结构<br>请简单说下YOLOv1,v2,v3,v4各自的特点与发展史<br>如何理解YOLO：YOLO详解<br>怎么理解YOLOv4<br>神经网络参数共享(parameter sharing)是指什么？<br>2021年网易互联网 计算机视觉 一面<br>美团实习算法岗<br>2021商汤-视频理解研究员-校招-技术面</p>
]]></content>
  </entry>
  <entry>
    <title>cv算法面试问题总结（四）</title>
    <url>/2021/10/01/cv%E7%AE%97%E6%B3%95%E9%9D%A2%E8%AF%95%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93%EF%BC%88%E5%9B%9B%EF%BC%89/</url>
    <content><![CDATA[<h1 id="问题列表"><a href="#问题列表" class="headerlink" title="问题列表"></a>问题列表</h1><ol>
<li><p>神经网络中的Addition/concatenate区别</p>
</li>
<li><p>目标检测中的anchor机制？作用？</p>
</li>
<li><p>BN（Batch Normalization）的原理和作用</p>
</li>
<li><p>随机梯度下降相比全局梯度下降的好处</p>
</li>
<li><p>网络初始化时给网络赋予0的权重，这个网络能正常训练吗？</p>
</li>
<li><p>梯度消失和梯度爆炸的原因？</p>
</li>
<li><p>深度学习为什么在计算机视觉领域这么好？</p>
</li>
<li><p>什么是正则化？L1正则化和L2正则化有什么区别</p>
</li>
<li><p>常用的模型压缩方式有哪些</p>
</li>
<li><p>残差网络的设计思想和作用</p>
<span id="more"></span>

<h1 id><a href="#" class="headerlink" title></a></h1></li>
</ol>
]]></content>
      <categories>
        <category>学习</category>
        <category>cv算法</category>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>面试</tag>
        <tag>总结</tag>
        <tag>技巧</tag>
      </tags>
  </entry>
  <entry>
    <title>hello-world</title>
    <url>/2021/09/29/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>
]]></content>
  </entry>
  <entry>
    <title>kaggle-糖尿病视网膜病变检测</title>
    <url>/2021/10/07/kaggle-%E7%B3%96%E5%B0%BF%E7%97%85%E8%A7%86%E7%BD%91%E8%86%9C%E7%97%85%E5%8F%98%E6%A3%80%E6%B5%8B/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>问题分析：糖尿病作为一种越来越高发的慢性病，每11个人就有一人是糖尿病患者。糖尿病视网膜病变作为并发症导致的视网膜病变失明也十分常见，如何预防因糖尿病继发的视网膜病变问题的是十分迫切的。这影响到62.5万英国人、甚至是全球1亿人的生活。</p>
<p>解决方法：</p>
<p>结果：</p>
<span id="more"></span>

<h1 id="背景知识"><a href="#背景知识" class="headerlink" title="背景知识"></a>背景知识</h1><p>Pearse Keane是Moorfields的一名眼科顾问，在DeepMind的官网上，他针对糖尿病视网膜病变和因衰老导致的黄斑病变(AMD)的眼部扫描图分析发表了一番见解。</p>
<p>根据统计，糖尿病患者的失明风险是普通人的25倍，而糖尿病作为一种越来越高发的慢性病，每11个人就有一人是糖友。但！如果早期发现，并进行有效干预，是能够避免因糖尿病继发的视网膜病变问题的。这影响到62.5万英国人、甚至是全球1亿人的生活。</p>
<p>而AMD已经成为英国最常见的失明原因。根据统计，仅仅在英国，每天就有接近200人无法看到明天的英国雨景（顺手一黑）。而如果让这种状况持续下去，到2020年，全球因AMD而失明的人将达到2亿人。而同样地，这种状况可以通过早期预防和治疗而避免。</p>
<h1 id="问题分析"><a href="#问题分析" class="headerlink" title="问题分析"></a>问题分析</h1><h1 id="数据概览"><a href="#数据概览" class="headerlink" title="数据概览"></a>数据概览</h1><p>查阅资料知道：糖尿病视网膜病变是由于糖尿病导致血糖过高损害视网膜毛细血管造成的视网膜病变。</p>
<p>检测目标：</p>
<p>检测视网膜内出血 红色的点</p>
<p>检测血管边缘病变 </p>
<p>蛋白质泛黄</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">0 - No DR</span><br><span class="line">1 - Mild</span><br><span class="line">2 - Moderate</span><br><span class="line">3 - Severe</span><br><span class="line">4 - Proliferative DR</span><br></pre></td></tr></table></figure>

<h1 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h1><h2 id="我的方法"><a href="#我的方法" class="headerlink" title="我的方法"></a>我的方法</h2><h2 id="top5的方法"><a href="#top5的方法" class="headerlink" title="top5的方法"></a>top5的方法</h2>]]></content>
      <categories>
        <category>学习</category>
        <category>cv算法</category>
        <category>Pytorch</category>
        <category>深度学习</category>
        <category>项目比赛</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>Kaggle</tag>
      </tags>
  </entry>
  <entry>
    <title>kaggle比赛-Molecular Translation</title>
    <url>/2021/10/03/kaggle%E6%AF%94%E8%B5%9B-Molecular-Translation/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>kaggle比赛-NFL Helmet Assignment</title>
    <url>/2021/10/03/kaggle%E6%AF%94%E8%B5%9B-NFL-Helmet-Assignment/</url>
    <content><![CDATA[<p><a href="https://www.kaggle.com/c/nfl-health-and-safety-helmet-assignment">https://www.kaggle.com/c/nfl-health-and-safety-helmet-assignment</a></p>
]]></content>
      <categories>
        <category>学习</category>
        <category>cv算法</category>
        <category>Pytorch</category>
        <category>深度学习</category>
        <category>项目比赛</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>Kaggle</tag>
      </tags>
  </entry>
  <entry>
    <title>kaggle比赛-脑肿瘤检测</title>
    <url>/2021/09/30/kaggle%E6%AF%94%E8%B5%9B-%E8%84%91%E8%82%BF%E7%98%A4%E6%A3%80%E6%B5%8B/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>数据集简介：脑肿瘤图片</p>
<p>任务：对于测试集中的每个<code>BraTS21ID</code>，预测出目标的<code>MGMT_value</code>概率</p>
<p>算法简介：</p>
<p>Version1.0: EfficientDet 3D 迁移学习+微调</p>
<p>Version2.0: EfficientDet 3D + ResNet 101</p>
<p>预测结果： </p>
<p>Version1.0  Score: 0.642 Rank: 498/1400</p>
<p>Version2.0  Score: 0.732 Rank: 90/1435</p>
<span id="more"></span>

<p>迁移学习，迁移学习是一种很常见的深度学习技巧，我们利用很多预训练的经典模型直接去训练我们自己的任务。虽然说领域不同，但是在学习权重的广度方面，两个任务之间还是有联系的。</p>
<p>我们直接拿来其他任务的训练权重，在进行optimize的时候，如何选择适当的学习率是一个很重要的问题。</p>
<p>神经网络(如下图)一般分为三个部分，输入层，隐含层和输出层，随着层数的增加，神经网络学习到的特征越抽象。因此，下图中的卷积层和全连接层的学习率也应该设置的不一样，一般来说，卷积层设置的学习率应该更低一些，而全连接层的学习率可以适当提高。</p>
<p>差分学习率的意思，在不同的层设置不同的学习率，可以提高神经网络的训练效果。</p>
<p>Version1.0:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> sys </span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> glob</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> pydicom</span><br><span class="line"><span class="keyword">from</span> pydicom.pixel_data_handlers.util <span class="keyword">import</span> apply_voi_lut</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.utils <span class="keyword">import</span> data <span class="keyword">as</span> torch_data</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> model_selection <span class="keyword">as</span> sk_model_selection</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> torch_functional</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> StratifiedKFold</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> roc_auc_score</span><br><span class="line"><span class="keyword">from</span> efficientnet_pytorch_3d <span class="keyword">import</span> EfficientNet3D</span><br><span class="line"></span><br><span class="line"><span class="comment"># 判断数据集和EfficientNet模型的路径</span></span><br><span class="line"><span class="keyword">if</span> os.path.exists(<span class="string">&quot;../input/rsna-miccai-brain-tumor-radiogenomic-classification&quot;</span>):</span><br><span class="line">    data_directory = <span class="string">&#x27;../input/rsna-miccai-brain-tumor-radiogenomic-classification&#x27;</span></span><br><span class="line">    pytorch3dpath = <span class="string">&quot;../input/efficientnetpyttorch3d/EfficientNet-PyTorch-3D&quot;</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    data_directory = <span class="string">&#x27;/media/roland/data/kaggle/rsna-miccai-brain-tumor-radiogenomic-classification&#x27;</span></span><br><span class="line">    pytorch3dpath = <span class="string">&quot;EfficientNet-PyTorch-3D&quot;</span></span><br><span class="line">    </span><br><span class="line">mri_types = [<span class="string">&#x27;FLAIR&#x27;</span>,<span class="string">&#x27;T1w&#x27;</span>,<span class="string">&#x27;T1wCE&#x27;</span>,<span class="string">&#x27;T2w&#x27;</span>]</span><br><span class="line">SIZE = <span class="number">256</span></span><br><span class="line">NUM_IMAGES = <span class="number">64</span></span><br><span class="line"></span><br><span class="line">sys.path.append(pytorch3dpath)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载数据集</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_dicom_image</span>(<span class="params">path, img_size=SIZE, voi_lut=<span class="literal">True</span>, rotate=<span class="number">0</span></span>):</span></span><br><span class="line">    dicom = pydicom.read_file(path) <span class="comment"># MRI扫描图像是多张dicom格式图像，通过pydicom</span></span><br><span class="line">    data = dicom.pixel_array</span><br><span class="line">    <span class="comment">#VOI LUT(DICOM 设备可用)将原始DICOM格式转为易于使用的格式voi_lut</span></span><br><span class="line">    <span class="keyword">if</span> voi_lut:</span><br><span class="line">        data = apply_voi_lut(dicom.pixel_array, dicom)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        data = dicom.pixel_array</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># 图像增强 添加旋转操作</span></span><br><span class="line">    <span class="keyword">if</span> rotate &gt; <span class="number">0</span>:</span><br><span class="line">        rot_choices = [<span class="number">0</span>, cv2.ROTATE_90_CLOCKWISE, cv2.ROTATE_90_COUNTERCLOCKWISE, cv2.ROTATE_180]</span><br><span class="line">        data = cv2.rotate(data, rot_choices[rotate])</span><br><span class="line">        </span><br><span class="line">    data = cv2.resize(data, (img_size, img_size))</span><br><span class="line">    <span class="keyword">return</span> data</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_dicom_images_3d</span>(<span class="params">scan_id, num_imgs=NUM_IMAGES, img_size=SIZE, mri_type=<span class="string">&quot;FLAIR&quot;</span>, split=<span class="string">&quot;train&quot;</span>, rotate=<span class="number">0</span></span>):</span></span><br><span class="line"></span><br><span class="line">    files = <span class="built_in">sorted</span>(glob.glob(<span class="string">f&quot;<span class="subst">&#123;data_directory&#125;</span>/<span class="subst">&#123;split&#125;</span>/<span class="subst">&#123;scan_id&#125;</span>/<span class="subst">&#123;mri_type&#125;</span>/*.dcm&quot;</span>), </span><br><span class="line">               key=<span class="keyword">lambda</span> var:[<span class="built_in">int</span>(x) <span class="keyword">if</span> x.isdigit() <span class="keyword">else</span> x <span class="keyword">for</span> x <span class="keyword">in</span> re.findall(<span class="string">r&#x27;[^0-9]|[0-9]+&#x27;</span>, var)])</span><br><span class="line">    middle = <span class="built_in">len</span>(files)//<span class="number">2</span></span><br><span class="line">    num_imgs2 = num_imgs//<span class="number">2</span></span><br><span class="line">    p1 = <span class="built_in">max</span>(<span class="number">0</span>, middle - num_imgs2)</span><br><span class="line">    p2 = <span class="built_in">min</span>(<span class="built_in">len</span>(files), middle + num_imgs2)</span><br><span class="line">    <span class="comment"># 取中间的num_imgs张图片，将他们构成3D数据</span></span><br><span class="line">    img3d = np.stack([load_dicom_image(f, rotate=rotate) <span class="keyword">for</span> f <span class="keyword">in</span> files[p1:p2]]).T </span><br><span class="line">    <span class="comment"># 如果要取的图片数量比总共分图片数量要多</span></span><br><span class="line">    <span class="keyword">if</span> img3d.shape[-<span class="number">1</span>] &lt; num_imgs:</span><br><span class="line">        n_zero = np.zeros((img_size, img_size, num_imgs - img3d.shape[-<span class="number">1</span>]))</span><br><span class="line">        img3d = np.concatenate((img3d,  n_zero), axis = -<span class="number">1</span>) <span class="comment"># 不够的拼接0</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">if</span> np.<span class="built_in">min</span>(img3d) &lt; np.<span class="built_in">max</span>(img3d):</span><br><span class="line">        img3d = img3d - np.<span class="built_in">min</span>(img3d)</span><br><span class="line">        img3d = img3d / np.<span class="built_in">max</span>(img3d)</span><br><span class="line">            </span><br><span class="line">    <span class="keyword">return</span> np.expand_dims(img3d,<span class="number">0</span>) <span class="comment"># 增加一维，axis=0时，高维扩展数组就给它在最外面再添个括号就</span></span><br><span class="line"></span><br><span class="line">a = load_dicom_images_3d(<span class="string">&quot;00000&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(a.shape)</span><br><span class="line"><span class="built_in">print</span>(np.<span class="built_in">min</span>(a), np.<span class="built_in">max</span>(a), np.mean(a), np.median(a))</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 固定随机数种子使用相同的网络结构，跑出来的效果完全一样</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">set_seed</span>(<span class="params">seed</span>):</span></span><br><span class="line">    random.seed(seed)</span><br><span class="line">    os.environ[<span class="string">&quot;PYTHONHASHSEED&quot;</span>] = <span class="built_in">str</span>(seed)</span><br><span class="line">    np.random.seed(seed)</span><br><span class="line">    torch.manual_seed(seed)</span><br><span class="line">    <span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">        torch.cuda.manual_seed_all(seed)</span><br><span class="line">        torch.backends.cudnn.deterministic = <span class="literal">True</span> <span class="comment">#固定torch框架种子</span></span><br><span class="line">random_seed = <span class="number">12</span></span><br><span class="line">set_seed(random_seed)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 数据集划分</span></span><br><span class="line">train_df = pd.read_csv(<span class="string">f&quot;<span class="subst">&#123;data_directory&#125;</span>/train_label.csv&quot;</span>)</span><br><span class="line">display(train_df)</span><br><span class="line"></span><br><span class="line">df_train, df_valid = sk_model_selection.train_test_split(</span><br><span class="line">	train_df,</span><br><span class="line">	test_size=<span class="number">0.2</span>,</span><br><span class="line">	random_state=random_seed,</span><br><span class="line">	straify=train_df[<span class="string">&#x27;MGMT_value&#x27;</span>] <span class="comment"># 使得train和test的数据的分布和原始数据分布一致</span></span><br><span class="line">	)</span><br></pre></td></tr></table></figure>

<p>数据集类创建</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Dataset</span>(<span class="params">torch_data.Dataset</span>):</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,paths,targets=<span class="literal">None</span>,mri_type=<span class="literal">None</span>,label_smoothing=<span class="number">0.01</span>,split=<span class="string">&quot;train&quot;</span>,augment=<span class="literal">False</span></span>):</span></span><br><span class="line">        self.paths = paths</span><br><span class="line">        self.targets = targets</span><br><span class="line">        self.mri_type = mri_type</span><br><span class="line">        self.label_smoothing = label_smoothing</span><br><span class="line">        self.split = split</span><br><span class="line">        self.augment = augment</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.paths)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self,index</span>):</span></span><br><span class="line">        scan_id = self.paths[index]</span><br><span class="line">        <span class="keyword">if</span> self.targets <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            data = load_dicom_images_3d(<span class="built_in">str</span>(scan_id).zfill(<span class="number">5</span>),mri_type=self.mri_type[index],split=self.split)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">if</span> self.augment: <span class="comment"># 是否做数据增强</span></span><br><span class="line">                rotation = np.random.randint(<span class="number">0</span>,<span class="number">4</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                rotation = <span class="number">0</span></span><br><span class="line">            data = load_dicom_images_3d(<span class="built_in">str</span>(scan_id).zfill(<span class="number">5</span>),mri_type=self.mri_type[index],split=<span class="string">&quot;train&quot;</span>,rotate=rotation)</span><br><span class="line">            </span><br><span class="line">                </span><br><span class="line">        <span class="keyword">return</span> x,y <span class="comment">#x是三维图像数据，y是对应的label</span></span><br></pre></td></tr></table></figure>

<p>Python3:  str.zfill(lenth) 方法返回指定长度的字符串，原字符串右对齐，前面填充0。</p>
<p>目标检测、语义分割、姿态估计、SLAM、3D重建</p>
]]></content>
      <categories>
        <category>学习</category>
        <category>cv算法</category>
        <category>Pytorch</category>
        <category>深度学习</category>
        <category>项目比赛</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>Kaggle</tag>
      </tags>
  </entry>
  <entry>
    <title>torch.backends.cudnn相关参数理解及配置</title>
    <url>/2021/10/08/torch-backends-cudnn%E7%9B%B8%E5%85%B3%E5%8F%82%E6%95%B0%E7%90%86%E8%A7%A3%E5%8F%8A%E9%85%8D%E7%BD%AE/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><h1 id="torch-backends-cudnn-deterministic"><a href="#torch-backends-cudnn-deterministic" class="headerlink" title="torch.backends.cudnn.deterministic"></a>torch.backends.cudnn.deterministic</h1><p>使用PyTorch等框架，还要看一下框架的种子是否固定了。</p>
<p>使用了cuda，必须要设置cuda的随机：将这个 flag 置为True的话，每次返回的卷积算法将是确定的，即默认算法。如果配合上设置 Torch 的随机种子为固定值的话，应该可以保证每次运行网络的时候相同输入的输出是固定的，代码大致这样:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">        torch.cuda.manual_seed_all(seed)</span><br><span class="line">        torch.backends.cudnn.deterministic = <span class="literal">True</span></span><br><span class="line">        torch.backends.cudnn.benchmark = <span class="literal">False</span></span><br></pre></td></tr></table></figure>

<h1 id="torch-backends-cudnn-benchmark"><a href="#torch-backends-cudnn-benchmark" class="headerlink" title="torch.backends.cudnn.benchmark"></a>torch.backends.cudnn.benchmark</h1><p>torch.backends.cudnn.benchmark设为True就可以大大提升卷积神经网络的运行速度</p>
<p>原因：<code>将会让程序在开始时花费一点额外时间，为整个网络的每个卷积层搜索最适合它的卷积实现算法，进而实现网络的加速</code>。</p>
<p>适用：适用场景是网络结构固定（不是动态变化的），网络的输入形状（包括 batch size，图片大小，输入的通道）是不变的</p>
<p>背景：</p>
<p>大多数主流深度学习框架都支持 cuDNN这个GPU加速库，来为训练加速。而卷积网络的具体计算方法又有很多，所以使用torch.backends.cudnn.benchmark就可以在 PyTorch 中<code>对模型里的卷积层进行预先的优化，也就是在每一个卷积层中测试 cuDNN 提供的所有卷积实现算法，然后选择最快的那个</code>。这样在模型启动的时候，只要额外多花一点点预处理时间，就可以较大幅度地减少训练时间。</p>
<p>影响卷积运行的因素：</p>
<p>为什么我们可以提前选择每层的算法，即使每次我们送入网络训练的图片是不一样的？即每次网络的输入都是变化的，那么我怎么确保提前选出来的最优算法同样也适用于这个输入呢？原因就是，对于<code>给定输入来说，其具体值的大小是不影响卷积的运行时间的，只有其尺寸才会影响</code>。举例来说，我们只要固定输入大小都是 (8, 64, 224, 224)，即 batch_size 为 8，输入的通道为 64，宽和高为 224，那么卷积层的运行时间都是几乎不变的，无论其中每个像素具体的值是 0.1 还是 1000.0。</p>
<p>所以当网络的模型不会一直发生变化，且输入的大小不会一直变化的话就可以使用torch.backends.cudnn.benchmark=True来加速训练</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">        torch.cuda.manual_seed_all(seed)</span><br><span class="line">        torch.backends.cudnn.deterministic = <span class="literal">True</span></span><br><span class="line">        torch.backends.cudnn.benchmark = <span class="literal">True</span></span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>学习</category>
        <category>Pytorch</category>
      </categories>
      <tags>
        <tag>torch</tag>
      </tags>
  </entry>
  <entry>
    <title>关于MLPerf Inference Benchmark的设计理解</title>
    <url>/2021/11/25/%E5%85%B3%E4%BA%8EMLPerf-Inference-Benchmark%E7%9A%84%E8%AE%BE%E8%AE%A1%E7%90%86%E8%A7%A3/</url>
    <content><![CDATA[<p>ML Comment 官网： <a href="https://mlcommons.org/en/">https://mlcommons.org/en/</a> </p>
<p><strong>LoadGenerator</strong>：负载生成器，MLPerf使用它生成不同的测试场景对inference进行测试</p>
<p><strong>SUT</strong>：被测试系统</p>
<p><strong>Sample</strong>：运行inference的单位，一个image或者一个sequence</p>
<p><strong>query</strong>：一组进行推理的N个样本</p>
<p><strong>Latency</strong>：LoadGenerator将query传递到SUT，到Inference完成并收到回复的时间，是一个数字，表示时间，一个sample就有一个latency</p>
<p><strong>Tail-latency</strong>：尾部延迟是指在一个系统提供的所有输入/输出（I/O）请求的响应中，与大部分响应时间相比，花费时间最长的那一小部分，就是说如果在系统中引入实时监控，总会有少量响应的延迟高于均值，我们把这些响应称为尾延迟（Tail Latency），本文所提到的tail-latency并不是一个表示延迟的数据，而是百分比</p>
<span id="more"></span>

<p>———论文精读———</p>
<h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>各种机器学习软硬件的迅速爆发，超过100家组织在构建ML inference chips， 涵盖现有模型的系统在能耗方面至少跨越是三个数量集，性能方面至少跨越五个数量级。ML软硬件的无数组合使得具有architecture-neutral，representative，and reproducible 的测评具有一定挑战性。MLPerf 联合了30多家组织和200多名ML engineers and practitioners, 制定了一些列的规范和最佳实践确保不同体系结构之间的可比性。第一次征集到了14个组织的600多个可复现的推论，代表了30多个不同的系统，展现了广泛的能力，证明了该benchmark的flexibility and adaptability.</p>
<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>为什么设计需要设计合适的ML benchmarking 度量，创建真实的ML inference场景，标准化评估方法能够实现推理质量的真实性能优化？</p>
<ol>
<li>选择了具有代表性的workloads为reproducibility和accessibility</li>
<li>为真实的评估识别场景</li>
<li>根据真实的用例规定目标质量和tail-latency bounds</li>
<li>设置了允许用户同时展示硬件和软件功能的规则</li>
<li>该基准测试方法允许模型在保持上述贡献的同时频繁更改</li>
</ol>
<h1 id="Challenges"><a href="#Challenges" class="headerlink" title="Challenges"></a>Challenges</h1><p>一个有用的ML Benchmark必须克服三个主要挑战：模型的多样性，部署场景的多样性，推理系统的部署</p>
<ol>
<li><p>Diversity of Models</p>
<p><img src="/2021/11/25/%E5%85%B3%E4%BA%8EMLPerf-Inference-Benchmark%E7%9A%84%E8%AE%BE%E8%AE%A1%E7%90%86%E8%A7%A3/Operations.png"></p>
<p>横坐标表示网络需要运算的次数，也就是模型的计算量（S表示gpu运算性能，s表示网络要运算的次数），纵坐标表示置信度最高的类的准确率，面积越大表示参数量越大，模型各有优缺点，需要在各种因素之间需要权衡</p>
</li>
<li><p>Deployment-Scenario Diversity</p>
<p>对于offline场景，比如图片分类，要求将图片在内存中随时可用才能保证加速器达到峰值性能。但是在实时程序中如自动驾驶，模型要持续处理数据流而不是一次性载入全部数据，只有在设备上的推理延迟并不能满足实际应用的需求</p>
</li>
<li><p>Inference-System Diversity</p>
<p><img src="/2021/11/25/%E5%85%B3%E4%BA%8EMLPerf-Inference-Benchmark%E7%9A%84%E8%AE%BE%E8%AE%A1%E7%90%86%E8%A7%A3/diversity.png"></p>
<p>上层应用到底层设备的栈图，每一层都有多种组合，每个组合都有其特点（idiosyn）导致推理系统的基准很难确定</p>
</li>
</ol>
<h1 id="Benchmark-Design"><a href="#Benchmark-Design" class="headerlink" title="Benchmark Design"></a>Benchmark Design</h1><ol>
<li><p>选用具有代表性和应用场景广泛的模型</p>
<p>任何在数学上等价于referenced model都是可以被认为是有效的，</p>
</li>
<li><p>可靠的质量目标</p>
<p>不同应用领域对于模型指标的要求不一样，有的侧重准确率，有的侧重吞吐量有的侧重延迟</p>
</li>
<li><p>真实的用户场景</p>
<p>MLPerf提供了四种inference的场景，四种测试场景的主要区别在于请求是怎样发送和接收的</p>
<p><img src="/2021/11/25/%E5%85%B3%E4%BA%8EMLPerf-Inference-Benchmark%E7%9A%84%E8%AE%BE%E8%AE%A1%E7%90%86%E8%A7%A3/scenario.png"></p>
<p>server场景和offline场景都是针对数据中心的，而single-stream和multi-stream是针对边缘计算和物联网的</p>
<p><strong>single-stream</strong></p>
<p>一次查询送入系统一个样本，到上个请求的响应之前不会发送下一个请求，性能指标<strong>是90%的延迟</strong>。记录所有的请求完成的延迟时间，将所有样本从小到大排列，第90%个样本的处理时间就是这个系统的性能指标</p>
<p><strong>MultiStream</strong></p>
<p>以固定的时间间隔发送请求（这个时间间隔就作为Multi-stream场景中的延迟边界，一般为50～100ms），一个请求中含有N个样本，当所有查询的(latency)延时都在（论文说99%）延迟边界中时，这时每个请求中包含的样本数N就是系统的性能指标。<strong>最大的在延时边界内可单个请求可包含的最大样本数N</strong></p>
<p><strong>Server</strong></p>
<p>为了模拟现实生活中的随机事件，请求将以泊松分布送入被测试系统中。每个请求只有一个样本，系统的性能指标是在延迟边界（latency bound）内<strong>每秒查询次数</strong>（QPS）。</p>
<p><strong>Offline</strong></p>
<p>一次请求将所有的测试样本送入到被测试系统中，被测试系统可以一次或多次以任何顺序返回测试结果，Offline场景的性能评判标准是<strong>每秒推理的样本数</strong>（论文中的原话是throughput measured in samples per second，单位时间的吞吐量，也就是吞吐率，所以我认为它指的是每秒处理的样本数）</p>
</li>
<li><p>可信的（statistically) Tail-Latency Bounds</p>
</li>
</ol>
<p> 不同任务的请求数量和请求中的样本个数</p>
<p>multi-stream和server要测试多次，multi-stream的测试时间要在2.5～7h，server的测试结果要测五次取最小值,所有的benchmark运行时间最小是60s</p>
<h1 id="Inference-submission-system"><a href="#Inference-submission-system" class="headerlink" title="Inference submission system"></a>Inference submission system</h1><ol>
<li>测试系统</li>
<li>Load Generator</li>
<li>数据集</li>
<li>Accuracy Checker</li>
</ol>
<h1 id="Submission-System-Evaluation"><a href="#Submission-System-Evaluation" class="headerlink" title="Submission-System Evaluation"></a>Submission-System Evaluation</h1><ol>
<li>结果提交，分类，类别</li>
<li>结果审查</li>
<li>结果报告</li>
</ol>
<h1 id="Benchmark-Assessment"><a href="#Benchmark-Assessment" class="headerlink" title="Benchmark Assessment"></a>Benchmark Assessment</h1><ol>
<li>任务范围</li>
<li>使用场景</li>
<li>处理器类型和软件框架</li>
<li>系统多样性</li>
<li>Open Division</li>
</ol>
<h1 id="Lessons-Learned"><a href="#Lessons-Learned" class="headerlink" title="Lessons Learned"></a>Lessons Learned</h1><ol>
<li>模型：广度 vs 深度</li>
<li>度量：延迟 vs 吞吐量</li>
<li>数据集：公开 VS 私有</li>
<li>性能：模型 VS 测量</li>
<li>过程：审核和可审核性</li>
</ol>
<h1 id="Prior-art-in-AI-ML-Benchmark"><a href="#Prior-art-in-AI-ML-Benchmark" class="headerlink" title="Prior art in AI/ML Benchmark"></a>Prior art in AI/ML Benchmark</h1><ol>
<li>AI. Benchmark</li>
<li>EEMBC MLmark</li>
<li>Fathom</li>
<li>AI Matrix</li>
<li>DeepBench</li>
<li>TBD</li>
<li>DAWNBench</li>
</ol>
<h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><h2 id="Performance-Metrics-性能度量"><a href="#Performance-Metrics-性能度量" class="headerlink" title="Performance Metrics 性能度量"></a>Performance Metrics 性能度量</h2><p>latency，latency-bounded throughout，throughout，and maximum number of inferences per query.这些都取决于预先设定的accuracy target和达到target的可能性。</p>
<p><strong>latency-bounded throughout</strong> 作为inference performance 的度量，将它引入到数据中心ML加速中。比latency和throughout反应更加现实的约束</p>
<h2 id="Accuracy-performance-tradeoff-精度和性能的平衡"><a href="#Accuracy-performance-tradeoff-精度和性能的平衡" class="headerlink" title="Accuracy/performance tradeoff 精度和性能的平衡"></a>Accuracy/performance tradeoff 精度和性能的平衡</h2><p>根据工业和学术专家的意见，设定对于MLPerf Inference的accuracy 和可容忍的退化阈值，允许使用分布式测量和结果优化。这种方法使得AI系统设计和评估标准化，学术和工业研究能够使用MLPerf Inference工作负载的精度要求去做比较。</p>
<h2 id="Evaluation-of-AI-inference-accelerators-人工智能推理加速器的评估"><a href="#Evaluation-of-AI-inference-accelerators-人工智能推理加速器的评估" class="headerlink" title="Evaluation of AI inference accelerators 人工智能推理加速器的评估"></a>Evaluation of AI inference accelerators 人工智能推理加速器的评估</h2><p>识别和描述matrice以及AI inference加速器在哪些推理场景下是有用的（server，single-stream，multi-stream，and offline），MLPerf引入批处理对于server，multi-stream，and offline在5个网络上，并且开发者和研究人员还可以在这些场景上继续额外的优化。</p>
]]></content>
  </entry>
  <entry>
    <title>关于MLPerf Training Benchmark的设计理解</title>
    <url>/2021/11/12/%E5%85%B3%E4%BA%8EMLPerf-Training-Benchmark%E7%9A%84%E8%AE%BE%E8%AE%A1%E7%90%86%E8%A7%A3/</url>
    <content><![CDATA[<p>摘要</p>
<p>问题解决：</p>
<p>原理</p>
<p>优点</p>
<p>缺点</p>
<span id="more"></span>

<p>ML Comment 官网： <a href="https://mlcommons.org/en/">https://mlcommons.org/en/</a> </p>
<p>论文链接：<a href="https://arxiv.org/pdf/1910.01500.pdf">https://arxiv.org/pdf/1910.01500.pdf</a></p>
<p>参考：<a href="https://www.cnblogs.com/caiyishuai/p/14361955.html">https://www.cnblogs.com/caiyishuai/p/14361955.html</a></p>
<h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>Mchine Learning的发展需要有行业标准的性能基准测评。Machione Learning有三个基准挑战：</p>
<ol>
<li><p>提升训练吞吐量</p>
</li>
<li><p>训练中的随机性以及solution的时间会有明显的差异性</p>
</li>
<li><p>软硬件的差异性导致难以用相同的精度，code和超参在不同的软硬件上面进行基准测评。</p>
</li>
</ol>
<p>MLPerf定量评估多个厂商的在驱动性能和可伸缩性改进方面的效能。</p>
<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>MLPerf的目标是创建一个具有代表性的基准测评，公正测评系统性能为了达到5个目标：</p>
<ol>
<li>公平竞赛同时鼓励创新</li>
<li>通过公平且有用的措施加速ML进程</li>
<li>加强可靠结果的复现性</li>
<li>同时服务于商用和研究</li>
<li>确保benchmark对于大家都是可负担的</li>
</ol>
<p>MLPerf Training最终是需要达到：</p>
<ol>
<li>创建为一个综合性的基准测评，能够覆盖不同的应用，DNN模型以及优化器等</li>
<li>创建每个基准测评的reference implementation去精确定义模型和训练程序</li>
<li>建立规则确保submissions对于reference implementation是等价的，并且也使用等价参数</li>
<li>创建时间规则，减少结果比较时的随机性的影响</li>
<li>将提交代码开源，以便结果复现</li>
<li>形成工作组保持benchmark的更新</li>
</ol>
<h2 id="MLPerf-实现回面临的问题"><a href="#MLPerf-实现回面临的问题" class="headerlink" title="MLPerf 实现回面临的问题"></a>MLPerf 实现回面临的问题</h2><p>ML 性能基准测试必须确保被测系统达到最先进的质量，同时提供足够的灵活性来适应不同的目标。质量和性能之间的这种权衡具有挑战性，因为多种因素会影响最终质量和实现质量的时间。</p>
<p>​    <strong>优化对质量的影响</strong>：尽管许多优化会立即改善传统性能指标（如吞吐量），但有些优化可能会降低最终的模型质量，而这种效果只能通过运行整个训练会话来观察到。例如，单精度训练和低精度训练之间的精度差异仅在后来的epoch出现</p>
<p>​    <strong>训练在时间尺度方面的影响</strong>：系统大小、batch size和动态学习率的变化对性能基准测试带来了另一个挑战。</p>
<p>​    <strong>运行中的偏差</strong>：DNN训练涉及许多随机影响，这些随机影响在实质性的运行到运行变化中是人为的。例如，随机权重初始化和随机数据传输和系统特征（例如，配置文件驱动的算法选择和浮点加法的非交换性质）。大型分布式训练任务可能涉及异步时间更新，从而改变梯度累积顺序。这些变化使得很难可靠地比较系统性能。</p>
<p>​    <strong>不同软件架构带来的影响</strong>：软件框架和底层数学库采用不同的算法来实现相同的操作但会产生略有不同的结果。例如，卷积层和完全连接层（现代 DNN 模型中常见的两种计算密集型运算符）通常使用缓存block来利用处理器内存层次结构。块大小和处理顺序（针对不同的硬件进行优化）的不同最最后的结果都有一定的影响</p>
<h2 id="MLPerf-Training的测试任务"><a href="#MLPerf-Training的测试任务" class="headerlink" title="MLPerf Training的测试任务"></a>MLPerf Training的测试任务</h2><p>Image Classification：</p>
<p>​        ResNet-50</p>
<p>Object Detection &amp; Segmentation</p>
<p>​        Mask R-CNN： 它有两个阶段：第一个阶段提出感兴趣的区域，第二个阶段处理它们以计算边界框和分割掩码。Mask R-CNN 为这些任务提供了高精度的结果，但代价是延迟更高，计算和内存要求更高。基准测试训练时，先做图像resize，将较短的边resize到800，用ResNet-50 作为主干</p>
<p>​        SSD(Single Shot Detection)： 适用于需要低延迟解决方案的实时应用。SSD以速度换取准确性。训练使用 300 × 300 的输入。选择 ResNet-34 主干网。ResNet-34具有与ResNet-50不同的残差块结构，增加了MLPerf所涵盖的计算主题的多样性。</p>
<p>Translation</p>
<p>​        Transformer</p>
<p>​        GNMT</p>
<p>Reinforcement Learning</p>
<p>​        MiniGo</p>
<p>Recommendation</p>
<p>​        Neural collaborative filtering (NCF)</p>
<h2 id="MLPerf-Training"><a href="#MLPerf-Training" class="headerlink" title="MLPerf Training"></a>MLPerf Training</h2><p>MLPerf training可以分为封闭模型分区（Closed Model Division）和开放模型分区（Open Model Division）。</p>
<p>封闭模型分区要求使用相同模型和优化器，并限制batch大小或学习率等超参数的值，它旨在硬件和软件系统的公平比较。</p>
<p>开放模型分区只会限制使用相同的数据解决相同的问题，其它模型或平台都不会限制，它旨在推进ML模型和优化的创新。</p>
<p>对于MLPerf Training每个Benchmark的评价标准是：<strong>在特定数据集上训练一个模型使其达到Quality Target时的Clock time</strong>。由于机器学习任务的训练时间有很大差异，因此，MLPerf 的最终训练结果是由指定次数的基准测试时间平均得出的，其中会去掉最低和最高的数字，一般是运行5次取平均值，Train测试时间包含了模型构建，数据预处理，训练以及质量测试等时间。</p>
<p>计时从系统接触任何训练或验证数据时开始，到系统在验证数据集上达到定义的质量目标时停止。<br>但不包括一下阶段：</p>
<p><strong>系统初始化</strong>System initialization，<strong>模型创建和初始化</strong>Model creation and initialization，<strong>数据重新格式化</strong> Data reformatting</p>
<p><strong>系统初始化</strong>System initialization。初始化（尤其是在大规模初始化）因群集管理选择和系统队列负载而异。例如，它可能涉及在开始训练作业之前对每个节点进行运行诊断。此类开销与系统的训练能力无关，因此将其排除在计时之外。<br><strong>模型创建和初始化</strong>Model creation and initialization.。某些框架可以编译模型图以优化后续执行。在使用行业规模的数据集时，对于较长的训练会话而言，此编译时间无关紧要。然而，MLPerf使用的公共数据集通常比行业数据集小得多。因此，大型分布式系统可以在几分钟内训练一些 MLPerf 基准测试，使累积时间占总时间的很大一部分。为了使基准测试代表在最大的工业数据集上的训练，允许排除长达20分钟的模型创建时间。此限制可确保 MLPerf 限制较小的训练作业，并且不鼓励使用计算和操作成本太高而无法在实践中使用的编译方法进行提交。<br><strong>数据重新格式化</strong> Data reformatting.。原始输入数据通常进行一次重新格式化，然后在许多后续的训练会话中提供。重新格式化示例包括更改图像文件格式和创建数据库（例如，LMDB、TFRecords 或 RecordIO）以实现更高效的访问。由于这些操作对于许多训练系统执行一次，MLPerf 计时不包括重新格式化。但它禁止训练中发生的任何数据处理或增强进入重新格式化阶段（例如，它阻止在定时训练阶段之前创建和保存每个图像的不同裁剪）</p>
<p>为了解决深度学习方法的随机性质和由此产生的运行到运行方差，MLPerf 要求提交提供每个基准的多次运行以稳定计时。<strong>视觉任务需要5次运</strong>行，以确保来自同一系统的90%的条目在5%以内; 所有其他任务都需要运行 10 次，以确保来自同一系统的 90% 的条目在 10% 以内。<strong>MLPerf 会丢弃最快和最慢的时间，并将剩余运行的算术平均值报告为结果。</strong></p>
<h3 id="质量阈值的选择"><a href="#质量阈值的选择" class="headerlink" title="质量阈值的选择"></a>质量阈值的选择</h3><p>选择较高阈值，可能需要更长训练的，原因有两个：首先，必须防止优化对最终结果产生不利影响。其次，我们必须尽量减少run-to-run variation，（不同的随机种子产生不同的影响）</p>
<p><strong>参考实现和超参数</strong><br>MLPerf 使用 PyTorch 或 TensorFlow 框架为每个基准测试提供了一个参考实现，没有被优化，主要目的是定义基准模型和training程序的具体实现。所有提交者都必须遵循这些引用 - 只要 DNN 模型和训练运算在数学上与引用等效，他们就可以在其选择的框架中重新实现基准标记。此外，MLPerf 使用参考实现来建立所需的质量阈值。<br>MLPerf 规则指定可修改的超参数 （Table 2） 以及对其修改的限制。这些限制旨在平衡对不同系统进行调整的需求，并限制超参数搜索空间的大小，以便对计算资源较小的提交者公平。<br>此外，在提交后的审核过程中，允许类似规模的系统之间”超参数借用”，其中一个提交者可以采用另一个提交者的超参数作为特殊基准测试并重新提交其结果（不允许进行其他硬件或软件更改）。在前两轮中，超参数借用成功地用于改进几个提交，表明超参数在某种程度上是可移植的。</p>
<p><img src="/2021/11/12/%E5%85%B3%E4%BA%8EMLPerf-Training-Benchmark%E7%9A%84%E8%AE%BE%E8%AE%A1%E7%90%86%E8%A7%A3/zhendeliu/Documents/GitHub/gitblog/source/_posts/%E5%85%B3%E4%BA%8EMLPerf-Training-Benchmark%E7%9A%84%E8%AE%BE%E8%AE%A1%E7%90%86%E8%A7%A3/table2.png"></p>
<h2 id="Report"><a href="#Report" class="headerlink" title="Report"></a>Report</h2><p>MLPerf 提交由系统说明、培训课程日志文件以及重现培训课程所需的所有代码和库组成。所有这些信息都可以在 MLPerf GitHub 网站上公开获得，以及 MLPerf 结果，从而实现可重复性，并使社区能够在后续回合中改进结果。系统描述包括硬件（节点数、处理器和加速器计数和类型、每个节点的存储期限和网络互连）和软件（操作系统以及库及其版本）。培训课程日志文件包含各种结构化信息，包括重要工作负载阶段的时间戳、按规定时间间隔进行的质量指标评估以及超参数选择。这些日志是分析结果的基础。</p>
<p>An MLPerf submission consists of a system description, training-session log files, and all code and libraries required to reproduce the training sessions. All of this information is publicly available on the MLPerf GitHub site, along with the MLPerf results, allowing for reproducibility and enabling the community to improve the results in subsequent rounds. A system description includes both the hardware (number of nodes, processor and accelerator counts and types, stor- age per node, and network interconnect) and the software (operating system as well as libraries and their versions). A training-session log file contains a variety of structured information including time stamps for important workload stages, quality-metric evaluations at prescribed intervals, and hyperparameter choices. These logs are the foundation for analyzing results.</p>
<p>4.2 报告结果<br>每个 MLPerf 提交都有多个标签：部门（开放或关闭）、类别（可用、预览或研究）和系统类型（本地或云）。<br>4.2.1 提交部门<br>MLPerf有两个提交部门：封闭和开放。两者都要求提交使用相同的数据集和质量指标作为相应的参考实现。<br>closed区旨在直接进行系统比较，因此它通过要求提交等效于参考实现来努力确保工作负载等效性。等价性包括数学上相同的模型导入、参数初始化、优化器和训练计划以及数据处理和遍历。为了确保公平性，此版本还限制了超参数修改。open区旨在鼓励对重要实际问题进行创新，并鼓励硬件/软件协同设计。它允许提交采用与参考实现不同的模型架构、优化过程和数据增强。</p>
<p>4.2.2 系统类别<br>为了支持广泛的研究和行业系统，我们定义了三个提交类别：可用、预览和研究。这些类别鼓励新技术和系统（例如，来自学术研究人员），但它们也区分了运输产品和概念验证或早期工程样品。<br>可用类别对硬件和软件可用性都有要求。硬件必须可用于云服务上的第三方租赁，或者，对于本地设备，则可供购买。租赁或购买的铺层和交货时间应适合系统规模和公司规模。为了确保基准测试提交的基准测试被广泛使用，并且不鼓励特定于基准测试的工程，我们还要求此类别中的软件版本化并支持一般用途。<br>预览系统包含自提交之日起 60 天内或下一个提交周期（以较晚者为准）满足可用类别条件的组件。届时，任何预览系统也必须提交到可用类别。<br>研究提交的内容包含无意生产的组件。一个例子是学术研究原型，它被设计为概念验证，而不是一个强大的产品。此类别还包括由专业管道硬件和软件构建但规模大于可用类别配置的系统。</p>
<p>4.2.3 报告规模<br>现代 ML 训练在系统功耗和成本方面跨越多个数量级。因此，如果报告的性能包括比例，则比较会更有用。通用的规模指标（如成本或功耗）无法在各种系统（云、本地和预生产）中定义，因此需要按系统类型进行区分。<br>在前两轮 MLPerf 中，我们在性能分数旁边加入了系统概念（处理器和/或加速器的数量）。对于本地示例，将来的版本将包括功率测量特性。对于云系统，我们从主机处理器的数量、主机内存量以及加速器的数量和类型中得出了一个”云规模”指标。我们从经验上证实了云规模与三大云提供商的成本密切相关。这些规模指标的报告在 MLPerf v0.5 和 v0.6 中是可选的。</p>
<p>4.2.4 报告分数<br>MLPerf 结果报告提供了针对每个基准测试进行训练的时间。尽管对于系统比较来说，可能需要一个跨越整个套件的单个摘要分数，但由于两个主要原因，它不适合MLPerf。首先，总分意味着对个人基准分数进行一些加权。鉴于系统用户的多样性和 MLPerf 涵盖的广泛应用，没有一种加权方案具有普遍代表性。其次，如果提交者拒绝报告所有基准测试的结果，则汇总分数的意义就会降低。提交者可能会因为省略某些基准测试而产生多种原因， 并非所有基准测试在每个系统规模上都是可行的（例如，某些模型在最大系统需要数据并行训练所需的迷你批大小下不可训练）。此外，某些处理器可能仅面向某些应用程序。</p>
<p>5 结果<br>与所有基准一样，MLPerf旨在通过建设性的竞争来鼓励创新;我们通过比较各轮提交的结果来衡量进度。到目前为止，我们已经进行了两轮 MLPerf 训练：v0.5 和 v0.6。它们相隔六个月，底层的硬件和软件系统保持不变。未经修改或在回合之间进行微小修改的结果表明，MLPerf正在推动实现和软件堆栈的快速性能和扩展改进。图3显示，在两个子任务回合之间，尽管目标质量更高，但16芯片系统的最佳性能结果平均提高了1.3×。图4显示，产生最佳整体性能结果所需的芯片数量平均增加了5.5×。其中一些改进归功于更好的基准测试实现，另一些归功于规则更改，例如允许LARS（You et al.，2017）优化器用于大型ResNet批处理。但我们相信，sub-mitters将大部分性能和扩展改进整合到底层软件基础架构中，并将其传递给用户。我们预计MLPerf将通过专注的硬件创新来推动类似的改进。</p>
<h2 id="MLPerf-Inference"><a href="#MLPerf-Inference" class="headerlink" title="MLPerf Inference"></a>MLPerf Inference</h2><p>对于MLPerf Inference测试，每个Benchmark的评价标准是：<strong>在特定数据集上测量模型的推理性能，包括Latency和吞吐量</strong>。</p>
<h1 id="MLPerf-Training-BenchMark"><a href="#MLPerf-Training-BenchMark" class="headerlink" title="MLPerf Training BenchMark"></a>MLPerf Training BenchMark</h1><h2 id="Image-Classification"><a href="#Image-Classification" class="headerlink" title="Image Classification"></a>Image Classification</h2><h2 id="Object-Detection-and-Segmentation"><a href="#Object-Detection-and-Segmentation" class="headerlink" title="Object Detection and Segmentation"></a>Object Detection and Segmentation</h2><ol>
<li><p>Mask R-CNN</p>
</li>
<li><p>SSD</p>
</li>
<li><p>Translation</p>
</li>
<li><p>Reinforcement Learning</p>
</li>
<li><p>Recommendation</p>
</li>
<li></li>
</ol>
<h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1>]]></content>
  </entry>
  <entry>
    <title>医疗影像dicom图像预处理-Pydicom</title>
    <url>/2021/10/06/%E5%8C%BB%E7%96%97%E5%BD%B1%E5%83%8Fdicom%E5%9B%BE%E5%83%8F%E9%A2%84%E5%A4%84%E7%90%86-Pydicom/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>pydicom是一个python中的第三方库，用于DICOM文件，主要为了以一种简单的”python式”方式检查和修改dicom数据而设计，可以提供给使用者轻松的修改，读写文件并转换成显式图像图片。</p>
<p>DICOM被广泛应用于放射医疗、心血管成像以及放射诊疗诊断设备（X射线，CT，核磁共振，超声等），并且在眼科和牙科等其它医学领域得到越来越深入广泛的应用。在数以万计的在用医学成像设备中，DICOM是部署最为广泛的医疗信息标准之一。当前大约有百亿级符合DICOM标准的医学图像用于临床使用。<br>患者的医学图像以DICOM文件格式进行存储，其中包含了图像信息以及患者的PHI（protected health information，即姓名、性别、年龄等），以及产生图像的设备的相关信息。</p>
<span id="more"></span>

<h1 id="人体内各介质的亨氏单位值"><a href="#人体内各介质的亨氏单位值" class="headerlink" title="人体内各介质的亨氏单位值"></a>人体内各介质的亨氏单位值</h1><p><img src="/2021/10/06/%E5%8C%BB%E7%96%97%E5%BD%B1%E5%83%8Fdicom%E5%9B%BE%E5%83%8F%E9%A2%84%E5%A4%84%E7%90%86-Pydicom/hu.png"></p>
<h1 id="pydicom基础操作"><a href="#pydicom基础操作" class="headerlink" title="pydicom基础操作"></a>pydicom基础操作</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pydicom</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">dcm = pydicom.read_file(<span class="string">&#x27;test1/Image-100.dcm&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(dcm.<span class="built_in">dir</span>()) <span class="comment"># 查看有哪些方法，属性</span></span><br><span class="line"><span class="built_in">print</span>(dcm.<span class="built_in">dir</span>(<span class="string">&#x27;pat&#x27;</span>)) <span class="comment"># 查看有哪些pat(属性)</span></span><br><span class="line"><span class="comment"># 通过字典关键字来获取图像的数据元信息（当然也可以根据TAG号）</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">info = &#123;&#125;</span><br><span class="line"><span class="comment"># 从dcm 文件中提取patient信息</span></span><br><span class="line">info[<span class="string">&quot;PatientID&quot;</span>] = dcm.PatientID               <span class="comment"># 患者ID</span></span><br><span class="line"><span class="comment"># info[&quot;PatientName&quot;] = dcm.PatientName           # 患者姓名</span></span><br><span class="line"><span class="comment"># info[&quot;PatientBirthData&quot;] = dcm.PatientBirthData # 患者出生日期</span></span><br><span class="line"><span class="comment"># info[&quot;PatientAge&quot;] = dcm.PatientAge             # 患者年龄</span></span><br><span class="line"><span class="comment"># info[&#x27;PatientSex&#x27;] = dcm.PatientSex             # 患者性别</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 从dcm 文件中提取study案例信息</span></span><br><span class="line"><span class="comment"># info[&#x27;StudyID&#x27;] = dcm.StudyID                   # 检查ID</span></span><br><span class="line"><span class="comment"># info[&#x27;StudyDate&#x27;] = dcm.StudyDate               # 检查日期</span></span><br><span class="line"><span class="comment"># info[&#x27;StudyTime&#x27;] = dcm.StudyTime               # 检查时间</span></span><br><span class="line"><span class="comment"># info[&#x27;InstitutionName&#x27;] = dcm.InstitutionName   # 机构名称</span></span><br><span class="line"><span class="comment"># info[&#x27;Manufacturer&#x27;] = dcm.Manufacturer         # 设备制造商</span></span><br><span class="line"><span class="comment"># info[&#x27;StudyDescription&#x27;]=dcm.StudyDescription   # 检查项目描述</span></span><br><span class="line"><span class="built_in">print</span>(info)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取图像唯一标识符</span></span><br><span class="line">uid = dcm.SOPInstanceUID</span><br><span class="line">img_arr = dcm.pixel_array</span><br><span class="line"><span class="built_in">print</span>(img_arr.shape)</span><br><span class="line">lens = img_arr.shape[<span class="number">0</span>] * img_arr.shape[<span class="number">1</span>] <span class="comment"># 获取像素点的数量</span></span><br><span class="line">tmp = np.reshape(img_arr,(lens,))</span><br><span class="line">max_val = <span class="built_in">max</span>(tmp)</span><br><span class="line">min_val = <span class="built_in">min</span>(tmp)</span><br><span class="line"><span class="comment"># 图像归一化</span></span><br><span class="line">img_arr = (img_arr-min_val)/(max_val-min_val)</span><br><span class="line">plt.figure(figsize=(<span class="number">12</span>,<span class="number">12</span>),dpi=<span class="number">250</span>) <span class="comment">#dpi每英寸点数，这张是1200*1200像素的图片</span></span><br><span class="line">plt.title(<span class="string">&quot;UID:&#123;&#125;&quot;</span>.<span class="built_in">format</span>(uid))</span><br><span class="line">plt.imshow(img_arr,cmap=plt.cm.gray)<span class="comment">#颜色图谱（colormap), 默认绘制为RGB(A)颜色空间。</span></span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改图片中的元素</span></span><br><span class="line"><span class="keyword">for</span> n,val <span class="keyword">in</span> <span class="built_in">enumerate</span>(dcm.pixel_array.flat):</span><br><span class="line">    <span class="keyword">if</span> val &lt; <span class="number">1300</span>:</span><br><span class="line">        dcm.pixel_array.flat[n] = <span class="number">0</span></span><br><span class="line">dcm.PixelData = dcm.pixel_array.tobytes()</span><br><span class="line">img_arr2 = dcm.pixel_array</span><br><span class="line">plt.figure(figsize=(<span class="number">12</span>,<span class="number">12</span>),dpi=<span class="number">250</span>) <span class="comment">#dpi每英寸点数，这张是1200*1200像素的图片</span></span><br><span class="line">plt.title(<span class="string">&quot;UID:&#123;&#125;&quot;</span>.<span class="built_in">format</span>(uid))</span><br><span class="line">plt.imshow(img_arr2,cmap=plt.cm.gray)<span class="comment">#颜色图谱（colormap), 默认绘制为RGB(A)颜色空间。</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<h1 id="案例：肺结节图像预处理"><a href="#案例：肺结节图像预处理" class="headerlink" title="案例：肺结节图像预处理"></a>案例：肺结节图像预处理</h1><ol>
<li></li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 加载必要的python包</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd </span><br><span class="line"><span class="keyword">import</span> dicom</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> scipy.ndimage</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> skimage <span class="keyword">import</span> measure, morphology</span><br><span class="line"><span class="keyword">from</span> mpl_toolkits.mplot3d.art3d <span class="keyword">import</span> Poly3DCollection</span><br><span class="line"></span><br><span class="line"><span class="comment"># 包含所有患者目录的根目录</span></span><br><span class="line">INPUT_FOLDER = <span class="string">&#x27;../input/sample_images/&#x27;</span></span><br><span class="line">patients = os.listdir(INPUT_FOLDER)</span><br><span class="line">patients.sort()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载一位患者的所有slice，按照顺序调整为等间隔扫描</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_scan</span>(<span class="params">path</span>):</span></span><br><span class="line">    slices = [dicom.read_file(path + <span class="string">&#x27;/&#x27;</span> + s) <span class="keyword">for</span> s <span class="keyword">in</span> os.listdir(path)]</span><br><span class="line">    <span class="comment">#按照扫描顺序叠加</span></span><br><span class="line">    slices.sort(key = <span class="keyword">lambda</span> x: <span class="built_in">float</span>(x.ImagePositionPatient[<span class="number">2</span>])) </span><br><span class="line">    <span class="comment"># 计算相邻扫描的间隔距离</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        slice_thickness = np.<span class="built_in">abs</span>(slices[<span class="number">0</span>].ImagePositionPatient[<span class="number">2</span>] - slices[<span class="number">1</span>].ImagePositionPatient[<span class="number">2</span>])</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        slice_thickness = np.<span class="built_in">abs</span>(slices[<span class="number">0</span>].SliceLocation - slices[<span class="number">1</span>].SliceLocation)</span><br><span class="line">		</span><br><span class="line">    <span class="keyword">for</span> s <span class="keyword">in</span> slices:</span><br><span class="line">        s.SliceThickness = slice_thickness</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> slices</span><br></pre></td></tr></table></figure>

<p>默认情况下，从DICOM文件中获得的值是HU这个单位。 需要解决这个问题。缩放斜率和截距由硬件制造商决定。<br>它指定从存储在磁盘表示中的像素到存储在内存表示中的像素的线性转换。磁盘存储的值定义为SV。而转化到内存中的像素值uints就需要两个dicom tag : Rescale intercept和Rescale slope。<br>OutputUnits=m∗SV+b<br>RescaleIntercept:b<br>RescaleSlope:m</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 排除扫描边界之外的数据，然后重新计算HU值（乘以重新缩放斜率并添加截距）</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_pixels_hu</span>(<span class="params">slices</span>):</span></span><br><span class="line">    image = np.stack([s.pixel_array <span class="keyword">for</span> s <span class="keyword">in</span> slices])</span><br><span class="line">    <span class="comment"># 转换为int16，int16是ok的，因为所有的数值都应该 &lt;32k</span></span><br><span class="line">    image = image.astype(np.int16)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 设置边界外的元素为0</span></span><br><span class="line">    image[image == -<span class="number">2000</span>] = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 转换为HU单位</span></span><br><span class="line">    <span class="keyword">for</span> slice_number <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(slices)):</span><br><span class="line"></span><br><span class="line">        intercept = slices[slice_number].RescaleIntercept</span><br><span class="line">        slope = slices[slice_number].RescaleSlope</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> slope != <span class="number">1</span>:</span><br><span class="line">            image[slice_number] = slope * image[slice_number].astype(np.float64)</span><br><span class="line">            image[slice_number] = image[slice_number].astype(np.int16)</span><br><span class="line"></span><br><span class="line">        image[slice_number] += np.int16(intercept)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> np.array(image, dtype=np.int16)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 产看第i位患者的图像</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show_img</span>():</span></span><br><span class="line">    first_patient = load_scan(INPUT_FOLDER + patients[<span class="number">0</span>])</span><br><span class="line">    first_patient_pixels = get_pixels_hu(first_patient)</span><br><span class="line">    plt.hist(first_patient_pixels.flatten(), bins=<span class="number">80</span>, color=<span class="string">&#x27;c&#x27;</span>)</span><br><span class="line">    plt.xlabel(<span class="string">&quot;Hounsfield Units (HU)&quot;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&quot;Frequency&quot;</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 显示一个中间位置的切片</span></span><br><span class="line">    plt.imshow(first_patient_pixels[<span class="number">80</span>], cmap=plt.cm.gray)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>

<p>统一像素间距spacing，spacing定义了图像像素的物理大小并且保证了实际距离测量的准确性。比如，如果知道x和y轴的像素间距为 0.4mm，那么在图像中的一条 10 像素的线就会有 4mm的长度。同样，由于知道图像像素中的宽和高(比如对于普通CT来说是 512×512)，就能够找到图像的实际尺寸了:512 × 0.4 mm = 204.8 mm。</p>
<p>在重采样过程中，让图像的spacing保持一致以及具体大小是多少是非常重要的：CNN中Conv操作被提出来的其中一个重要motivation就是图像中有相似的块能用共享的卷积来提取特征，因此对所有图像重采样能减少不同图像之间的不一致性，便于卷积操作提取共同的特征。</p>
<p>值得注意的是，图像的spacing保持一致，图像中像素值的个数（即图像分辨率）却不一定相同。而分割网络的框架是固定的，一般是需要输入图片的分辨率大小是一致的。所以通常在训练的时候是用一个固定大小的patch从图像中裁剪采样，在这个过程中原图分辨率不一样是不影响的。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">resample</span>(<span class="params">image, scan, new_spacing=[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>]</span>):</span></span><br><span class="line">    <span class="comment"># Determine current pixel spacing</span></span><br><span class="line">    spacing = np.array([scan[<span class="number">0</span>].SliceThickness] + scan[<span class="number">0</span>].PixelSpacing, dtype=np.float32)</span><br><span class="line"></span><br><span class="line">    resize_factor = spacing / new_spacing</span><br><span class="line">    new_real_shape = image.shape * resize_factor</span><br><span class="line">    new_shape = np.<span class="built_in">round</span>(new_real_shape)</span><br><span class="line">    real_resize_factor = new_shape / image.shape</span><br><span class="line">    new_spacing = spacing / real_resize_factor</span><br><span class="line"></span><br><span class="line">    image = scipy.ndimage.interpolation.zoom(image, real_resize_factor, mode=<span class="string">&#x27;nearest&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> image, new_spacing</span><br><span class="line"></span><br><span class="line">pix_resampled, spacing = resample(first_patient_pixels, first_patient, [<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Shape before resampling\t&quot;</span>, first_patient_pixels.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Shape after resampling\t&quot;</span>, pix_resampled.shape)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 画3D图像，用立方体为我们的3D对象创建一个近似网格</span></span><br><span class="line"><span class="comment"># 采用的阈值可以被用来绘制某些结构，例如所有组织或仅骨骼。 400是仅显示骨骼的阈值（见上面的Hounsfield单位表）</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_3d</span>(<span class="params">image, threshold=-<span class="number">300</span></span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Position the scan upright, </span></span><br><span class="line">    <span class="comment"># so the head of the patient would be at the top facing the camera</span></span><br><span class="line">    p = image.transpose(<span class="number">2</span>,<span class="number">1</span>,<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    verts, faces = measure.marching_cubes(p, threshold)</span><br><span class="line"></span><br><span class="line">    fig = plt.figure(figsize=(<span class="number">10</span>, <span class="number">10</span>))</span><br><span class="line">    ax = fig.add_subplot(<span class="number">111</span>, projection=<span class="string">&#x27;3d&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Fancy indexing: `verts[faces]` to generate a collection of triangles</span></span><br><span class="line">    mesh = Poly3DCollection(verts[faces], alpha=<span class="number">0.70</span>)</span><br><span class="line">    face_color = [<span class="number">0.45</span>, <span class="number">0.45</span>, <span class="number">0.75</span>]</span><br><span class="line">    mesh.set_facecolor(face_color)</span><br><span class="line">    ax.add_collection3d(mesh)</span><br><span class="line"></span><br><span class="line">    ax.set_xlim(<span class="number">0</span>, p.shape[<span class="number">0</span>])</span><br><span class="line">    ax.set_ylim(<span class="number">0</span>, p.shape[<span class="number">1</span>])</span><br><span class="line">    ax.set_zlim(<span class="number">0</span>, p.shape[<span class="number">2</span>])</span><br><span class="line"></span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>



<p>肺部切割</p>
<ul>
<li>阈值图像（-320 HU）</li>
<li>做连接组件，确定人周围的空气标签，在二进制图像中用1s填充</li>
<li>可选：对于扫描中的每个轴向切片，确定最大的固体连接组件（人体周围的身体+空气），并将其他组件设置为0。这样可以填充面罩中肺部的结构。</li>
<li>只保留最大的气袋（人体在这里和那里都有其他的气袋）</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">largest_label_volume</span>(<span class="params">im, bg=-<span class="number">1</span></span>):</span></span><br><span class="line">    vals, counts = np.unique(im, return_counts=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    counts = counts[vals != bg]</span><br><span class="line">    vals = vals[vals != bg]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(counts) &gt; <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> vals[np.argmax(counts)]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">segment_lung_mask</span>(<span class="params">image, fill_lung_structures=<span class="literal">True</span></span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># not actually binary, but 1 and 2. </span></span><br><span class="line">    <span class="comment"># 0 is treated as background, which we do not want</span></span><br><span class="line">    binary_image = np.array(image &gt; -<span class="number">320</span>, dtype=np.int8)+<span class="number">1</span></span><br><span class="line">    labels = measure.label(binary_image)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Pick the pixel in the very corner to determine which label is air.</span></span><br><span class="line">    <span class="comment">#   Improvement: Pick multiple background labels from around the patient</span></span><br><span class="line">    <span class="comment">#   More resistant to &quot;trays&quot; on which the patient lays cutting the air </span></span><br><span class="line">    <span class="comment">#   around the person in half</span></span><br><span class="line">    background_label = labels[<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment">#Fill the air around the person</span></span><br><span class="line">    binary_image[background_label == labels] = <span class="number">2</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Method of filling the lung structures (that is superior to something like </span></span><br><span class="line">    <span class="comment"># morphological closing)</span></span><br><span class="line">    <span class="keyword">if</span> fill_lung_structures:</span><br><span class="line">        <span class="comment"># For every slice we determine the largest solid structure</span></span><br><span class="line">        <span class="keyword">for</span> i, axial_slice <span class="keyword">in</span> <span class="built_in">enumerate</span>(binary_image):</span><br><span class="line">            axial_slice = axial_slice - <span class="number">1</span></span><br><span class="line">            labeling = measure.label(axial_slice)</span><br><span class="line">            l_max = largest_label_volume(labeling, bg=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> l_max <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>: <span class="comment">#This slice contains some lung</span></span><br><span class="line">                binary_image[i][labeling != l_max] = <span class="number">1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    binary_image -= <span class="number">1</span> <span class="comment">#Make the image actual binary</span></span><br><span class="line">    binary_image = <span class="number">1</span>-binary_image <span class="comment"># Invert it, lungs are now 1</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Remove other air pockets insided body</span></span><br><span class="line">    labels = measure.label(binary_image, background=<span class="number">0</span>)</span><br><span class="line">    l_max = largest_label_volume(labels, bg=<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">if</span> l_max <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>: <span class="comment"># There are air pockets</span></span><br><span class="line">        binary_image[labels != l_max] = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> binary_image</span><br><span class="line"></span><br><span class="line">segmented_lungs = segment_lung_mask(pix_resampled, <span class="literal">False</span>)</span><br><span class="line">segmented_lungs_fill = segment_lung_mask(pix_resampled, <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">plot_3d(segmented_lungs, <span class="number">0</span>)</span><br><span class="line">plot_3d(segmented_lungs_fill, <span class="number">0</span>)</span><br></pre></td></tr></table></figure>

<p>当使用这个时，记得首先在它上面应用扩张形态学操作（即用圆形内核）。这会在所有方向上扩展蒙版。仅肺部的空气+结构将不包含所有结节，特别是它会遗漏那些粘在肺部侧面的结节，它们经常出现在那里！所以扩大面具一点:)</p>
<p>对于某些边缘情况，此分段可能会失败。它依赖于患者体外的空气不与肺部空气相连的事实。如果患者进行了气管造口术，情况也可能并非如此，不知道这是否存在于数据集中。此外，特别是噪声图像（例如由于下图中的起搏器），这种方法也可能失败。相反，身体中的第二大气袋将被分割。您可以通过检查蒙版对应的图像分数来识别这一点，对于这种情况，这将是非常小的。然后，你可以首先使用几毫米大小的内核进行形态学关闭操作以关闭这些孔，之后它应该可以工作（或者更简单地说，不要对此图像使用蒙版）。</p>
<p>数据归一化</p>
<p>目前所有的数值在-1024到2000左右。超过400的任何东西其实是不用关心的，因为只是一些具有不同辐射密度的骨骼。 常用的阈值集合在-1000到400之间。这里有一些代码可以使用：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">MIN_BOUND = -<span class="number">1000.0</span></span><br><span class="line">MAX_BOUND = <span class="number">400.0</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">normalize</span>(<span class="params">image</span>):</span></span><br><span class="line">    image = (image - MIN_BOUND) / (MAX_BOUND - MIN_BOUND)</span><br><span class="line">    image[image&gt;<span class="number">1</span>] = <span class="number">1.</span></span><br><span class="line">    image[image&lt;<span class="number">0</span>] = <span class="number">0.</span></span><br><span class="line">    <span class="keyword">return</span> image</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据零居中</span></span><br><span class="line"><span class="comment"># 将数据平均值设置为零。为此，只需从所有像素中减去平均像素值。</span></span><br><span class="line">PIXEL_MEAN = <span class="number">0.25</span> <span class="comment"># 假设均值为0.25</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">zero_center</span>(<span class="params">image</span>):</span></span><br><span class="line">    image = image - PIXEL_MEAN</span><br><span class="line">    <span class="keyword">return</span> image</span><br></pre></td></tr></table></figure>



<p>未完 参考 <a href="https://zhuanlan.zhihu.com/p/59413289">https://zhuanlan.zhihu.com/p/59413289</a></p>
]]></content>
      <categories>
        <category>学习</category>
        <category>Python</category>
        <category>图像识别</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>医学图像</tag>
      </tags>
  </entry>
  <entry>
    <title>关于MLPerf和AIPerf的设计理解</title>
    <url>/2021/11/11/%E5%85%B3%E4%BA%8EMLPerf%E5%92%8CAIPerf%E7%9A%84%E8%AE%BE%E8%AE%A1%E7%90%86%E8%A7%A3/</url>
    <content><![CDATA[<p>摘要</p>
<p>问题解决：</p>
<p>原理</p>
<p>优点</p>
<p>缺点</p>
<span id="more"></span>

<h1 id="MLPerf"><a href="#MLPerf" class="headerlink" title="MLPerf"></a>MLPerf</h1><p>MLPerf 是AI芯片的一个基准测试，主要包括：Training 和Inference两个方面的性能测试。Training是于测量系统将模型训练到目标质量指标的速度；Inference是用于测试系统使用训练有素的模型处理输入和产生结果的速度。</p>
<p>具体的详细信息可参见论文：MLPERF TRAINING BENCHMARK以及MLPERF INFERENCE BENCHMARK。</p>
<p>该基准有两种测试形式：*<strong>Closed Division*</strong> 和 <em><strong>Open Division，</strong></em>其中封闭的测试是基于平台固定的测试模型进行测试；开放的测试是用户可以自定义需要测试的模型。</p>
<p>当前的测试结果根据芯片当前的使用状态分为：Cloud;Available;Preview;Research, Development, and Other（category）。</p>
<p><img src="/2021/11/11/%E5%85%B3%E4%BA%8EMLPerf%E5%92%8CAIPerf%E7%9A%84%E8%AE%BE%E8%AE%A1%E7%90%86%E8%A7%A3/zhendeliu/Documents/GitHub/gitblog/source/_posts/%E5%85%B3%E4%BA%8EMLPerf%E5%92%8CAIPerf%E7%9A%84%E8%AE%BE%E8%AE%A1%E7%90%86%E8%A7%A3/8-benchmarks.png"></p>
<h1 id="AIPerf"><a href="#AIPerf" class="headerlink" title="AIPerf"></a>AIPerf</h1><p>大规模人工智能计算系统的计算持续发展，需要一个有效评价计算机系统人工智能算力的评价指标；</p>
<p>浮点计算峰值是指计算机每秒可以完成的浮点计算次数，包括理论浮点峰值和实测浮点峰值。理论浮点峰值是该计算机理论上每秒可以完成的浮点计算次数，主要由CPU的主频决定。</p>
<p>理论浮点峰值＝CPU主频×CPU核数×CPU每周期执行浮点运算的次数。</p>
<p>目前的实测方式如下：</p>
<p>Linpack Benchmark: 广泛用于高性能计算机双精度浮点运算性能基准评测，但大部分人工智能训练任务以单精度浮点数或者16位浮点数为主</p>
<p>HPL-AI: 通以Linpack为基础的改善的混合精度的基准测评</p>
<p>Mobile AI Bench：</p>
<p>DeepBench：</p>
<p>AIIA DNN Benchmark:</p>
<p>AIPerf Benchmark:</p>
<p> 基于微软NNI（neural network intelligence)框架实现，以自动化机器学习AutoML为负载，进行网络结构搜索（网络态射network morphism ）和超参搜索（树状结构Parzen估计）寻找进度更高的网络结构和超参（batch size；最大epoch；lr；最大搜索模型个数，最长搜索总时间；最大并发搜索模型数量等）</p>
<h1 id="设计目标"><a href="#设计目标" class="headerlink" title="设计目标"></a>设计目标</h1><h2 id="统一的评价指标Score"><a href="#统一的评价指标Score" class="headerlink" title="统一的评价指标Score"></a>统一的评价指标Score</h2><p>AIPerf的评价指标是Tops（平均每秒处理的混合精度AI浮点操作数），可以用来做横向比较</p>
<h2 id="处理问题规模可变性"><a href="#处理问题规模可变性" class="headerlink" title="处理问题规模可变性"></a>处理问题规模可变性</h2><p>人工智能计算集群的系统规模在节点数量，加速器数量，加速类型，内存大小等指标上存在差异。需要能适应各种规模的计算集群，因此，在AutoML调整问题规模来适应集群规模的变化。</p>
<h2 id="实际的人工智能意义"><a href="#实际的人工智能意义" class="headerlink" title="实际的人工智能意义"></a>实际的人工智能意义</h2><p>相较于传统的高性能计算机基准测试程序需要具有人工智能的意义在其中，也是其能够检测集群人工智能算力的核心。AIPerf通过在ImageNet数据集上训练神经网络运行CV方面的算法。</p>
<h2 id="必要的多机通信"><a href="#必要的多机通信" class="headerlink" title="必要的多机通信"></a>必要的多机通信</h2><p>在计算集群中任务分发，多级训练，结果收集等都是以多机通信为基础，是人工智能计算集群计算能力的重要组成。</p>
<h1 id="论文精读"><a href="#论文精读" class="headerlink" title="论文精读"></a>论文精读</h1>]]></content>
  </entry>
  <entry>
    <title>图像分割-Unet[项目应用]</title>
    <url>/2021/09/29/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2-Unet/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>背景问题：</p>
<p>方法简述：</p>
<p>优点：</p>
<p>缺点：</p>
<span id="more"></span>
]]></content>
      <categories>
        <category>学习</category>
        <category>cv算法</category>
        <category>Pytorch</category>
        <category>深度学习</category>
        <category>图像分割</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>论文复现</tag>
        <tag>项目总结</tag>
      </tags>
  </entry>
  <entry>
    <title>图像分类-EfficientNet&amp;EfficientDet[项目应用]</title>
    <url>/2021/09/29/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB-EfficientNet/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>背景问题：</p>
<p>方法简述：EfficientNet每个主干包含7个block。这些block还有不同数量的子block，这些子block的数量随着EfficientNetB0到EfficientNetB7而增加。</p>
<p>优点：</p>
<p>缺点：</p>
<span id="more"></span>

<p>EfficientNet每个主干包含7个block。这些block还有不同数量的子block，这些子block的数量随着EfficientNetB0到EfficientNetB7而增加。</p>
<p>EfficientNet-B0的总层数，总数是237层，而EfficientNet-B7的总数是813层！！但不用担心，所有这些层都可以由下面的5个模块和上面的主干组成。</p>
<p><img src="/2021/09/29/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB-EfficientNet/EfficientNet.png"></p>
]]></content>
      <categories>
        <category>学习</category>
        <category>cv算法</category>
        <category>深度学习</category>
        <category>图像分类</category>
        <category>目标检测</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>论文复现</tag>
        <tag>项目总结</tag>
        <tag>kaggle</tag>
      </tags>
  </entry>
  <entry>
    <title>图像处理基础-滤波器</title>
    <url>/2021/09/29/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E5%9F%BA%E7%A1%80-%E6%BB%A4%E6%B3%A2%E5%99%A8/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>在图像处理中，经常需要对图像进行平滑去噪、锐化、边界增强等，这些功能可以通过滤波器filter来实现。</p>
<p>噪音的分类及如何添加</p>
<p>椒盐噪声：是一种随机出现的白点(salt)或者黑点(peppe)</p>
<p>高斯噪声：概率密度函数服从高斯分布（即正态分布）的一类噪声</p>
<p>常用滤波算法：</p>
<p>均值滤波器：把邻域内的平均值赋给中心元素。邻域内的像素权重是相等的；但不能保护细节，不能消除椒盐噪声</p>
<p>高斯滤波器：邻域内各个像素值不同权重的和，将中心点的权重增大，远离中心的的权重减小</p>
<p>中值滤波器：不使用权重，邻域内所有像素值的中间值来代替当前像素点的像素值</p>
<p>双边滤波器：同时考虑距离信息（距离越远，权重越小)和色彩信息（色彩差别越大，权重越小）</p>
<span id="more"></span>

<h1 id="滤波相关概念："><a href="#滤波相关概念：" class="headerlink" title="滤波相关概念："></a>滤波相关概念：</h1><p>图像的时域： 自变量是时间,即横轴是时间,纵轴是信号的变化。其动态信号x（t）是描述信号在不同时刻取值的函数</p>
<p>图像的频域：自变量是频率,即横轴是频率,纵轴是该频率信号的幅度,也就是通常说的频谱图。频谱图描述了信号的频率结构及频率与该频率信号幅度的关系</p>
<p>图像的频率： 图像的频率又称为空间频率，它反映了图像的像素灰度在空间中变化的情况</p>
<p>如何定量的测量图像的空间频率，最为常用的方法就是二维傅里叶变换。图像经过二维傅里叶变换后会形成与图像等大的复数矩阵，取其幅值形成幅度谱，取其相位形成相位谱。图像的频率能量分布主要体现在幅度谱中。通常习惯将低频成分放在幅度谱的中央，而将高频成分放在幅度谱边缘。</p>
<p>在图像频域里面，频率低的地方是比较平滑的，低频的区域中灰度值变化是比较小的；频率高的地方通常是边缘或者噪声，这些区域灰度值是突变的</p>
<p>高通滤波：让频率较高的部分通过，突出边缘等</p>
<p>低通滤波：保留频率比较低的部分，通常为平滑图像，弱化边缘，消除噪声</p>
<p>在时域中的滤波器和在频域中的滤波器组成了傅里叶变换对，这部分暂时不深入了。</p>
<h2 id="滤波器分类"><a href="#滤波器分类" class="headerlink" title="滤波器分类"></a>滤波器分类</h2><p>线性滤波： 对邻域中的像素的计算为线性运算时，如利用窗口函数进行平滑加权求和的运算，或者某种卷积运算，都可以称为线性滤波。常见的线性滤波有：方框滤波、均值滤波、高斯滤波、拉普拉斯滤波等等，通常线性滤波器之间只是模版的系数不同。</p>
<p>非线性滤波： 非线性滤波利用原始图像跟模版之间的一种逻辑关系得到结果，如最值滤波器，中值滤波器。比较常用的有中值滤波器和双边滤波器。</p>
<p>reference：<a href="https://blog.csdn.net/qq_44957388/article/details/105763906">https://blog.csdn.net/qq_44957388/article/details/105763906</a></p>
<h1 id="常见的滤波器："><a href="#常见的滤波器：" class="headerlink" title="常见的滤波器："></a>常见的滤波器：</h1><h2 id="均值滤波器"><a href="#均值滤波器" class="headerlink" title="均值滤波器"></a>均值滤波器</h2><p><img src="/2021/09/29/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E5%9F%BA%E7%A1%80-%E6%BB%A4%E6%B3%A2%E5%99%A8/%E5%9D%87%E5%80%BC.png"></p>
<h3 id="均值滤波的缺点"><a href="#均值滤波的缺点" class="headerlink" title="均值滤波的缺点"></a>均值滤波的缺点</h3><p>均值滤波本身存在着固有的缺陷，即它不能很好地保护图像细节，在图像去噪的同时也破坏了图像的细节部分，从而使图像变得模糊，不能很好地去除噪声点。特别是椒盐噪声。</p>
<h3 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h3><p>均值模糊可以模糊图像以便得到感兴趣物体的粗略描述，也就是说，去除图像中的不相关细节，其中“不相关”是指与滤波器模板尺寸相比较小的像素区域，从而对图像有一个整体的认知。即为了对感兴趣的物体得到一个大致的整体的描述而模糊一幅图像，忽略细小的细节。</p>
<h2 id="高斯滤波器"><a href="#高斯滤波器" class="headerlink" title="高斯滤波器"></a>高斯滤波器</h2><p><img src="/2021/09/29/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E5%9F%BA%E7%A1%80-%E6%BB%A4%E6%B3%A2%E5%99%A8/%E9%AB%98%E6%96%AF.png"></p>
<p><strong>应用：</strong> 高斯滤波是一种线性平滑滤波器，对于服从正态分布的噪声有很好的抑制作用。在实际场景中，我们通常会假定图像包含的噪声为高斯白噪声，所以在许多实际应用的预处理部分，都会采用高斯滤波抑制噪声，如传统车牌识别等。</p>
<h2 id="中值滤波器"><a href="#中值滤波器" class="headerlink" title="中值滤波器"></a>中值滤波器</h2><p><img src="/2021/09/29/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E5%9F%BA%E7%A1%80-%E6%BB%A4%E6%B3%A2%E5%99%A8/%E4%B8%AD%E5%80%BC.png"></p>
<p>中值滤波不再采用加权求和的方式计算滤波结果，它用邻域内所有像素值的中间值来代替当前像素点的像素值。</p>
<p>中值滤波会取当前像素点及其周围临近像素点的像素值，一般有奇数个像素点，将这些像素值排序，将排序后位于中间位置的像素值作为当前像素点的像素值。</p>
<p>中值滤波对于斑点噪声（speckle noise）和椒盐噪声（salt-and-pepper<br>noise）来说尤其有用，因为它不依赖于邻域内那些与典型值差别很大的值，而且噪声成分很难被选上，所以可以在几乎不影响原有图像的情况下去除全部噪声。但是由于需要进行排序操作，中值滤波的计算量较大。</p>
<p>中值滤波器在处理连续图像窗函数时与线性滤波器的工作方式类似，但滤波过程却不再是加权运算。</p>
<h2 id="双边滤波器"><a href="#双边滤波器" class="headerlink" title="双边滤波器"></a>双边滤波器</h2><p>双边滤波是综合考虑空间信息和色彩信息的滤波方式，在滤波过程中能有效的保护图像内的边缘信息。</p>
<p>双边滤波在计算某一个像素点的像素值时，同时考虑距离信息（距离越远，权重越小)和色彩信息（色彩差别越大，权重越小）。既能去除噪声，又能较好的保护边缘信息。</p>
<p><img src="/2021/09/29/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E5%9F%BA%E7%A1%80-%E6%BB%A4%E6%B3%A2%E5%99%A8/%E5%8F%8C%E8%BE%B9.png"></p>
<p>在双边滤波中，计算左侧白色区域的滤波结果时：</p>
<p>对于白色的点，权重较大<br>对于黑色的点，与白色的色彩差别较大（0和255），所以可以将他们的权重设置为0<br>计算右侧黑色区域的滤波结果时：</p>
<p>对于黑色的点，权重较大<br>对于白色的点，与黑色的色彩差别较大（255和0），所以可以将他们的权重设置为0<br>这样，左侧白色的滤波结果仍是白色，黑色的像素点权重为0，对它不会有影响；右侧黑色的滤波结果仍是黑色，白色的像素点权重为0，对它不会有影响。所以，双边滤波会将边缘信息保留。</p>
<p>参考链接：<a href="https://blog.csdn.net/qq_44957388/article/details/105763906">https://blog.csdn.net/qq_44957388/article/details/105763906</a></p>
]]></content>
      <categories>
        <category>学习</category>
        <category>cv算法</category>
      </categories>
      <tags>
        <tag>OpenCV</tag>
        <tag>基础操作</tag>
        <tag>噪音</tag>
        <tag>编程</tag>
        <tag>滤波器</tag>
      </tags>
  </entry>
  <entry>
    <title>图像识别-[2012]AlexNet</title>
    <url>/2021/09/29/%E5%9B%BE%E5%83%8F%E8%AF%86%E5%88%AB-AlexNet/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>AlexNet是2012年ImageNet竞赛冠军获得者Alex Krizhevsky设计的</p>
<p>方法简述：AlexNet有6亿个参数和650,000个神经元，包含5个卷积层，有些层后面跟了max-pooling层，3个全连接层，为了减少过拟合，在全连接层使用了dropout</p>
<p>优点：</p>
<ol>
<li>使用Relu作为激活函数</li>
<li>使用Dropou避免过拟合</li>
<li>使用重叠的MaxPooling,让stride小于池化核的大小，池化层的输出有重叠和覆盖，提升了特征的丰富性</li>
<li>？？？提出LRN层局部响应归一化层？？？</li>
<li>使用分组卷机通过CUDA加速深度卷机网络的训练</li>
<li>数据增强，避免过拟合</li>
</ol>
<p>缺点：</p>
<ol>
<li>参数过多，计算量大</li>
<li>网络不深，准确率不高</li>
</ol>
<span id="more"></span>

<p>ImageNet是图像算法领域最常见的数据集，训练集包含120万张图片，验证集包含5万张图片，测试集包含15万张图片，这些图片分为了1000个类别，并且有多种不同的分辨率，</p>
<h1 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h1><p><img src="/2021/09/29/%E5%9B%BE%E5%83%8F%E8%AF%86%E5%88%AB-AlexNet/%E7%BB%93%E6%9E%84.png"></p>
<h1 id="参数计算"><a href="#参数计算" class="headerlink" title="参数计算"></a>参数计算</h1><p><img src="/2021/09/29/%E5%9B%BE%E5%83%8F%E8%AF%86%E5%88%AB-AlexNet/%E5%8F%82%E6%95%B0%E9%87%8F.png"></p>
]]></content>
      <categories>
        <category>学习</category>
        <category>cv算法</category>
        <category>深度学习</category>
        <category>图像识别</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>论文复现</tag>
      </tags>
  </entry>
  <entry>
    <title>图像识别-GoogLeNet</title>
    <url>/2021/09/29/%E5%9B%BE%E5%83%8F%E8%AF%86%E5%88%AB-GoogLeNet/</url>
    <content><![CDATA[<h1 id="GoogleNet"><a href="#GoogleNet" class="headerlink" title="GoogleNet"></a>GoogleNet</h1><h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>背景问题：一般来说提升网络性能最直接的办法是增加网络深度（卷积层数）和宽度（神经元个数），但是伴随而来的是参数过多，容易过拟合；网络庞大计算复杂；梯度弥散，难以优化；所以需要设计一种模型：1. 让网络模型结构变得稀疏；2.能利用密集矩阵的高性能计算（大部分硬件是对密集矩阵计算优化的，稀疏矩阵计算时耗费的时间并没有减小）</p>
<p>方法简述：</p>
<p>Inception网络结构就是为了构造一种基础神经元结构，来搭建稀疏且有高性能的网络结构。</p>
<p>Inception V1: 卷积核分别采用1、3、5有不同的感受野，最后拼接意味着不同尺度特征的融合</p>
<p>Inception V2: 引入了BatchNormalization，</p>
<p>Inception V3: 引入分解</p>
<p>Inception V4: 结合Residual Connection</p>
<p>优点：</p>
<p>1）增加了网络的宽度；</p>
<p>2）增加了网络对尺度的适应性，提高了网络内部计算资源的利用率；</p>
<p>3）1x1减少网络参数，且起到信息融合的作用。</p>
<p>缺点：</p>
<span id="more"></span>



<h1 id="Inception-V1"><a href="#Inception-V1" class="headerlink" title="Inception V1:"></a>Inception V1:</h1><p><img src="/2021/09/29/%E5%9B%BE%E5%83%8F%E8%AF%86%E5%88%AB-GoogLeNet/InceptionV1.png"></p>
<p>Inception的结构如图所示，其中1*1卷积主要用来降维，用了Inception之后整个网络结构的宽度和深度都可扩大，能够带来2-3倍的性能提升。</p>
<p><img src="/2021/09/29/%E5%9B%BE%E5%83%8F%E8%AF%86%E5%88%AB-GoogLeNet/V1-architecture.png"></p>
<p>对Inception的结构做以下说明：</p>
<ol>
<li><p>采用不同大小的卷积核意味着不同大小的感受野，最后拼接意味着不同尺度特征的融合；</p>
</li>
<li><p>卷积核大小采用1、3和5，主要是为了方便对齐。设定卷积步长stride=1后，只要分别设定pad=0、1、2，那么卷积后便可以得到相同维度的特征，然后这些特征就可以直接拼接在一起了；</p>
</li>
<li><p>文章说很多地方都表明pooling挺有效，所以Inception里面也嵌入了;</p>
</li>
<li><p>网络越到后面，特征越抽象，而且每个特征所涉及的感受野也更大了，因此随着层数的增加，3x3和5x5卷积的比例也要增加。</p>
</li>
<li><p>使用5x5的卷积核仍然会带来巨大的计算量。 为此，文章借鉴NIN2卷积神经网络 1*1 卷积核 ，采用1x1卷积核来进行降维。</p>
</li>
<li><p>该模型最后采用了average pooling来代替全连接层。但是，实际在最后还是加了一个全连接层，主要是为了方便以后大家finetune。</p>
</li>
</ol>
<p><img src="/2021/09/29/%E5%9B%BE%E5%83%8F%E8%AF%86%E5%88%AB-GoogLeNet/global-average-pooling.jpeg"></p>
<p>————————————————<br>参考链接：<a href="https://blog.csdn.net/weixin_42535423/article/details/103674098">https://blog.csdn.net/weixin_42535423/article/details/103674098</a></p>
<h1 id="Inception-V2"><a href="#Inception-V2" class="headerlink" title="Inception V2"></a>Inception V2</h1><p>作者在论文中提出了几个网路结构的设计准则</p>
<ol>
<li>避免表达瓶颈，尤其是网络早期：表达瓶颈也就是高度压缩的层，它会损失大量有用的信息。另外特征的维度（通道数）会逐渐增加，维度通道数不代表信息的多少</li>
<li>高维特征更容易处理：高维特征是经过多次非线性映射从而带有更多的判别信息，网络更容易训练</li>
<li>可以在低维特征时进行空间融合，不必担心信息过多损失，有助于加速训练</li>
<li>平衡网络深度和宽度，过宽或过深都不能达到最优</li>
</ol>
<p>改进：</p>
<ol>
<li><p>在输入时加入batch_normal层，加速收敛，减少使用dropout</p>
</li>
<li><p>将V1版本中的 5x5改进为两个3x3的卷积</p>
</li>
<li><p><strong>Inception-V2 其他优势与思考</strong>：</p>
<p>\1) 可以使用更高的学习率；</p>
<p>  解释：如果每层的scale不一致，实际上每层需要的学习率不一致。同一层不同维度的scale往往也需要不同大小的学习率，通常使用最小的那个学习率才能保证损失函数有效下降，Batch Normalization将每层、每维的scale保持一致，才可以直接使用较高的学习率进行优化。</p>
<p>\2) 移除或使用较低的dropout；</p>
<p>   解释：dropout是常用的防止overfit的方法，而导致overfit的位置往往在数据边界处，如果初始化权重就已经落在数据内部，overfit现象就可以得到一定的缓解。在论文中，最后的模型分别使用10%、5%和0%的dropout训练模型，与之前的40%-50%相比，可以大大提高训练速度。</p>
<p>\3) 降低L2权重衰减系数；</p>
<p>  解释：边界处的局部最优往往有几维的权重（斜率）较大，使用L2衰减可以缓解这一问题，使用Batch Normalization，就可以把这个值降低了，论文中降低为原来的5倍。</p>
<p>\4) 取消Local Response Normalization层；</p>
<p>  解释：由于使用了一种Normalization，再使用LRN就显得没那么必要了。而且LRN实际上也没那么work。</p>
<p>\5) 减少图像扭曲的使用；</p>
<p>  解释：由于现在训练epoch数降低，所以要对输入数据少做一些扭曲，让神经网络多看看真实的数据。</p>
<p>参考：Hugh_1：<a href="https://www.jianshu.com/p/ddfcf7ed08ab">https://www.jianshu.com/p/ddfcf7ed08ab</a></p>
</li>
</ol>
<h1 id="Inception-V3"><a href="#Inception-V3" class="headerlink" title="Inception V3"></a>Inception V3</h1><ol>
<li>将nxn的卷积代替为两个不对称的1xn和nx1的卷积，比如将3x3的卷积在输入输出filter一定的情况下分解为1x3和3x1的卷积，计算量减下33%，分解为2个2x2的卷积只能减少11%</li>
</ol>
<p><img src="/2021/09/29/%E5%9B%BE%E5%83%8F%E8%AF%86%E5%88%AB-GoogLeNet/V3-architecture.png"></p>
<center>Inception V3 网络架构</center>

<p><img src="/2021/09/29/%E5%9B%BE%E5%83%8F%E8%AF%86%E5%88%AB-GoogLeNet/V3-A.png">两个3X3取代5X5</p>
<p><img src="/2021/09/29/%E5%9B%BE%E5%83%8F%E8%AF%86%E5%88%AB-GoogLeNet/V3-B.png"> 7X1和1X7取代7X7</p>
<p><img src="/2021/09/29/%E5%9B%BE%E5%83%8F%E8%AF%86%E5%88%AB-GoogLeNet/V3-C.png"></p>
<p>3X1和1X3取代3X3</p>
<p>![](图像识别-GoogLeNet/V3-Grid Size Reduction.png)</p>
<p>下采样时采用conv+max pooling来取代max pooling，避免信息损失</p>
<p>![](图像识别-GoogLeNet/V3-Auxiliary Classifier.png)</p>
<p>Auxiliary Classifier辅助分类器： 在V3最后的 17×17 层的顶部仅使用 1 个辅助分类器，而不是使用 2 个辅助分类器，目的也不一样。在 GoogLeNet / Inception-v1 [4] 中，辅助分类器用于拥有更深的网络。在 Inception-v3 中，辅助分类器用作正则化。所以，实际上，在深度学习中，模块还是很直观的。</p>
<p>Inception-v2 [6] 中建议的批量归一化也用于辅助分类器</p>
<p><a href="https://sh-tsang.medium.com/review-inception-v3-1st-runner-up-image-classification-in-ilsvrc-2015-17915421f77c">https://sh-tsang.medium.com/review-inception-v3-1st-runner-up-image-classification-in-ilsvrc-2015-17915421f77c</a></p>
<p><strong>优点</strong>：</p>
<p>  1）加速计算（多余的计算能力可以用来加深网络），将1个conv拆成2个conv，使得网络深度进一步增加，增加了网络的非线性；</p>
<p>  2）网络输入从224x224变为了299x299，更加精细设计了35x35/17x17/8x8的模块。</p>
<p><strong>细节：</strong></p>
<p>  1）在辅助层加入了BN-auxiliary</p>
<p>  2）全连接层后面也进行BN操作。</p>
<h1 id="Inception-V4"><a href="#Inception-V4" class="headerlink" title="Inception V4"></a>Inception V4</h1><p><img src="/2021/09/29/%E5%9B%BE%E5%83%8F%E8%AF%86%E5%88%AB-GoogLeNet/V4.png"></p>
<p>ResNet的结构可以极大地加速训练，同时性能也有提升。</p>
]]></content>
      <categories>
        <category>学习</category>
        <category>cv算法</category>
        <category>深度学习</category>
        <category>图像识别</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>论文复现</tag>
      </tags>
  </entry>
  <entry>
    <title>图像识别-ResNet</title>
    <url>/2021/09/29/%E5%9B%BE%E5%83%8F%E8%AF%86%E5%88%AB-ResNet/</url>
    <content><![CDATA[<h1 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a>ResNet</h1><h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>背景问题：<strong>训练集上的性能下降，可以排除过拟合，BN层的引入也基本解决了plain net的梯度消失和梯度爆炸问题。</strong>如果不是过拟合以及梯度消失导致的，那原因是什么？为什么非常深度的网络在增加更多层时会表现得更差？深度网络点退化（Degradation problem）问题</p>
<p>方法简述：</p>
<p>ResNet网络是参考了VGG19网络，在其基础上进行了修改，并通过短路机制加入了残差单元，如图5所示。变化主要体现在ResNet直接使用stride=2的卷积做下采样，并且用global average pool层替换了全连接层。ResNet的一个重要设计原则是：当feature map大小降低一半时，feature map的数量增加一倍，这保持了网络层的复杂度。</p>
<p>优点：</p>
<p>ResNet通过残差学习解决了深度网络的退化问题，让我们可以训练出更深的网络</p>
<p>缺点：</p>
<span id="more"></span>

<p>认知上，深层网络不会比浅层网络的表现更差。基本假设是第N+1层什么也没学到也只是简单的copy或者恒等映射了前一层的结果，也至少和前一层有相同的准确度。</p>
<p><img src="/2021/09/29/%E5%9B%BE%E5%83%8F%E8%AF%86%E5%88%AB-ResNet/ResNet-Architecture.png"></p>
<p>参考：<a href="https://www.cnblogs.com/shine-lee/p/12363488.html">https://www.cnblogs.com/shine-lee/p/12363488.html</a></p>
<p>ResNet的设计有如下特点：</p>
<ul>
<li>与plain net相比，ResNet多了很多“旁路”，即shortcut路径，其首尾圈出的layers构成一个Residual Block；</li>
<li>ResNet中，所有的Residual Block都没有pooling层，<strong>降采样是通过conv的stride实现的</strong>；</li>
<li>分别在conv3_1、conv4_1和conv5_1 Residual Block，降采样1倍，同时feature map数量增加1倍，如图中虚线划定的block；</li>
<li><strong>通过Average Pooling得到最终的特征</strong>，而不是通过全连接层；</li>
<li>每个卷积层之后都紧接着BatchNorm layer，为了简化，图中并没有标出；</li>
</ul>
<p>ResNet结构非常容易修改和扩展，通过调整block内的channel数量以及堆叠的block数量，就可以很容易地调整网络的宽度和深度，来得到不同表达能力的网络，而不用过多地担心网络的“退化”问题，只要训练数据足够，逐步加深网络，就可以获得更好的性能表现。</p>
<p><img src="/2021/09/29/%E5%9B%BE%E5%83%8F%E8%AF%86%E5%88%AB-ResNet/resnet%E6%A8%A1%E5%9D%97.png"></p>
<h1 id="Residual-Block的分析与改进"><a href="#Residual-Block的分析与改进" class="headerlink" title="Residual Block的分析与改进"></a>Residual Block的分析与改进</h1><p><img src="/2021/09/29/%E5%9B%BE%E5%83%8F%E8%AF%86%E5%88%AB-ResNet/ResNet%E6%94%B9%E8%BF%9B.png"></p>
<p>论文：<a href="https://arxiv.org/abs/1603.05027">https://arxiv.org/abs/1603.05027</a></p>
<p>新提出的Residual Block结构，具有更强的泛化能力，能更好地避免“退化”，堆叠大于1000层后，性能仍在变好。具体的变化在于</p>
<ul>
<li><strong>通过保持shortcut路径的“纯净”，可以让信息在前向传播和反向传播中平滑传递，这点十分重要。</strong>为此，如无必要，不引入1×11×1卷积等操作，同时将上图灰色路径上的ReLU移到了𝐹(𝑥)F(x)路径上。</li>
<li>在残差路径上，<strong>将BN和ReLU统一放在weight前作为pre-activation</strong>，获得了“Ease of optimization”以及“Reducing overfitting”的效果。</li>
</ul>
]]></content>
      <categories>
        <category>学习</category>
        <category>cv算法</category>
        <category>深度学习</category>
        <category>图像识别</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>论文复现</tag>
      </tags>
  </entry>
  <entry>
    <title>图像识别-VGGNet</title>
    <url>/2021/09/29/%E5%9B%BE%E5%83%8F%E8%AF%86%E5%88%AB-VGGNet/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>背景问题：随着AlexNet提出，很多人开始利用卷积神经网络来解决图像识别的问题。一般的做法都是重复几层卷积网络，每个卷积网络之后接一些池化层，最后再加上几个全连接层。而如何更优的设计网络结构（深度，kernel size等）</p>
<p>方法简述：用更小更深的卷积核代替大的卷积核，较深的网络层次来提升深度学习的效果。</p>
<p>使用很小的3x3、步长为1的卷积核来扫描输入。两个3x3的卷积核堆起来，和一个5x5的卷积核的感受野一样；三个3x3的卷积核堆起来，其感受野等同于一个7x7的卷积核。</p>
<p>VGG16:</p>
<p>VGG19:</p>
<p>优点：</p>
<ol>
<li>结构简单 整个网络使用相同size 的卷积核（3X3）和最大池化（2X2）</li>
<li>证明更深的层可以使得函数具有更好的分辨能力</li>
<li>用多个较小的卷积核可以减少参数</li>
<li>去掉局部响应归一化模块</li>
</ol>
<p>缺点：</p>
<ol>
<li>计算耗费更多资源，参数量大（主要是全联接层导致的）， 内存空间占用高 140M</li>
</ol>
<span id="more"></span>

<h1 id="基本结构-VGG16"><a href="#基本结构-VGG16" class="headerlink" title="基本结构-VGG16"></a>基本结构-VGG16</h1><p><img src="/2021/09/29/%E5%9B%BE%E5%83%8F%E8%AF%86%E5%88%AB-VGGNet/VGG16%E7%BB%93%E6%9E%84.png"></p>
<p>图片来源：<a href="https://www.cxyzjd.com/article/weixin_26726011/108260201">https://www.cxyzjd.com/article/weixin_26726011/108260201</a></p>
<h1 id="参数计算-VGG16"><a href="#参数计算-VGG16" class="headerlink" title="参数计算-VGG16"></a>参数计算-VGG16</h1><p><img src="/2021/09/29/%E5%9B%BE%E5%83%8F%E8%AF%86%E5%88%AB-VGGNet/%E5%8F%82%E6%95%B0%E8%AE%A1%E7%AE%97.png"></p>
<h1 id="VGG16-VS-VGG19"><a href="#VGG16-VS-VGG19" class="headerlink" title="VGG16 VS VGG19"></a>VGG16 VS VGG19</h1><p><img src="/2021/09/29/%E5%9B%BE%E5%83%8F%E8%AF%86%E5%88%AB-VGGNet/VGG16VS19.png"></p>
<p>图片来源：<a href="https://www.cnblogs.com/jesse123/p/7110721.html">https://www.cnblogs.com/jesse123/p/7110721.html</a></p>
]]></content>
      <categories>
        <category>学习</category>
        <category>cv算法</category>
        <category>深度学习</category>
        <category>图像识别</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>论文复现</tag>
      </tags>
  </entry>
  <entry>
    <title>基于YOLOv5的安全帽和口罩检测</title>
    <url>/2021/10/02/%E5%9F%BA%E4%BA%8EYOLOv5%E7%9A%84%E5%AE%89%E5%85%A8%E5%B8%BD%E5%92%8C%E5%8F%A3%E7%BD%A9%E6%A3%80%E6%B5%8B/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>基于python的CUDA编程基础</title>
    <url>/2021/10/07/%E5%9F%BA%E4%BA%8Epython%E7%9A%84CUDA%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>GPU全名为Graphics Processing Unit，又称视觉处理器、图形显示卡。GPU负责渲染出2D、3D、VR效果，主要专注于计算机图形图像领域。无论是CPU还是GPU，在进行计算时，都需要用核心（Core）来做算术逻辑运算，比如加减乘与或非等。核心中有ALU（逻辑运算单元）和寄存器等电路。在进行计算时，一个核心只能顺序执行某项任务。</p>
<p>个人桌面电脑CPU只有2到8个CPU核心，数据中心的服务器上也只有20到40个左右CPU核心，GPU却有上千个核心。与CPU的核心不同，GPU的核心只能专注于某些特定的任务。知乎上有人把CPU比作大学教授，把GPU比作一个学校几千个小学生：同样是做加减法，几千个小学生所能做的计算，远比几十个大学教授要多得多。</p>
<span id="more"></span>

<p>CPU主要从主存（Main Memory）中读写数据，并通过总线（Bus）与GPU交互。GPU除了有超多计算核心外，也有自己独立的存储，被称之为显存。GPU核心在做计算时，只能直接从显存中读写数据，程序员需要在代码中指明哪些数据需要从内存和显存之间相互拷贝。这些数据传输都是在总线上，因此总线的传输速度和带宽成了部分计算任务的瓶颈。也因为这个瓶颈，很多计算任务并不适合放在GPU上，比如笔者这两年关注的推荐系统虽然也在使用深度学习，但因为输入是大规模稀疏特征，GPU加速获得的收益小于数据互相拷贝的时间损失。</p>
<p><img src="/2021/10/07/%E5%9F%BA%E4%BA%8Epython%E7%9A%84CUDA%E7%BC%96%E7%A8%8B%E5%9F%BA%E7%A1%80/cpu_gpu.png"></p>
<p>PU和主存被称为<strong>Host</strong>，GPU被称为<strong>Device</strong>。Host和Device概念会贯穿整个英伟达GPU编程。PU和主存被称为<strong>Host</strong>，GPU被称为<strong>Device</strong>。Host和Device概念会贯穿整个英伟达GPU编程。</p>
<p>2007年，英伟达发布了CUDA编程模型，软件开发人员从此可以使用CUDA在英伟达的GPU上进行并行编程。在此之前，GPU编程并不友好。</p>
<p>继CUDA之后，英伟达不断丰富其软件技术栈，提供了科学计算所必须的cuBLAS线性代数库，cuFFT快速傅里叶变换库等，当深度学习大潮到来时，英伟达提供了cuDNN深度神经网络加速库，目前常用的TensorFlow、PyTorch深度学习框架的底层大多基于cuDNN库。英伟达能在人工智能时代击败Intel、AMD等强大对手，很大一部分是因为它丰富的软件体系。这些软件工具库使研发人员专注于自己的研发领域，不用再去花大量时间学习GPU底层知识。CUDA对于GPU就像个人电脑上的Windows、手机上的安卓系统，一旦建立好生态，吸引了开发者，用户非常依赖这套软件生态体系。</p>
<p>CUDA是英伟达提供给开发者的一个GPU编程框架，程序员可以使用这个框架轻松地编写并行程序。本系列第一篇文章提到，CPU和主存被称为<strong>主机（Host）</strong>，GPU和显存（显卡内存）被称为<strong>设备（Device）</strong>，CPU无法直接读取显存数据，GPU无法直接读取主存数据，主机与设备必须通过总线（Bus）相互通信。</p>
<p>CUDA程序执行时会独霸一张卡，如果你的机器上有多张GPU卡，CUDA默认会选用0号卡。如果你与其他人共用这台机器，最好协商好谁在用哪张卡。一般使用<code>CUDA_VISIBLE_DEVICES</code>这个环境变量来选择某张卡。如选择5号GPU卡运行你的程序。</p>
<p>CUDA编程的基本流程为：</p>
<ol>
<li>初始化，并将必要的数据拷贝到GPU设备的显存上。</li>
<li>使用某个执行配置，以一定的并行粒度调用CUDA核函数。</li>
<li>CPU和GPU异步计算。</li>
<li>将GPU计算结果拷贝回主机。</li>
</ol>
<p>案例：计算两个2千万维的向量的加法</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> numba <span class="keyword">import</span> cuda</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">from</span> time <span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在GPU核函数上添加@cuda.jit装饰符，表示该函数是一个在GPU设备上运行的函数</span></span><br><span class="line"><span class="meta">@cuda.jit</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gpu_add</span>(<span class="params">a, b, result, n</span>):</span></span><br><span class="line">    <span class="comment"># a, b为输入向量，result为输出向量</span></span><br><span class="line">    <span class="comment"># 所有向量都是n维</span></span><br><span class="line">    <span class="comment"># 得到当前thread的索引</span></span><br><span class="line">    idx = cuda.threadIdx.x + cuda.blockDim.x * cuda.blockIdx.x</span><br><span class="line">    <span class="keyword">if</span> idx &lt; n :</span><br><span class="line">        result[idx] = a[idx] + b[idx]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>():</span></span><br><span class="line">    n = <span class="number">20000000</span></span><br><span class="line">    x = np.arange(n).astype(np.int32)</span><br><span class="line">    y = <span class="number">2</span> * x</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 拷贝数据到设备端</span></span><br><span class="line">    x_device = cuda.to_device(x)</span><br><span class="line">    y_device = cuda.to_device(y)</span><br><span class="line">    <span class="comment"># 在显卡设备上初始化一块用于存放GPU计算结果的空间</span></span><br><span class="line">    gpu_result = cuda.device_array(n)</span><br><span class="line">    cpu_result = np.empty(n)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># CUDA配置</span></span><br><span class="line">    threads_per_block = <span class="number">1024</span></span><br><span class="line">    blocks_per_grid = math.ceil(n / threads_per_block)</span><br><span class="line">    start = time()</span><br><span class="line">    gpu_add[blocks_per_grid, threads_per_block](x_device, y_device, gpu_result, n)</span><br><span class="line">    cuda.synchronize() <span class="comment"># 等待当前设备上所有流中的所有核心完成。</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;gpu vector add time &quot;</span> + <span class="built_in">str</span>(time() - start))</span><br><span class="line">    start = time()</span><br><span class="line">    cpu_result = np.add(x, y)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;cpu vector add time &quot;</span> + <span class="built_in">str</span>(time() - start))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (np.array_equal(cpu_result, gpu_result.copy_to_host())):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;result correct!&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    main()</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>不同的执行配置会影响GPU程序的速度，一般需要多次调试才能找到较好的执行配置，在实际编程中，执行配置<code>[gridDim, blockDim]</code>应参考下面的方法：</p>
<ul>
<li>block运行在SM上，不同硬件架构（Turing、Volta、Pascal…）的CUDA核心数不同，一般需要根据当前硬件来设置block的大小<code>blockDim</code>（执行配置中第二个参数）。一个block中的thread数最好是32、128、256的倍数。==注意，限于当前硬件的设计，block大小不能超过1024。==</li>
<li>grid的大小<code>gridDim</code>（执行配置中第一个参数），即一个grid中block的个数可以由总次数<code>N</code>除以<code>blockDim</code>，并向上取整。</li>
</ul>
<p>例如，我们想并行启动1000个thread，可以将blockDim设置为128，<code>1000 ÷ 128 = 7.8</code>，向上取整为8。使用时，执行配置可以写成<code>gpuWork[8, 128]()</code>，CUDA共启动<code>8 * 128 = 1024</code>个thread，实际计算时只使用前1000个thread，多余的24个thread不进行计算。</p>
<p>注意，这几个变量比较容易混淆，再次明确一下：<code>blockDim</code>是block中thread的个数，一个block中的<code>threadIdx</code>最大不超过<code>blockDim</code>；<code>gridDim</code>是grid中block的个数，一个grid中的<code>blockIdx</code>最大不超过<code>gridDim</code>。</p>
<h1 id="PyTorch使用CUDA"><a href="#PyTorch使用CUDA" class="headerlink" title="PyTorch使用CUDA"></a>PyTorch使用CUDA</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Hello World, Hello PyTorch &#123;&#125;&quot;</span>.<span class="built_in">format</span>(torch.__version__))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\nCUDA is available:&#123;&#125;, version is &#123;&#125;&quot;</span>.<span class="built_in">format</span>(torch.cuda.is_available(), torch.version.cuda))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\ndevice_name: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(torch.cuda.get_device_name(<span class="number">0</span>)))</span><br><span class="line"></span><br><span class="line">device = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>



]]></content>
      <categories>
        <category>学习</category>
        <category>Python</category>
        <category>CUDA</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>GPU</tag>
      </tags>
  </entry>
  <entry>
    <title>常见的算法面试（一）</title>
    <url>/2021/10/03/%E5%B8%B8%E8%A7%81%E7%9A%84%E7%AE%97%E6%B3%95%E9%9D%A2%E8%AF%95%EF%BC%88%E4%B8%80%EF%BC%89/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>底层避障算法ORCA(Optimal Reciprocal Collision Avoidance)</title>
    <url>/2021/11/02/%E5%BA%95%E5%B1%82%E9%81%BF%E9%9A%9C%E7%AE%97%E6%B3%95ORCA-Optimal-Reciprocal-Collision-Avoidance/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>问题解决：</p>
<p>原理</p>
<p>优点</p>
<p>缺点</p>
<span id="more"></span>

<h1 id="VO（Velocity-Obstacle）"><a href="#VO（Velocity-Obstacle）" class="headerlink" title="VO（Velocity Obstacle）"></a>VO（Velocity Obstacle）</h1><p>以两个移动的物体为例：</p>
<ol>
<li>计算两个移动物体的相对速度和速度方向</li>
<li>假设一个物体是静止的，另一个物体以相对速度进行运动，</li>
<li>判断移动的物体的中心是否在一定时间内到达另一个物体的中心为中心和两个物体的半径和为半径的区域内</li>
<li>如果到达则认为会碰撞，否则不会</li>
</ol>
<p>缺点：</p>
<p><img src="/2021/11/02/%E5%BA%95%E5%B1%82%E9%81%BF%E9%9A%9C%E7%AE%97%E6%B3%95ORCA-Optimal-Reciprocal-Collision-Avoidance/zhendeliu/Documents/GitHub/gitblog/source/_posts/%E5%BA%95%E5%B1%82%E9%81%BF%E9%9A%9C%E7%AE%97%E6%B3%95ORCA-Optimal-Reciprocal-Collision-Avoidance/VO%E7%BC%BA%E7%82%B9.jpg"></p>
<p>如图，加入两个物体都采用相同的策略进行移动，两个物体既要确保移动的物体的速度都处于各自的Velocity Obstacle之外，又要尽可能快，所以说速度和方向是不断会被推拉，因此会导致轨迹抖动如图所示，会影响到效率</p>
<h1 id="RVO（Reciprocal-Velocity-Obstacles）"><a href="#RVO（Reciprocal-Velocity-Obstacles）" class="headerlink" title="RVO（Reciprocal Velocity Obstacles）"></a>RVO（Reciprocal Velocity Obstacles）</h1><p>RVO的目标是解决VO中产生的抖动问题。</p>
<p>在避障行为的速度选择时，与VO避障中智能体单纯选择VO区域之外的速度不同，这里我们选择的新速度是VO之外的某个速度与当前速度的平均值</p>
<p><img src="/2021/11/02/%E5%BA%95%E5%B1%82%E9%81%BF%E9%9A%9C%E7%AE%97%E6%B3%95ORCA-Optimal-Reciprocal-Collision-Avoidance/zhendeliu/Documents/GitHub/gitblog/source/_posts/%E5%BA%95%E5%B1%82%E9%81%BF%E9%9A%9C%E7%AE%97%E6%B3%95ORCA-Optimal-Reciprocal-Collision-Avoidance/RVO.png"></p>
<p>如图所示，RVO是将先前的VO进行平移的到的， By the way, 是Va和VO内的任意Vb求平均得到的。也就是将原来顶点在Vb的夹角区域平移到顶点为(Va+Vb)/2 的位置。也就是说，</p>
<p>可通过数学证明 <a href="https://blog.csdn.net/zhiai315/article/details/114241684?spm=1001.2101.3001.6650.3&amp;utm_medium=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~default-3.no_search_link&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~default-3.no_search_link">https://blog.csdn.net/zhiai315/article/details/114241684?spm=1001.2101.3001.6650.3&amp;utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-3.no_search_link&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-3.no_search_link</a></p>
<p>这样是架设了每个智能体在避免碰撞的过程中各自平均分担了相同的避让。而实际上，各个智能体之间可能在规避碰撞的过程中分别承担不同比例，事实上是可以通过动态划分的。如果A承担的比例是a1，则B承担的是1-a1. </p>
<p>Va’ = a1*Vr + (1-a1)*Va 由此可知 Vr = (1/a1) * Va’ +(1-1/a1) * Va</p>
<h1 id="ORCA"><a href="#ORCA" class="headerlink" title="ORCA"></a>ORCA</h1><p>组合RVO区域</p>
]]></content>
  </entry>
  <entry>
    <title>强化学习Reinforcement Learning</title>
    <url>/2021/10/26/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0Reinforcement-Learning/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>背景问题：</p>
<p>方法简述：</p>
<p>优点：</p>
<p>缺点：</p>
<span id="more"></span>

<p><img src="/2021/10/26/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0Reinforcement-Learning/zhendeliu/Documents/GitHub/gitblog/source/_posts/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0Reinforcement-Learning/%E5%88%86%E7%B1%BB.png"></p>
<p>强化学习的主流分类</p>
]]></content>
      <categories>
        <category>学习</category>
        <category>cv算法</category>
        <category>深度学习</category>
        <category>强化学习</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>论文复现</tag>
      </tags>
  </entry>
  <entry>
    <title>无人驾驶-视觉方向基础</title>
    <url>/2021/10/07/%E6%97%A0%E4%BA%BA%E9%A9%BE%E9%A9%B6-%E8%A7%86%E8%A7%89%E6%96%B9%E5%90%91%E5%9F%BA%E7%A1%80/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>无人驾驶作为人工智能的集大成应用，从来就不是某单一的技术，而是众多技术点的整合。技术上它需要有算法上的创新、系统上的融合，以及来自云平台的支持。</p>
<p>什么是SLAM？</p>
<p>什么是VIO？</p>
<span id="more"></span>

<h1 id="什么是SLAM"><a href="#什么是SLAM" class="headerlink" title="什么是SLAM"></a>什么是SLAM</h1><p>SLAM是 Simultaneous Localization And Mapping的 英文首字母组合，一般翻译为：同时定位与建图、同时定位与地图构建。</p>
<p><strong>SLAM是指当某种移动设备（如机器人、无人机、手机等）从一个未知环境里的未知地点出发，在运动过程中通过传感器（如激光雷达、摄像头等）观测定位自身位置、姿态、运动轨迹，再根据自身位置进行增量式的地图构建，从而达到同时定位和地图构建的目的</strong>。定位和建图是两个相辅相成的过程，地图可以提供更好的定位，而定位也可以进一步扩建地图。</p>
<p>比如：扫地机器人，如果在室内随机游走，遇到障碍物就转弯，这样会导致有很多地方会漏掉，扫地效率非常低。一台智能的扫地机器人，可以通过自身的传感器对室内进行扫描建图，根据当前的定位进行Z字形规划清扫，还能实现自动回充、断点续扫等高级功能。</p>
<p>扫地机器人至少需要知道以下几件事情：</p>
<p>1、我在哪里？也就是扫地机器人在工作过程中要知道自己在房间的具体位置。对应的术语叫：**定位（Localization)**。</p>
<p>2、我周围的环境是什么样子？也就是扫地机器人需要知道整个房间的地面结构信息。对应的术语叫：**建图(Mapping)**。</p>
<p>3、我怎样到达指定地点（充电器）？当扫地机器人电量不足时，如何以最短的路径到达充电器所在位置进行自动充电。对应的术语叫：路径规划（Route Planning）。</p>
<h1 id="什么是VIO"><a href="#什么是VIO" class="headerlink" title="什么是VIO"></a>什么是VIO</h1><p>参考：<a href="https://zhuanlan.zhihu.com/p/44787066">https://zhuanlan.zhihu.com/p/44787066</a></p>
]]></content>
  </entry>
  <entry>
    <title>机器学习-K-means算法</title>
    <url>/2021/10/01/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-K-means%E7%AE%97%E6%B3%95/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>背景问题：</p>
<p>方法简述：</p>
<p>优点：</p>
<p>缺点：</p>
<span id="more"></span>

]]></content>
      <categories>
        <category>学习</category>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习-KNN邻近算法</title>
    <url>/2021/10/01/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-KNN%E9%82%BB%E8%BF%91%E7%AE%97%E6%B3%95/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>背景问题：</p>
<p>方法简述：</p>
<p>优点：</p>
<p>缺点：</p>
<span id="more"></span>

]]></content>
      <categories>
        <category>学习</category>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习-sklearn包的学习</title>
    <url>/2021/10/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-sklearn%E5%8C%85%E7%9A%84%E5%AD%A6%E4%B9%A0/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>机器学习-主成分分析PCA</title>
    <url>/2021/10/01/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90PCA/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>背景问题：</p>
<p>方法简述：</p>
<p>优点：</p>
<p>缺点：</p>
<span id="more"></span>

]]></content>
      <categories>
        <category>学习</category>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title>【未完成】机器学习-优化算法：牛顿法和拟牛顿法</title>
    <url>/2021/10/02/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95%EF%BC%9A%E7%89%9B%E9%A1%BF%E6%B3%95%E5%92%8C%E6%8B%9F%E7%89%9B%E9%A1%BF%E6%B3%95/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>牛顿法（Newton method）和拟牛顿法（quasi Newton method）是求解无约束最优化问题的常用方法，有收敛速度快的优点。牛顿法是迭代算法，每一步都需求解目标函数的海塞矩阵（Hessian Matrix），计算比较复杂。拟牛顿法通过正定矩阵近似海塞矩阵的逆矩阵或海塞矩阵，简化了这一计算过程。</p>
<p>方法：</p>
<p>牛顿法：是使用函数F(x)的泰勒级数的前面几项来寻找方程F(x)=0的根。</p>
<p>拟牛顿法：牛顿法虽然收敛速度快，但是需要计算海塞矩阵的逆矩阵 ，而且有时目标函数的海塞矩阵无法保持正定，从而使得牛顿法失效。为了克服这两个问题，人们提出了拟牛顿法。这个方法的基本思想是：不用二阶偏导数而构造出可以近似海塞矩阵（或海塞矩阵的逆)的正定对称阵。不同的构造方法就产生了不同的拟牛顿法。</p>
<p>优点：</p>
<p>缺点：</p>
]]></content>
      <categories>
        <category>学习</category>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>优化</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习-决策树Decision Tree</title>
    <url>/2021/09/30/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%86%B3%E7%AD%96%E6%A0%91Decision-Tree/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>背景问题：根据属性值做if … else …划分。问题在于先行选择哪个属性做划分是最优的？用什么标准来定量的去做这个选择？</p>
<p>方法简述：</p>
<p>ID3 利用信息增益来选择特征的。信息增益最大的特征来建立决策树的当前节点</p>
<p>C4.5 是根据“信息增益比”指标来做特征选择</p>
<p>CART(Classification and Regression Tree) 使用基尼系数</p>
<p>优点：</p>
<p>缺点：</p>
<p>决策树的回归用法</p>
<span id="more"></span>

<h1 id="ID3-决策算法"><a href="#ID3-决策算法" class="headerlink" title="ID3 决策算法"></a>ID3 决策算法</h1><h1 id="C4-5-决策算法"><a href="#C4-5-决策算法" class="headerlink" title="C4.5 决策算法"></a>C4.5 决策算法</h1><p>该算法解决了ID3算法中的一下问题：</p>
<ol>
<li><p>不能处理连续特征，</p>
<ol start="2">
<li>用信息增益作为标准容易偏向于取值较多的特征</li>
<li>缺失值处理的问</li>
<li>过拟合问题。</li>
</ol>
</li>
</ol>
<h1 id="CART算法"><a href="#CART算法" class="headerlink" title="CART算法"></a>CART算法</h1><p>该算法解决的是C4.5算法中的一下问题：</p>
<ol>
<li><p>由于决策树算法非常容易过拟合，因此对于生成的决策树必须要进行剪枝。</p>
</li>
<li><p>C4.5生成的是多叉树，即一个父节点可以有多个节点。很多时候，在计算机中二叉树模型会比多叉树运算效率高。如果采用二叉树，可以提高效率。</p>
</li>
<li><p>C4.5只能用于分类，如果能将决策树用于回归的话可以扩大它的使用范围。</p>
</li>
<li><p>C4.5由于使用了熵模型，里面有大量的耗时的对数运算,如果是连续值还有大量的排序运算。</p>
</li>
</ol>
]]></content>
      <categories>
        <category>学习</category>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习-回归树Regression-Tree</title>
    <url>/2021/10/02/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%9B%9E%E5%BD%92%E6%A0%91Regression-Tree/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>背景问题：决策树是解决分类问题的主要方法，分类是离散问题，但回归是连续问题，</p>
<p>方法简述： CART（Classification and Regression Tree）可以用来做回归，在分类问题中CART只用基尼系数作为特征选择和划分的依据；在回归问题中CART使用MSE(Mean Square Error)或者MAE(Mean Absolute Error)作为特征选择和划分的依据。每个叶子代表一个预测值，取值是连续的。</p>
<p>优点：</p>
<p>训练速度和预测速度较快；<br>善于获取数据集中的非线性关系；<br>了解数据集中的特征交互；<br>善于处理数据集中出现的异常值；<br>善于在数据集中找到最重要的特征；<br>不需要特征缩放；<br>结果可解释，并易于说明；</p>
<p>缺点：</p>
<p>预测精确度较低；<br>需要一些参数的调整；<br>不适用于小型数据集；<br>分离信号和噪声的效果不理想；<br>当新增数据时，不易更新模型；<br>在实践中很少使用，而是更多地使用集合树；<br>可能会出现过度拟合</p>
<span id="more"></span>

<p>参考：<a href="https://zhuanlan.zhihu.com/p/82054400">https://zhuanlan.zhihu.com/p/82054400</a></p>
]]></content>
      <categories>
        <category>学习</category>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习-支持向量机(SVM)</title>
    <url>/2021/09/30/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA-SVM/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>背景问题：逻辑回归是找到可以划分数据的超平面，但是如何找到最优的超平面来划分数据集呢？</p>
<p>方法简述：</p>
<p>线性支持向量机：</p>
<p>非线性支持向量机：</p>
<p>核函数：</p>
<p>软间隔支持向量机：</p>
<p>优点：</p>
<p>​    小规模数据集训练，比LR和随机森林准确率高，泛化能力强。</p>
<p>​    在非线性特征空间中效果较好，有大量的核函数可以使用来解决非线性分类问题</p>
<p>​    在高维度特征的分类问题和回归问题很有效，即便是特征维度大于样本量的时候</p>
<p>​    不需要依赖全部样本，仅仅使用一部分样本做支持向量来完成超平面决策</p>
<p>​    无局部极小值问题；（相对于神经网络等算法）</p>
<p>缺点：</p>
<p>​    SVM不能产生分类的概率值，</p>
<p>​    SVM对大规模训练数据集是不适用的，计算量十分复杂</p>
<p>​    解决非线性问题时，找到一个合适的核函数是比较困难的</p>
<p>​    多分类问题支持不友好</p>
<p>​    对缺失数据敏感</p>
<p>​    ？？？对于核函数的高维映射解释力不强，尤其是径向基函数；</p>
<p>应用：</p>
<p>文本分类领域效果最好的机器学习算法，在工业界主要应用在网页分类、微博情感分析、舆情监控、用户评论挖掘、文本过滤等诸多领域</p>
<span id="more"></span>
]]></content>
      <categories>
        <category>学习</category>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习-奇异值分解SVD</title>
    <url>/2021/10/01/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%A5%87%E5%BC%82%E5%80%BC%E5%88%86%E8%A7%A3SVD/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>背景问题：</p>
<p>方法简述：</p>
<p>优点：</p>
<p>缺点：</p>
<span id="more"></span>

]]></content>
      <categories>
        <category>学习</category>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习-朴素贝叶斯</title>
    <url>/2021/10/01/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>背景问题：</p>
<p>方法简述：</p>
<p>优点：</p>
<p>缺点：</p>
<span id="more"></span>

]]></content>
      <categories>
        <category>学习</category>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习-线性回归Linear Regression</title>
    <url>/2021/09/30/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92Linear-Regression/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>背景问题：找到一系列的参数W，使得f(x) = XW 和真实输出Y之间无限接近或一致。</p>
<p>方法简述：</p>
<p>优点：直接，快速，可解释性高</p>
<p>缺点：基于一系列假设；对异常值敏感；对数据分布敏感；存在多重共线性，自相关，异方差问题；容易出现过拟合与欠拟合问题；</p>
<span id="more"></span>

<h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><p>什么是回归模型？</p>
<p>回归是用来拟合输入变量和输出变量之间的关系，回归模型就是表示从输入变量到输出变量的映射函数。</p>
<p>所以线性回归的目标就是找到一系列的参数W，使得f(x) = XW 和真实输出Y之间无限接近或一致。</p>
<h1 id="理论"><a href="#理论" class="headerlink" title="理论"></a>理论</h1><h1 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h1><h1 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h1><h2 id="过拟合问题"><a href="#过拟合问题" class="headerlink" title="过拟合问题"></a>过拟合问题</h2><p>​    分析数据，重新做数据清冼，将征工程。</p>
<p>​    扩充数据集，收集更多数据。</p>
<p>​    减少特征数量 。</p>
<p>​    **采用正则化方法</p>
<h2 id="欠拟合问题"><a href="#欠拟合问题" class="headerlink" title="欠拟合问题 **"></a>欠拟合问题 **</h2><p>​    分析数据，增加特征维度</p>
<p>​    ** 增加多项式特征阶数</p>
<p>​    ** 减小正则项的超参数系数</p>
<p>​    ** 局部加权回归 </p>
<h2 id="多重共线性"><a href="#多重共线性" class="headerlink" title="多重共线性"></a>多重共线性</h2><p>​    那共线性会引发什么问题。。。。：</p>
<p>1、模型参数估计不准确，有时甚至会出现回归系数的符号与实际情况完全相反的情况，比如逻辑上应该系数为正的特征系数 算出来为负。</p>
<p>2、本应该显著的自变量不显著，本不显著的自变量却呈现出显著性（也就是说，无法从p-值的大小判断出变量是否显著——下面会给一个例子）</p>
<p>3、多重共线性使参数估计值的方差增大，模型参数不稳定，也就是每次训练得到的权重系数差异都比较大。</p>
<p>其实多重共线性这样理解会简单很多:</p>
<p>假设原始的线性回归公式为：</p>
<p>y=w1<em>x1+w2</em>x2+w3*x3</p>
<p>训练完毕的线性回归公式为：</p>
<p>y=5x1+7x2+10x3,</p>
<p>此时加入一个新特征x4，假设x4和x3高度相关，x4=2x3,则</p>
<p>y=w1<em>x1+w2</em>x2+w3<em>x3+w4</em>x4=w1<em>x1+w2</em>x2+(w3+2w4)*x3</p>
<p>因为我们之前拟合出来的最优的回归方程为：</p>
<p>y=5x1+7x2+10x3</p>
<p>显然w3+2w4可以合并成一个新的权重稀疏 w5，则</p>
<p>y=w1<em>x1+w2</em>x2+w5*x3,显然：</p>
<p>y=w1<em>x1+w2</em>x2+w3<em>x3和y=w1</em>x1+w2<em>x2+w5</em>x3是等价的。。。。</p>
<p>那么最终最优的模型应该也是 y=5x1+7x2+10x3</p>
<p>但是考虑到引入了x4，所以w4和w3的权重是分开计算出来的，这就导致了</p>
<p>w5=10=w3+2w4，显然这个方程有无穷多的解，比如w3=4，w4=3，或者w4=-1，w3=12等，因此导致了模型系数估计的不稳定并且可能会出现负系数的问题。</p>
<h1 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h1><h2 id="基于-Python-scikit-learn-工具包"><a href="#基于-Python-scikit-learn-工具包" class="headerlink" title="基于 Python scikit-learn 工具包"></a>基于 Python scikit-learn 工具包</h2><h2 id="Python自建实现"><a href="#Python自建实现" class="headerlink" title="Python自建实现"></a>Python自建实现</h2>]]></content>
      <categories>
        <category>学习</category>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习-逻辑回归Logistic Regression</title>
    <url>/2021/09/30/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92Logistic-Regression/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>逻辑回归 = 线性回归+逻辑分布 （如：sigmoid函数）</p>
<p>背景问题：</p>
<p>回归是连续的，分类是离散的，怎么将解决分类问题转化为解决回归问题</p>
<p>方法简述：先拟合决策边界(不局限于线性，还可以是多项式)，再建立这个边界与分类的概率联系，从而得到了二分类情况下的概率。分类是离散的，但类别的概率是连续的，让模型拟合概率相关的一个指标（对数几率函数logit）</p>
<p>优点：</p>
<p>​    (1)对率函数任意阶可导，具有很好的数学性质，许多现有的数值优化算法都可以用来求最优解，训练速度快;</p>
<p>​    (2)简单易理解，模型的可解释性非常好，从特征的权重可以看到不同的特征对最后结果的影响;</p>
<p>​    (3)适合二分类问题，不需要缩放输入特征</p>
<p>​    (4)内存资源占用小，因为只需要存储各个维度的特征值;</p>
<p>​    (5)直接对分类可能性进行建模，无需事先假设数据分布，避免了假设分布不准确所带来的问题</p>
<p>​    (6)以概率的形式输出，对许多利用概率辅助决策的任务很有用</p>
<p>缺点：</p>
<p>​    (1)不能用于解决非线性问题</p>
<p>​    (2)对多重共线性数据较为敏感;</p>
<p>​    (3)很难处理数据不平衡的问题;</p>
<p>​    (4)准确率并不是很高，因为形式非常的简单(非常类似线性模型)，很难去拟合数据的真实分布;</p>
<p>​    (5)无法筛选特征</p>
<span id="more"></span>

<p>线性分类器：线性分类器的学习目标便是要在n维的数据空间中找到一个超平面，使得这个超平面可以将已知的数据点分为两个类别</p>
<p>逻辑回归不是解决回归问题是用来解决分类问题，本质就是假设数据符合这个分布，然后使用极大似然估计做参数的估计</p>
<h1 id="Logistic-分布函数"><a href="#Logistic-分布函数" class="headerlink" title="Logistic 分布函数"></a>Logistic 分布函数</h1><p>Logistic 分布是一种连续型的分布，它形状与正态分布的形状相似，但是 Logistic 分布的尾部更长，所以我们可以使用 Logistic 分布来建模比正态分布具有更长尾部和更高波峰的数据分布。在深度学习中常用到的 Sigmoid 函数就是 Logistic 的分布函数在 <img src="https://www.zhihu.com/equation?tex=%5Cmu=0,+%5Cgamma=1" alt="[公式]"> 的特殊形式。</p>
<p>logistic regression是使用线性回归的预测值逼近真实分类的对数几率，优点是：</p>
<ol>
<li>直接对分类概率建模，无需进行假设，避免假设带来的不准确</li>
<li>不仅可预测出类别，还能得到类别的概率，</li>
<li>对数几率函数是任意阶可导的函数，有许多数值优化算法都是可以求出最优解的</li>
</ol>
<p>损失函数：</p>
<p>优化的主要目标是找到一个方向，参数朝这个方向移动之后使得损失函数的值能够减小，</p>
<p>优点：</p>
<p>(1)对率函数任意阶可导，具有很好的数学性质，许多现有的数值优化算法都可以用来求最优解，训练速度快;</p>
<p>(2)简单易理解，模型的可解释性非常好，从<a href="https://www.cda.cn/map/tezheng/">特征</a>的权重可以看到不同的<a href="https://www.cda.cn/map/tezheng/">特征</a>对最后结果的影响;</p>
<p>(3)适合二分类问题，不需要缩放输入<a href="https://www.cda.cn/map/tezheng/">特征</a>;</p>
<p>(4)内存资源占用小，因为只需要存储各个维度的<a href="https://www.cda.cn/map/tezheng/">特征</a>值;</p>
<p>(5)直接对分类可能性进行建模，无需事先假设数据分布，避免了假设分布不准确所带来的问题</p>
<p>(6)以概率的形式输出，而非知识0.1判定，对许多利用概率辅助决策的任务很有用</p>
<p>缺点：</p>
<p>(1)不能用<a href="https://www.cda.cn/map/luojihuigui/">逻辑回归</a>去解决非线性问题，因为Logistic的决策面试线性的;</p>
<p>(2)对多重共线性数据较为敏感;</p>
<p>(3)很难处理数据不平衡的问题;</p>
<p>(4)准确率并不是很高，因为形式非常的简单(非常类似线性模型)，很难去拟合数据的真实分布;</p>
<p>(5)<a href="https://www.cda.cn/map/luojihuigui/">逻辑回归</a>本身无法筛选<a href="https://www.cda.cn/map/tezheng/">特征</a>，有时会用gbdt来筛选<a href="https://www.cda.cn/map/tezheng/">特征</a>，然后再上<a href="https://www.cda.cn/map/luojihuigui/">逻辑回归</a>。</p>
]]></content>
      <categories>
        <category>学习</category>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习-集成算法-AdaBoost</title>
    <url>/2021/10/01/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E9%9B%86%E6%88%90%E7%AE%97%E6%B3%95-AdaBoost/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>背景问题：</p>
<p>方法简述：</p>
<p>优点：</p>
<p>缺点：</p>
<span id="more"></span>

]]></content>
      <categories>
        <category>学习</category>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习-集成算法-随机森林</title>
    <url>/2021/10/01/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E9%9B%86%E6%88%90%E7%AE%97%E6%B3%95-%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>背景问题：</p>
<p>方法简述：</p>
<p>优点：</p>
<p>缺点：</p>
<span id="more"></span>

]]></content>
      <categories>
        <category>学习</category>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习经典算法概览</title>
    <url>/2021/09/29/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%BB%8F%E5%85%B8%E7%AE%97%E6%B3%95%E6%A6%82%E8%A7%88/</url>
    <content><![CDATA[<p>这是一篇机器学习经典算法的简述，包含了线性回归、逻辑回归、支持向量机(SVM)、最近邻居(KNN)、决策树、k平均、随机森林、朴素贝叶斯、降维、梯度增强（更新ing）</p>
<span id="more"></span>

<h1 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h1><h1 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h1><h1 id="集成算法-Ensemble-algorithms"><a href="#集成算法-Ensemble-algorithms" class="headerlink" title="集成算法 Ensemble algorithms"></a>集成算法 Ensemble algorithms</h1><p>将多个弱模型组合，弱模型单独训练，将哥哥弱模型的预测结果以某种方式结合完成总体的预测</p>
<p>主要问题在于找到可以组合的弱模型和弱模型结果的结合方式</p>
<ol>
<li>Boosting</li>
<li>Bagging</li>
<li>AdaBoost</li>
<li>Blending</li>
<li>Random Forest随机森林</li>
<li>** GBM（Gradient Boosting Machine）梯度推进机</li>
<li>** GBRT（Gradient Boosted Regression Tree） 梯度提升回归树</li>
</ol>
<p>优点：结合最优秀的模型们，可以得到更加优秀的预测结果</p>
<p>缺点：多模型融合计算量大</p>
<h3 id><a href="#" class="headerlink" title></a></h3><h1 id="决策树算法（Decision-Tree-Algorithm"><a href="#决策树算法（Decision-Tree-Algorithm" class="headerlink" title="决策树算法（Decision Tree Algorithm)"></a>决策树算法（Decision Tree Algorithm)</h1><p>Step1 选择分裂节点：当根据某个属性的值不能明确分到样本哪个类别是就将此属性作为节点对其分裂</p>
<p>Step2 选择一个合适的阈值进行分裂使其分类的错误率最小</p>
<h2 id="1-ID3"><a href="#1-ID3" class="headerlink" title="1.ID3"></a>1.ID3</h2>]]></content>
      <categories>
        <category>学习</category>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>概览</tag>
      </tags>
  </entry>
  <entry>
    <title>模型评价方法及指标</title>
    <url>/2021/10/01/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BB%B7%E6%96%B9%E6%B3%95%E5%8F%8A%E6%8C%87%E6%A0%87/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>背景问题：</p>
<p>方法简述：</p>
<p>优点：</p>
<p>缺点：</p>
<span id="more"></span>
]]></content>
      <categories>
        <category>学习</category>
        <category>深度学习</category>
        <category>神经网络</category>
      </categories>
      <tags>
        <tag>总结</tag>
        <tag>编程</tag>
        <tag>优化</tag>
        <tag>调参</tag>
      </tags>
  </entry>
  <entry>
    <title>模型过拟合问题解决方法</title>
    <url>/2021/10/01/%E6%A8%A1%E5%9E%8B%E8%BF%87%E6%8B%9F%E5%90%88%E9%97%AE%E9%A2%98%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>背景问题：</p>
<p>方法简述：</p>
<p>优点：</p>
<p>缺点：</p>
<span id="more"></span>

<h1 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h1><p>dropout是指在深度学习网络的训练过程中，对于神经网络单元，按照一定的概率将其暂时从网络中丢弃。注意是 <strong>「暂时」</strong>，对于随机梯度下降来说，由于是随机丢弃，故而每一个mini-batch都在训练不同的网络。</p>
<p>Dropout类似于bagging ensemble减少variance。也就是投通过投票来减少可变性。通常我们在全连接层部分使用dropout，在卷积层则不使用。但 <strong>「dropout」</strong> 并不适合所有的情况，不要无脑上<code>Dropout</code>。</p>
<p><code>Dropout</code>一般适合于全连接层部分，而卷积层由于其参数并不是很多，所以不需要dropout，加上的话对模型的泛化能力并没有太大的影响。</p>
<p>我们一般在网络的最开始和结束的时候使用全连接层，而hidden layers则是网络中的卷积层。所以一般情况，在全连接层部分，采用较大概率的dropout而在卷积层采用低概率或者不采用dropout。</p>
<h2 id="数据集处理"><a href="#数据集处理" class="headerlink" title="数据集处理"></a>数据集处理</h2><p>主要有 <strong>「数据筛选」</strong> 以及 <strong>「数据增强」</strong></p>
<h2 id="难例挖掘-hard-negative-mining"><a href="#难例挖掘-hard-negative-mining" class="headerlink" title="难例挖掘 hard-negative-mining"></a>难例挖掘 hard-negative-mining</h2><p>分析模型难以预测正确的样本，给出针对性方法。</p>
<h2 id="多模型融合"><a href="#多模型融合" class="headerlink" title="多模型融合"></a>多模型融合</h2><p>Ensemble是论文刷结果的终极核武器,深度学习中一般有以下几种方式</p>
<ul>
<li>同样的参数,不同的初始化方式</li>
<li>不同的参数,通过cross-validation,选取最好的几组</li>
<li>同样的参数,模型训练的不同阶段，即不同迭代次数的模型。</li>
<li>不同的模型,进行线性融合. 例如RNN和传统模型.</li>
</ul>
<p>提高模型性能和鲁棒性大法：probs融合 和 投票法。</p>
<p>假设这里有model 1, model 2, model 3，可以这样融合：</p>
<blockquote>
<p>\1. model1 probs + model2 probs + model3 probs ==&gt; final label</p>
</blockquote>
<blockquote>
<p>\2. model1 label , model2 label , model3 label ==&gt; voting ==&gt; final label</p>
</blockquote>
<blockquote>
<p>\3. model1_1 probs + … + model1_n probs ==&gt; mode1 label, model2 label与model3获取的label方式与1相同 ==&gt; voting ==&gt; final label多模型融合</p>
<p>Ensemble是论文刷结果的终极核武器,深度学习中一般有以下几种方式</p>
<ul>
<li>同样的参数,不同的初始化方式</li>
<li>不同的参数,通过cross-validation,选取最好的几组</li>
<li>同样的参数,模型训练的不同阶段，即不同迭代次数的模型。</li>
<li>不同的模型,进行线性融合. 例如RNN和传统模型.</li>
</ul>
<p>提高模型性能和鲁棒性大法：probs融合 和 投票法。</p>
<p>假设这里有model 1, model 2, model 3，可以这样融合：</p>
<blockquote>
<p>\1. model1 probs + model2 probs + model3 probs ==&gt; final label</p>
</blockquote>
<blockquote>
<p>\2. model1 label , model2 label , model3 label ==&gt; voting ==&gt; final label</p>
</blockquote>
<blockquote>
<p>\3. model1_1 probs + … + model1_n probs ==&gt; mode1 label, model2 label与model3获取的label方式与1相同 ==&gt; voting ==&gt; final label 这个方式的启发来源于，如果一个model的随机种子没有固定，多次预测得到的结果可能不同。</p>
</blockquote>
</blockquote>
<p>以上方式的效果要根据label个数，数据集规模等特征具体问题具体分析，表现可能不同，方式无非是probs融合和投票法的单独使用or结合。</p>
<h2 id="多尺度训练"><a href="#多尺度训练" class="headerlink" title="多尺度训练"></a>多尺度训练</h2><p>多尺度训练是一种 <strong>「直接有效」</strong> 的方法，通过输入不同尺度的图像数据集，因为神经网络卷积池化的特殊性，这样可以让神经网络充分地学习不同分辨率下图像的特征，可以提高机器学习的性能。</p>
<p>也可以用来处理过拟合效应，在图像数据集不是特别充足的情况下，可以先训练小尺寸图像，然后增大尺寸并再次训练相同模型，这样的思想在Yolo-v2的论文中也提到过</p>
]]></content>
      <categories>
        <category>学习</category>
        <category>深度学习</category>
        <category>神经网络</category>
      </categories>
      <tags>
        <tag>总结</tag>
        <tag>编程</tag>
        <tag>优化</tag>
        <tag>调参</tag>
      </tags>
  </entry>
  <entry>
    <title>深度学习-[2013]ZFNet网络模型</title>
    <url>/2021/10/01/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-2013-ZFNet%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>背景问题：在神经网络卷集训练中没有办法知道每一次卷积、每一次池化、每一次经过激活函数到底发生了什么，也不知道神经网络为什么取得了如此好的效果。ZFNet通过使用可视化技术揭示了神经网络各层到底在干什么，起到了什么作用。一旦知道了这些，如何调整我们的神经网络，往什么方向优化，就有了较好的依据。</p>
<p>方法简述：</p>
<p><strong>过程一：</strong>特征提取过程，输入图像-》卷积-》Relu激活-》最大化池化-》feature map</p>
<p><strong>过程二：</strong>特征还原过程，feature map-》反池化-》反Relu激活-》反卷积-》可视化（原始）图像</p>
<p><strong>结论一：</strong>CNN网络前面的层学习的是物理轮廓、边缘、颜色、纹理等特征，后面的层学习的是和类别相关的抽象特征</p>
<p><strong>结论二：</strong>CNN学习到的特征具有平移和缩放不变性，但是，没有旋转不变性</p>
<p><strong>结论三：</strong>CNN网络的特征提取具有通用性，这是后面微调的理论支持</p>
<p>优点：</p>
<ol>
<li>使用反卷积，可视化feature map 为模型优化提供依据</li>
<li>与AlexNet相比，前面层使用更小的卷积核和小的步长，保留更多特征</li>
<li>通过遮挡找到决定图像类别的关键部位，说明了深度增加时，网络课学习到更具区分的特征</li>
<li>训练时，底层参数收敛快，越高层的参数需要越长的训练时间</li>
</ol>
<p>缺点：</p>
<span id="more"></span>

<p><img src="/2021/10/01/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-2013-ZFNet%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/ZFNet.png"></p>
]]></content>
      <categories>
        <category>学习</category>
        <category>cv算法</category>
        <category>深度学习</category>
        <category>目标检测</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>论文复现</tag>
      </tags>
  </entry>
  <entry>
    <title>深度学习-Bottleneck结构的理解</title>
    <url>/2021/10/01/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Bottleneck%E7%BB%93%E6%9E%84%E7%9A%84%E7%90%86%E8%A7%A3/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>背景问题：</p>
<p>方法简述：</p>
<p>优点：</p>
<p>缺点：</p>
<span id="more"></span>
]]></content>
      <categories>
        <category>学习</category>
        <category>cv算法</category>
        <category>深度学习</category>
        <category>目标检测</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>论文复现</tag>
      </tags>
  </entry>
  <entry>
    <title>深度学习-Inception结构的理解</title>
    <url>/2021/10/01/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Inception%E7%BB%93%E6%9E%84%E7%9A%84%E7%90%86%E8%A7%A3/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>背景问题：</p>
<p>方法简述：</p>
<p>优点：</p>
<p>缺点：</p>
<span id="more"></span>

<h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><p>GoogleNet只使用500万个参数，是AlexNet的1/12，它使用了6000万个参数。VGGNet使用了比AlexNet3倍多的参数。</p>
<p>Inception的计算成本也低于VGGNet，这使它能够应用于大数据场景，或是在有限的内存和计算能力的情况下以相对合理的成本处理较大的数据，如移动端。当然我们可以通过计算技巧来优化一些特定的操作来解决该问题。但是这些方法加大了复杂性。另外，这些优化的方法也可以应用了Inception结构，扩大了效率差。</p>
<h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><h2 id="设计原则"><a href="#设计原则" class="headerlink" title="设计原则"></a>设计原则</h2><p> 1、避免表征瓶颈，特别是在网络早期。前向传播网络可以从输入层到<a href="https://baike.sogou.com/lemma/ShowInnerLink.htm?lemmaId=54926098&ss_c=ssc.citiao.link">分类器</a>或回归器的无环图来体现。这定义了清晰的信息流。从每一个分割输入和输出的切入，能够获得通过这个切入的信息流量。应该避免使用极端压缩导致的瓶颈。一般讲表征规模应平缓的从输入向输出递减知道最终任务。理论上，信息内容无法仅通过表征的维度来评估，因为它舍弃了一些重要因素相关性结构；维度仅提供了信息内容的粗略估计。</p>
<p>2、更高维度的表征更容易在一个网络内本地化处理。在卷积网络中加大每层的激活能获得更多的非纠缠特征，可使网络训练更快速。</p>
<p>3、可以在更低维度嵌入上进行空间聚合，不会损失或损失太多的体现能力。例如在进行3*3卷积之前，可以在空间聚合之前降低输入表征，不会有严重问题。我们假设它是因为在空间聚合情况下使用输出，相邻单元结果的强相关性在降低维度时损失较小。基于此这些信号可便利的被压缩，并且维度降低能使学习更快。</p>
<p>4、平衡网络的宽度和深度。网络的优化表现可以通过平衡每阶段的过滤器数量和网络深度实现。同时提高宽度和深度可以提高网络质量，但是只有并行提高时才能对计算常量优化提升，因此要在网路的深度和宽度合理平衡分配计算能力。</p>
<h2 id="Inception-V1模型"><a href="#Inception-V1模型" class="headerlink" title="Inception V1模型"></a>Inception V1模型</h2><p>将1x1，3x3，5x5的conv和3x3的pooling，堆叠在一起，一方面增加了网络的width，另一方面增加了网络对尺度的适应性。所有的卷积核都在上一层的所有输出上来做，5×5的卷积核所需的计算量就太大，造成了特征图厚度很大。为了避免这一现象提出的inception具有如下结构，在3x3前，5x5前，max pooling后分别加上了1x1的卷积核起到了降低特征图厚度的作用，</p>
<p><img src="/2021/10/01/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Inception%E7%BB%93%E6%9E%84%E7%9A%84%E7%90%86%E8%A7%A3/zhendeliu/Documents/GitHub/gitblog/source/_posts/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Inception%E7%BB%93%E6%9E%84%E7%9A%84%E7%90%86%E8%A7%A3/V1.jpg"></p>
<h2 id="Inception-v2模型"><a href="#Inception-v2模型" class="headerlink" title="Inception v2模型"></a>Inception v2模型</h2><p><code>Inception v2 和 Inception v3</code> 来自同一篇论文《Rethinking the Inception Architecture for Computer Vision》，作者提出了一系列能增加准确度和减少计算复杂度的修正方法。</p>
<p>一方面了加入了BN层，减少了Internal Covariate Shift（内部neuron的数据分布发生变化），使每一层的输出都规范化到一个N(0, 1)的<a href="https://baike.sogou.com/lemma/ShowInnerLink.htm?lemmaId=64912573&ss_c=ssc.citiao.link">高斯</a>；</p>
<p>另外一方面学习VGG用2个3x3的conv替代inception模块中的5x5，既降低了参数数量，也加速计算；</p>
<p><img src="/2021/10/01/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Inception%E7%BB%93%E6%9E%84%E7%9A%84%E7%90%86%E8%A7%A3/zhendeliu/Documents/GitHub/gitblog/source/_posts/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Inception%E7%BB%93%E6%9E%84%E7%9A%84%E7%90%86%E8%A7%A3/v2-v3.jpg"></p>
<p>使用3×3的已经很小，那么更小的虽然能使得参数进一步降低，但是不如另一种方式更加有效，就是Asymmetric方式，即使用1×3和3×1两种来代替3×3的卷积核。这种结构在前几层效果不太好，但对特征图大小为12~20的<a href="https://baike.sogou.com/lemma/ShowInnerLink.htm?lemmaId=340385&ss_c=ssc.citiao.link">中间层</a>效果明显。</p>
<p><img src="/2021/10/01/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Inception%E7%BB%93%E6%9E%84%E7%9A%84%E7%90%86%E8%A7%A3/zhendeliu/Documents/GitHub/gitblog/source/_posts/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Inception%E7%BB%93%E6%9E%84%E7%9A%84%E7%90%86%E8%A7%A3/V2.jpg"></p>
<h2 id="Inception-v3模型"><a href="#Inception-v3模型" class="headerlink" title="Inception v3模型"></a>Inception v3模型</h2><p>V3一个最重要的改进是分解（<a href="https://baike.sogou.com/lemma/ShowInnerLink.htm?lemmaId=3448023&ss_c=ssc.citiao.link">Factorization</a>），将7x7分解成两个一维的卷积（1x7,7x1），3x3也是一样（1x3,3x1），这样的好处，既可以加速计算（多余的计算能力可以用来加深网络），又可以将1个conv拆成2个conv，使得网络深度进一步增加，增加了网络的非线性，还有值得注意的地方是网络输入从224x224变为了299x299，更加精细设计了35x35/17x17/8x8的模块。</p>
<h2 id="Inception-v4模型"><a href="#Inception-v4模型" class="headerlink" title="Inception v4模型"></a>Inception v4模型</h2><p>Inception v4 和 Inception -ResNet 在同一篇论文《Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning》中提出来。v4研究了Inception模块结合Residual Connection能不能有改进。发现ResNet的结构可以极大地加速训练，同时性能也有提升，得到一个Inception-ResNet v2网络，同时还设计了一个更深更优化的Inception v4模型，能达到与Inception-ResNet v2相媲美的性能</p>
<h1 id="问题总结"><a href="#问题总结" class="headerlink" title="问题总结"></a>问题总结</h1><h3 id="为什么更深的网络有更好的效果？"><a href="#为什么更深的网络有更好的效果？" class="headerlink" title="为什么更深的网络有更好的效果？"></a>为什么更深的网络有更好的效果？</h3><p>神经网络（主要指BP神经网络及其衍生的各种类型）中的层（主要是隐藏层）越多，对输入特征抽象层次越高。因为在神经网络中，后一层神经元的输入是前一层输出的加权和，前一层的特征在后一层就被抽象出来了，学习的过程其实就是调节和优化各连接权重和阈值的过程。</p>
<p>随着深度的增加，逼近函数的效果好，已经有paper证明过了，个人理解就是，神经网络训练的过程就是调整参数的过程，可以调整的参数（weights and bias）越多，意味着调整的自由度越大，从而逼近效果越好，可以举例子:逼近某一个函数，比较单层，2层，和多层的神经网络逼近效果。</p>
<p>然而，预测效果却不一定好。针对同一个问题，层数少的时候效果差，这时候逐渐增加层数可以提高效果，但是如果盲目不停地增加层数，则会容易引起overfitting，从而导致预测效果不好，所以并不是层数越多，预测效果就一定会越好的。此外，添加更多层会导致更高的训练误差。最后想提一下，其实增加神经元数也可以提高逼近效果。</p>
<p>因此，神经网络随着深度的增加，实际的效果是先变好，然后再变差。</p>
<h3 id="网络深度越深权值越多？"><a href="#网络深度越深权值越多？" class="headerlink" title="网络深度越深权值越多？"></a>网络深度越深权值越多？</h3><p>是的，只要增加卷积，权值数量就会增加</p>
]]></content>
      <categories>
        <category>学习</category>
        <category>cv算法</category>
        <category>深度学习</category>
        <category>目标检测</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>论文复现</tag>
      </tags>
  </entry>
  <entry>
    <title>深度学习-RNN的改进：LSTM和GRU</title>
    <url>/2021/10/01/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-LSTM%E9%95%BF%E7%9F%AD%E6%9C%9F%E8%AE%B0%E5%BF%86/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>背景问题：RNN算法中越晚的输入对结果影响越大，但越晚的输入不一定是最重要的。如何治保留最重要的信息？</p>
<p>方法简述：</p>
<p>优点：</p>
<ol>
<li>长期信息可以有效的保留</li>
<li>挑选重要信息保留，不重要的信息会选择“遗忘”</li>
</ol>
<p>缺点：</p>
<span id="more"></span>



<p><img src="/2021/10/01/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-LSTM%E9%95%BF%E7%9F%AD%E6%9C%9F%E8%AE%B0%E5%BF%86/LSTMandGRU.png"></p>
<p>LSTM隐含层的中间单元 t 的数据流动示意图，它接收上一个单元 t-1 的输入 Ct-1 和 ht-1，当然还有来自本单元的输入 xt ，LSTM一共经过4步完成这些信息的处理，<strong>输出 t 单元的状态 Ct ，和 其输出 ht</strong> 。</p>
<ol>
<li>forget 阶段：这个阶段主要是对上一个节点传进来的输入进行<strong>选择性</strong>忘记。简单来说就是会 “忘记不重要的，记住重要的”。它是由 sigmoid 神经元层和按照点的乘法操作组成的，sigmoid函数的取值范围为0~1，当为0时，也就是不让当前单元的输入xt 的任何信息进入到这个单元的Ct中，如果等于1，意思是全部进入到 Ct 中。wf和bf是sigmoi节点的权重和偏执</li>
</ol>
<p><img src="/2021/10/01/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-LSTM%E9%95%BF%E7%9F%AD%E6%9C%9F%E8%AE%B0%E5%BF%86/forget.png"></p>
<ol start="2">
<li><p>选择记忆阶段。这个阶段将这个阶段的输入有选择性地进行“记忆”。</p>
<p>sigmoid层确定我们将要更新哪些值</p>
<p>tanh层创建新的值向量</p>
<p><img src="/2021/10/01/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-LSTM%E9%95%BF%E7%9F%AD%E6%9C%9F%E8%AE%B0%E5%BF%86/input.png"></p>
</li>
<li><p>更新细胞状态Ct: 圈X是乘法，圈+是加法</p>
<p><img src="/2021/10/01/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-LSTM%E9%95%BF%E7%9F%AD%E6%9C%9F%E8%AE%B0%E5%BF%86/update_ct.png"></p>
</li>
<li><p>输出阶段。这个阶段将决定哪些将会被当成当前状态的输出。Ct只不过隐含地输入给了下一个单元 t+1，因此，最后一步要确定 ht </p>
<p><img src="/2021/10/01/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-LSTM%E9%95%BF%E7%9F%AD%E6%9C%9F%E8%AE%B0%E5%BF%86/output.png"></p>
<h1 id="GRU"><a href="#GRU" class="headerlink" title="GRU"></a>GRU</h1><p>对LSTM的有一种改动版本叫做带门的循环单元（Gated Recurrent Unit），简称为 GRU，在2014年由 Cho 等人提出，它将遗忘门和输入门结合为一个“更新门”，同时，将单元的状态 Ct 和隐藏状态合并为一体，这样的修改的结果是比我们上面介绍的标准的LSTM模型更加简化了，因此变得越来越受欢迎。</p>
</li>
</ol>
<p><img src="/2021/10/01/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-LSTM%E9%95%BF%E7%9F%AD%E6%9C%9F%E8%AE%B0%E5%BF%86/GRU.png"></p>
]]></content>
      <categories>
        <category>学习</category>
        <category>cv算法</category>
        <category>深度学习</category>
        <category>目标检测</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>论文复现</tag>
      </tags>
  </entry>
  <entry>
    <title>深度学习-RNN循环神经网络</title>
    <url>/2021/10/01/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-RNN%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>背景问题：CNN算法的输入和输出基本是一对一的，不同的输入之间是没有联系的，但是需要处理序列数据（一串互相依赖的数据流）的场景就不能很好的完成了。比如文本，音频，股票走势等</p>
<p>方法简述：每一个样特征的训练计算的时候不仅仅考虑当前样本的值，也要考虑前一个样本的输出值</p>
<p>优点：</p>
<p>缺点：</p>
<ol>
<li>RNN 有短期记忆问题，无法处理很长的输入序列</li>
<li>训练 RNN 需要投入极大的成本</li>
</ol>
<span id="more"></span>



<p><img src="/2021/10/01/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-RNN%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/RNN%E5%8D%95%E5%85%83%E7%BB%93%E6%9E%84.jpeg"></p>
<p>x是一个向量，它表示<strong>输入层</strong>的值（这里面没有画出来表示神经元节点的圆圈）；</p>
<p>s是一个向量，它表示<strong>隐藏层</strong>的值（可以想象这一层其实是多个节点，节点数与向量s的维度相同）；</p>
<p>U是输入层到隐藏层的<strong>权重矩阵</strong>，</p>
<p>o也是一个向量，它表示<strong>输出层</strong>的值；</p>
<p>V是隐藏层到输出层的<strong>权重矩阵</strong></p>
<p><strong>循环神经网络</strong>的<strong>隐藏层</strong>的值s不仅仅取决于当前这次的输入x，还取决于上一次<strong>隐藏层</strong>的值s。<strong>权重矩阵</strong> W就是<strong>隐藏层</strong>上一次的值作为这一次的输入的权重。</p>
<p>另外还有一个偏置项b</p>
<p>在计算时，<strong>每一步使用的参数U、W、b都是一样的，也就是说每个步骤的参数都是共享的，这是RNN的重要特点，一定要牢记。</strong></p>
<p>结构示意图</p>
<p><img src="/2021/10/01/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-RNN%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/RNN%E5%8D%95%E5%85%83.gif"></p>
<h1 id="N-vs-N模型"><a href="#N-vs-N模型" class="headerlink" title="N vs N模型"></a>N vs N模型</h1><p>Yi = Softmax(V*hi+c)</p>
<p><img src="/2021/10/01/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-RNN%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E8%BF%9E%E7%BB%AD%E7%BB%93%E6%9E%84.jpeg"></p>
<p>最后可以看出：<strong>这里的RNN的输入和输出序列是等长的</strong></p>
<h1 id="N-vs-1-模型"><a href="#N-vs-1-模型" class="headerlink" title="N vs 1 模型"></a>N vs 1 模型</h1><p><img src="/2021/10/01/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-RNN%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E8%BF%9E%E7%BB%AD%E7%BB%93%E6%9E%84n_1.jpeg"></p>
<p>这种结构通常用来处理序列分类问题。输入是一个序列输出为一个单独的值，如输入一段文字判别它所属的类别，输入一个句子判断其情感倾向，输入一段视频并判断它的类别等等。</p>
<h1 id="1-VS-N模型"><a href="#1-VS-N模型" class="headerlink" title="1 VS N模型"></a>1 VS N模型</h1><p><img src="/2021/10/01/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-RNN%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E8%BF%9E%E7%BB%AD%E7%BB%93%E6%9E%841_n.jpeg"></p>
<p>or</p>
<p><img src="/2021/10/01/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-RNN%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E8%BF%9E%E7%BB%AD%E7%BB%93%E6%9E%841_n2.jpeg"></p>
<p>这种1 VS N的结构可以处理的问题有：</p>
<ul>
<li>从图像生成文字（image caption），此时输入的X就是图像的特征，而输出的y序列就是一段句子</li>
<li>从类别生成语音或音乐等</li>
</ul>
<h1 id="N-vs-M-模型"><a href="#N-vs-M-模型" class="headerlink" title="N vs M 模型"></a>N vs M 模型</h1><p>这种结构又叫Encoder-Decoder模型，也可以称之为Seq2Seq模型。</p>
<p>原始的N vs N RNN要求序列等长，然而我们遇到的大部分问题序列都是不等长的，如机器翻译中，源语言和目标语言的句子往往并没有相同的长度。<strong>为此，Encoder-Decoder结构先将输入数据编码成一个上下文向量c：****拿到c之后，就用另一个RNN网络对其进行解码</strong>，这部分RNN网络被称为Decoder。具体做法就是将c当做之前的初始状态h0输入到Decoder中：</p>
<ol>
<li>c = h4</li>
<li>c = q(h4)</li>
<li>c = q(h1,h2,h3,h4)</li>
</ol>
<p><img src="/2021/10/01/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-RNN%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/N_M.jpeg"></p>
<p>Or 另一种做法将C当作每一步输入</p>
<p><img src="/2021/10/01/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-RNN%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/N_M2.jpeg"></p>
<p>由于这种Encoder-Decoder结构不限制输入和输出的序列长度，因此应用的范围非常广泛，比如：</p>
<ul>
<li>机器翻译。Encoder-Decoder的最经典应用，事实上这一结构就是在机器翻译领域最先提出的</li>
<li>文本摘要。输入是一段文本序列，输出是这段文本序列的摘要序列。</li>
<li>阅读理解。将输入的文章和问题分别编码，再对其进行解码得到问题的答案。</li>
<li>语音识别。输入是语音信号序列，输出是文字序列。</li>
</ul>
<h1 id="Attention-机制"><a href="#Attention-机制" class="headerlink" title="Attention 机制"></a>Attention 机制</h1><p>问题：在Encoder-Decoder结构中，Encoder把所有的输入序列都编码成一个统一的语义特征c再解码，<strong>因此， c中必须包含原始序列中的所有信息，它的长度就成了限制模型性能的瓶颈。</strong>如机器翻译问题，当要翻译的句子较长时，一个c可能存不下那么多信息，就会造成翻译精度的下降。</p>
<p>解决方法：Attention机制通过在每个时间输入不同的c来解决这个问题，下图是带有Attention机制的Decoder：</p>
<p><img src="/2021/10/01/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-RNN%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/attention1.jpeg"></p>
<p><strong>每一个c会自动去选取与当前所要输出的y最合适的上下文信息。具体来说，我们用 a_ij衡量Encoder中第j阶段的hj和解码时第i阶段的相关性，最终Decoder中第i阶段的输入的上下文信息 c_i 就来自于所有 h_i对 a_ij的加权和。</strong></p>
<p><img src="/2021/10/01/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-RNN%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/attention2.jpeg"></p>
<p>Encoder中的h1、h2、h3、h4就可以分别看做是“我”、“爱”、“中”、“国”所代表的信息。在翻译成英语时，第一个上下文c1应该和“我”这个字最相关，因此对应的 <img src="https://www.zhihu.com/equation?tex=a_%7B11%7D" alt="[公式]"> 就比较大，而相应的 <img src="https://www.zhihu.com/equation?tex=a_%7B12%7D" alt="[公式]"> 、 <img src="https://www.zhihu.com/equation?tex=a_%7B13%7D" alt="[公式]"> 、 <img src="https://www.zhihu.com/equation?tex=a_%7B14%7D" alt="[公式]"> 就比较小。c2应该和“爱”最相关，因此对应的 <img src="https://www.zhihu.com/equation?tex=a_%7B22%7D" alt="[公式]"> 就比较大。最后的c3和h3、h4最相关，因此 <img src="https://www.zhihu.com/equation?tex=a_%7B33%7D" alt="[公式]"> 、 <img src="https://www.zhihu.com/equation?tex=a_%7B34%7D" alt="[公式]"> 的值就比较大。</p>
<p>至此，关于Attention模型，我们就只剩最后一个问题了，那就是：<strong>这些权重 <img src="https://www.zhihu.com/equation?tex=a_%7Bij%7D" alt="[公式]"> 是怎么来的？</strong></p>
<p>事实上， <img src="https://www.zhihu.com/equation?tex=a_%7Bij%7D" alt="[公式]"> 同样是从模型中学出的，它实际和Decoder的第i-1阶段的隐状态、Encoder第j个阶段的隐状态有关。</p>
<p>同样还是拿上面的机器翻译举例， <img src="https://www.zhihu.com/equation?tex=a_%7B1j%7D" alt="[公式]"> 的计算（此时箭头就表示对h’和 <img src="https://www.zhihu.com/equation?tex=h_j" alt="[公式]"> 同时做变换）：</p>
<p><img src="/2021/10/01/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-RNN%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/attention3.jpeg"></p>
<p>Reference: <a href="https://zhuanlan.zhihu.com/p/28054589">https://zhuanlan.zhihu.com/p/28054589</a></p>
]]></content>
      <categories>
        <category>学习</category>
        <category>cv算法</category>
        <category>深度学习</category>
        <category>目标检测</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>论文复现</tag>
      </tags>
  </entry>
  <entry>
    <title>深度学习-深度可分离卷积Depthwise Separable Conv</title>
    <url>/2021/10/01/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E6%B7%B1%E5%BA%A6%E5%8F%AF%E5%88%86%E7%A6%BB%E5%8D%B7%E7%A7%AFDepthwise-Separable-Conv/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>可分离卷积卷积分为空间可分离卷积和深度可分离卷积</p>
<p><strong>空间可分卷积</strong></p>
<p>优点：空间可分卷积能节省成本，计算成本为标准卷积的 2/h（h为卷积核size）</p>
<p>缺点：并非所有的核都能分成两个更小的核。如果用空间可分卷积替代所有的传统卷积，在训练过程中搜索所有可能的核。这样得到的训练结果可能是次优的。</p>
<p><strong>深度可分卷积</strong></p>
<p>优点：深度可分卷积会降低卷积中参数的数量，计算成本为标准卷积的 1/(h*h)（h为filter的size）</p>
<p>缺点：对于较小的模型而言，如果用深度可分卷积替代 2D 卷积，模型的能力可能会显著下降。但是，如果使用得当，深度可分卷积能在不降低模型性能的前提下帮助你实现效率提升。</p>
<span id="more"></span>



<h1 id="可分离卷积"><a href="#可分离卷积" class="headerlink" title="可分离卷积"></a>可分离卷积</h1><p>使用模型： MobileNet， Xcaption</p>
<h2 id="空间可分卷积"><a href="#空间可分卷积" class="headerlink" title="空间可分卷积"></a>空间可分卷积</h2><p>空间可分卷积操作的是图像的 2D 空间维度，即高和宽。从概念上看，空间可分卷积是将一个卷积分解为两个单独的运算。对于下面的示例，3×3 的 Sobel 核被分成了一个 3×1 核和一个 1×3 核。</p>
<p><img src="/2021/10/01/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E6%B7%B1%E5%BA%A6%E5%8F%AF%E5%88%86%E7%A6%BB%E5%8D%B7%E7%A7%AFDepthwise-Separable-Conv/%E5%8F%AF%E5%88%86%E7%A6%BB1.png"></p>
<p>在卷积中，3×3 核直接与图像卷积。在空间可分卷积中，3×1 核首先与图像卷积，然后再应用 1×3 核。这样，执行同样的操作时仅需 6 个参数，而不是 9 个。此外，使用空间可分卷积时所需的矩阵乘法也更少。</p>
<p>例如，5×5 图像与 3×3 核的卷积（步幅=1，填充=0）要求在 3 个位置水平地扫描核（还有 3 个垂直的位置）。总共就是 9 个位置，表示为下图中的点。在每个位置，会应用 9 次逐元素乘法。总共就是 9×9=81 次乘法。</p>
<p>但是，先在 5×5 的图像上应用一个 3×1 的过滤器。我们可以在水平 5 个位置和垂直 3 个位置扫描这样的核。总共就是 5×3=15 个位置，表示为下图中的点。在每个位置，会应用 3 次逐元素乘法。总共就是 15×3=45 次乘法。现在得到了一个 3×5 的矩阵。这个矩阵再与一个 1×3 核卷积，即在水平 3 个位置和垂直 3 个位置扫描这个矩阵。对于这 9 个位置中的每一个，应用 3 次逐元素乘法。这一步需要 9×3=27 次乘法。因此，总体而言，空间可分卷积需要 45+27=72 次乘法，少于普通卷积。</p>
<p><img src="/2021/10/01/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E6%B7%B1%E5%BA%A6%E5%8F%AF%E5%88%86%E7%A6%BB%E5%8D%B7%E7%A7%AFDepthwise-Separable-Conv/%E5%8F%AF%E5%88%86%E7%A6%BB2.jpg"></p>
<p><strong>归纳：</strong>假设我们现在将卷积应用于一张 N×N 的图像上，卷积核为 m×m，步幅为 1，填充为 0。传统卷积需要 (N-2) x (N-2) x m x m 次乘法，空间可分卷积需要 N x (N-2) x m + (N-2) x (N-2) x m = (2N-2) x (N-2) x m 次乘法。空间可分卷积与标准卷积的计算成本比为：</p>
<p><img src="/2021/10/01/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E6%B7%B1%E5%BA%A6%E5%8F%AF%E5%88%86%E7%A6%BB%E5%8D%B7%E7%A7%AFDepthwise-Separable-Conv/%E5%8F%AF%E5%88%86%E7%A6%BB3.png"></p>
<p>因为图像尺寸 N 远大于过滤器大小（N&gt;&gt;m），所以这个比就变成了 2/m。也就是说，在这种渐进情况（N&gt;&gt;m）下，当过滤器大小为 3×3 时，空间可分卷积的计算成本是标准卷积的 2/3。过滤器大小为 5×5 时这一数值是 2/5；过滤器大小为 7×7 时则为 2/7。</p>
<p><strong>优点：</strong>空间可分卷积能节省成本</p>
<p><strong>缺点：</strong>并非所有的核都能分成两个更小的核。如果用空间可分卷积替代所有的传统卷积，在训练过程中搜索所有可能的核。这样得到的训练结果可能是次优的。</p>
<h2 id="深度可分卷积"><a href="#深度可分卷积" class="headerlink" title="深度可分卷积"></a>深度可分卷积</h2><p>MobileNet 和 Xception</p>
<p>深度可分卷积包含两个步骤： 深度卷积 +  1×1 逐点卷积</p>
<p>例如：输入层的大小是 7×7×3（高×宽×通道） 需要转换成输出层（5×5×128）</p>
<p>标准的2D卷积的操作如下：128 个 5×5×1 的输出映射图（map）。然后我们将这些映射图堆叠成大小为 5×5×128 的单层</p>
<p><img src="/2021/10/01/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E6%B7%B1%E5%BA%A6%E5%8F%AF%E5%88%86%E7%A6%BB%E5%8D%B7%E7%A7%AFDepthwise-Separable-Conv/%E6%B7%B1%E5%BA%A6%E5%8F%AF%E5%88%86%E7%A6%BB1.jpg"></p>
<p>深度可分卷积实现同样的变换: </p>
<p>深度可分卷积——第一步：分开使用 3 个核。每个过滤器的大小为 3×3×1。每个核与输入层的一个通道卷积（仅一个通道，而非所有通道！）。每个这样的卷积都能提供大小为 5×5×1 的映射图。然后将这些映射图堆叠在一起，创建一个 5×5×3 的图像。操作之后得到大小为 5×5×3 的输出。现在降低空间维度了，但深度还是和之前一样。</p>
<p><img src="/2021/10/01/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E6%B7%B1%E5%BA%A6%E5%8F%AF%E5%88%86%E7%A6%BB%E5%8D%B7%E7%A7%AFDepthwise-Separable-Conv/%E6%B7%B1%E5%BA%A6%E5%8F%AF%E5%88%86%E7%A6%BB2.jpg"></p>
<p>在深度可分卷积的第二步，逐点卷积，为了扩展深度，应用128个核大小为 1×1×3 的 1×1 卷积。将 5×5×3 的输入图像与每个 1×1×3 的核卷积，可得到大小为 5×5×1 的映射图。</p>
<p><img src="/2021/10/01/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E6%B7%B1%E5%BA%A6%E5%8F%AF%E5%88%86%E7%A6%BB%E5%8D%B7%E7%A7%AFDepthwise-Separable-Conv/%E6%B7%B1%E5%BA%A6%E5%8F%AF%E5%88%86%E7%A6%BB3.jpg"></p>
<p><img src="/2021/10/01/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E6%B7%B1%E5%BA%A6%E5%8F%AF%E5%88%86%E7%A6%BB%E5%8D%B7%E7%A7%AFDepthwise-Separable-Conv/%E6%B7%B1%E5%BA%A6%E5%8F%AF%E5%88%86%E7%A6%BB4.jpg"></p>
<p><strong>2D 卷积例子的成本：</strong>有 128 个 3×3×3 个核移动了 5×5 次，也就是 128 x 3 x 3 x 3 x 5 x 5 = 86400 次乘法。</p>
<p><strong>可分卷积的成本：</strong></p>
<p>在第一个深度卷积步骤，有 3 个 3×3×1 核移动 5×5 次，也就是 3x3x3x1x5x5 = 675 次乘法。</p>
<p>在 1×1 卷积的第二步，有 128 个 1×1×3 核移动 5×5 次，即 128 x 1 x 1 x 3 x 5 x 5 = 9600 次乘法。因此，深度可分卷积共有 675 + 9600 = 10275 次乘法。这样的成本大概仅有 2D 卷积的 12%！</p>
<p><strong>归纳：</strong>对于大小为 H×W×D 的输入图像，如果使用 Nc 个大小为 h×h×D 的核执行 2D 卷积（步幅为 1，填充为 0，其中 h 是偶数）。为了将输入层（H×W×D）变换到输出层（(H-h+1)x (W-h+1) x Nc），所需的总乘法次数为：</p>
<p><strong>Nc x h x h x D x (H-h+1) x (W-h+1)</strong></p>
<p>另一方面，对于同样的变换，深度可分卷积所需的乘法次数为：</p>
<p>**D x h x h x 1 x (H-h+1) x (W-h+1) + Nc x 1 x 1 x D x (H-h+1) x (W-h+1) **</p>
<p><strong>= (h x h + Nc) x D x (H-h+1) x (W-h+1)</strong></p>
<p>则深度可分卷积与 2D 卷积所需的乘法次数比为：</p>
<p><img src="/2021/10/01/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E6%B7%B1%E5%BA%A6%E5%8F%AF%E5%88%86%E7%A6%BB%E5%8D%B7%E7%A7%AFDepthwise-Separable-Conv/%E6%B7%B1%E5%BA%A6%E5%8F%AF%E5%88%86%E7%A6%BB5.png"></p>
<p>大多数架构的输出层通常都有很多通道，可达数百甚至上千。对于这样的层（Nc &gt;&gt; h），则上式可约简为 1 / h²。基于此，如果使用 3×3 过滤器，则 2D 卷积所需的乘法次数是深度可分卷积的 9 倍。如果使用 5×5 过滤器，则 2D 卷积所需的乘法次数是深度可分卷积的 25 倍。</p>
<p>优点：深度可分卷积会降低卷积中参数的数量</p>
<p>缺点：对于较小的模型而言，如果用深度可分卷积替代 2D 卷积，模型的能力可能会显著下降。但是，如果使用得当，深度可分卷积能在不降低模型性能的前提下帮助你实现效率提升。</p>
]]></content>
      <categories>
        <category>学习</category>
        <category>cv算法</category>
        <category>深度学习</category>
        <category>目标检测</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>论文复现</tag>
      </tags>
  </entry>
  <entry>
    <title>目标检测-R-CNN系列</title>
    <url>/2021/09/29/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B-R-CNN%E7%B3%BB%E5%88%97/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>背景问题：</p>
<p>方法简述：</p>
<p>优点：</p>
<p>缺点：</p>
<span id="more"></span>
]]></content>
      <categories>
        <category>学习</category>
        <category>cv算法</category>
        <category>深度学习</category>
        <category>目标检测</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>论文复现</tag>
      </tags>
  </entry>
  <entry>
    <title>目标检测-Retina-Net</title>
    <url>/2021/09/29/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B-Retina-Net/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>背景问题：</p>
<p>方法简述：</p>
<p>优点：</p>
<p>缺点：</p>
<span id="more"></span>
]]></content>
      <categories>
        <category>学习</category>
        <category>cv算法</category>
        <category>深度学习</category>
        <category>目标检测</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>论文复现</tag>
      </tags>
  </entry>
  <entry>
    <title>目标检测-SSD系列</title>
    <url>/2021/09/29/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B-SSD%E7%B3%BB%E5%88%97/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>背景问题：解决速度和精度问题</p>
<p>方法简述：</p>
<p>SSD （Single Shot MultiBox Detector）是一个one-stage目标检测模型，在一个网络中就可以同时完成目标检测和分类。通过CNN进行特征提取，均匀的在不同位置采用不同的尺寸和长宽比进行密集抽样。同时做物体分类和预选框位置的回归，使得模型速度十分快。</p>
<p>优点：</p>
<p>速度快；准确度相对来说比较高；</p>
<p>缺点：</p>
<p>均匀密集采样带来的缺点是正样本与负样本（背景）极其不均衡，会导致模型准确度降低，模型训练比较困难</p>
<span id="more"></span>

<h1 id="设计思路"><a href="#设计思路" class="headerlink" title="设计思路"></a>设计思路</h1><p><img src="/2021/09/29/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B-SSD%E7%B3%BB%E5%88%97/zhendeliu/Documents/GitHub/gitblog/source/_posts/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B-SSD%E7%B3%BB%E5%88%97/%E7%BB%86%E8%8A%82%E6%9E%B6%E6%9E%84.png"></p>
<p>SSD anchor的选择具体参考【目标检测中的anchor】</p>
<h1 id="论文精读"><a href="#论文精读" class="headerlink" title="论文精读"></a>论文精读</h1><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>SSD 是将bounding box的输出空间离散化到一组不同比例的默认框，scales每个特征映射的位置</p>
<p>预测阶段：对每个默认框分布在各个类别上进行评分，同时调整 #默认框？？？使得与物体形状更加匹配；另外，网络结合了来自多个不同分辨率的特征图的预测结果，可以自然处理不同size的目标。SSD和 object proposal 方法只有简单的相关（完全取消了预选框的生成和后序的像素处理和重采样阶段），将所有的计算都封装在一个网络。</p>
<p>SSD mAP达到76.9% 超过Faster R-CNN； 并且比其他单阶段模型有更高的accuracy，即便是输入更小的图片。</p>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2>]]></content>
      <categories>
        <category>学习</category>
        <category>cv算法</category>
        <category>深度学习</category>
        <category>目标检测</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>论文复现</tag>
      </tags>
  </entry>
  <entry>
    <title>目标检测-YOLOv2</title>
    <url>/2021/10/01/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B-YOLOv2/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>背景问题：从预测<strong>更准确（Better）</strong>，<strong>速度更快（Faster）</strong>，<strong>识别对象更多（Stronger）</strong>这三个方面进行了改进。</p>
<p>方法简述：YOLOv2又叫YOLO9000，其能检测超过9000种类别的物体，在VOC2007数据集中在76FPS的速度下，能达到76.8%的mAP，在40FPS的速度下，能达到78.6%的mAP，很好的达到速度与精度的平衡。</p>
<span id="more"></span>

<h1 id="YOLOv2"><a href="#YOLOv2" class="headerlink" title="YOLOv2"></a>YOLOv2</h1><p>从预测<strong>更准确（Better）</strong>，<strong>速度更快（Faster）</strong>，<strong>识别对象更多（Stronger）</strong>这三个方面进行了改进。</p>
<p>采用联合训练算法的基本思路就是：同时在检测数据集和分类数据集上训练物体检测器（Object Detectors ），<strong>用检测数据集的数据学习物体的准确位置，用分类数据集的数据来增加分类的类别量、提升健壮性。</strong></p>
<h2 id="相比于v1的改进"><a href="#相比于v1的改进" class="headerlink" title="相比于v1的改进"></a><strong>相比于v1的改进</strong></h2><p><strong>算法层面：</strong></p>
<ul>
<li><p><strong>Anchor:<strong>引入了Faster R-CNN中使用的Anchor，作者在YOLOv2中设计的Anchor并不是像Faster R-CNN中人为事先设计的尺寸和高宽比一级个数，作者通过在所有训练图像的所有边界框上运行k-means聚类来选择锚的个数和形状(k = 5，因此它找到五个最常见的目标形状)。</strong>因此，YOLO的锚是特定于您正在训练(和测试)的数据集的。</strong>k-means算法找到了将所有数据点划分聚类的方法。这里的数据点是数据集中所有真实边界框的宽度和高度。但5个锚是否最佳选择?我们可以在不同数量的聚类上多次运行k-means，并计算真实标签框与它们最接近的锚框之间的平均IOU。毫无疑问，使用更多质心(k值越大)平均IOU越高，但这也意味着我们需要在每个网格单元中使用更多的检测器，并使模型运行速度变慢。对于YOLO v2，他们选择了5个锚作为召回率和模型复杂度之间的良好折衷。</p>
</li>
<li><p>**坐标预测:**在这里作者虽然引入了Faster R-CNN中类似的anchor，但是作者并没有像其意义，对bbox中心坐标的预测是基于anchor坐标的偏移量得到的，而是采用了v1中预测anchor中心点相对于对于单元格左上角位置的偏移</p>
</li>
<li><p><strong>损失函数：</strong></p>
<p>1.在计算类概率误差时，YOLOv1中仅对每个单元格计算；而YOLOv2中对每一个anchor box都会计算类概率误差。</p>
<p>2.YOLOv1中使用w和h的开方来缓和box的尺寸不平衡问题，而在YOLOv2中则通过赋值一个和w，h相关的权重函数达到该目的。</p>
<p>3.与YOLOv1不同的是修正系数的改变，YOLOv1中no_objects_loss和objects_loss分别是0.5和1，而YOLOv2中则是1和5</p>
</li>
</ul>
<p><strong>网络层面：</strong></p>
<ul>
<li>Darknet19: 与v1不同采用的是全卷积网络，取掉了v1中的全连接层，改用全局平均池化，去掉v1中最后一个池化层，增加特征的分辨率。网络共19个卷积层，5个最大池化层，</li>
</ul>
<p><strong>训练检测方面：</strong></p>
<ul>
<li><strong>训练图像分辨率：</strong>v1在ImageNet上预训练时用的224x224尺寸的图片，正式训练时用448x448,这需要模型适应新的分辨率。YOLOv2是直接使用448x448的输入训练，随着输入分辨率的增加，模型提高了4%的mAP。</li>
<li><strong>使用了WordTree：</strong>通过WordTree来混合检测数据集与识别数据集之中的数据，使得这一网络结构可以实时地检测超过9000种物体分类。</li>
<li><strong>联合训练算法：</strong>使用这种联合训练技术同时在ImageNet和COCO数据集上进行训练。YOLO9000进一步缩小了监测数据集与识别数据集之间的代沟。联合训练算法的基本思路就是：同时在检测数据集和分类数据集上训练物体检测器（Object Detectors ），用检测数据集的数据学习物体的准确位置，用分类数据集的数据来增加分类的类别量、提升健壮性。分类信息学习自ImageNet分类数据集，而物体位置检测则学习自COCO检测数据集。</li>
<li><strong>多尺度训练：</strong>为了提高模型的鲁棒性，在训练的时候采用了多尺度的输入进行训练，由于网络的下采样因子是32，故输入尺寸选择32的倍数288，352，…，544</li>
<li><strong>多尺度检测，reorg层：</strong>作者将前一层的26<em>26的特征图做一个reorg操作，将其变成13</em>13但又不破坏其大特征图的特征，然后和本层的13*13的1特征图进行concat。</li>
</ul>
<p><strong>技巧方面：</strong></p>
<ul>
<li><strong>Batch Normalization：</strong>使用Batch Normalization对网络进行优化，让网络提高了收敛性，同时还消除了对其他形式的正则化（regularization）的依赖。通过对YOLO的每一个卷积层增加Batch Normalization，最终使得mAP提高了2%，同时还使model正则化。使用Batch Normalization可以从model中去掉Dropout，而不会产生过拟合。</li>
</ul>
<ol>
<li><p>改进点</p>
<ol>
<li><p>BN层</p>
</li>
<li><p>高分辨率分类 224*224 -》448 * 448</p>
</li>
<li><p>Convolution with anchor boxes： YOLOv1包含有全连接层，从而能直接预测Bounding Boxes的坐标值。Faster R-CNN算法只用卷积层与Region Proposal Network来预测Anchor Box的偏移值与置信度，而不是直接预测坐标值，YOLOv2作者发现通过预测偏移量而不是坐标值能够简化问题，让神经网络学习起来更容易。</p>
<p>借鉴Faster RCNN的做法，YOLOv2也尝试采用先验框（anchor）。在每个grid预先设定一组不同大小和宽高比的边框，来覆盖整个图像的不同位置和多种尺度，这些先验框作为预定义的候选区在神经网络中将检测其中是否存在对象，以及微调边框的位置</p>
</li>
<li><p>YOLOv2的做法是对训练集中标注的边框进行K-means聚类分析，以寻找尽可能匹配样本的边框尺寸。</p>
</li>
</ol>
</li>
</ol>
<p>下面我们具体看看y1,y2,y3是如何而来的。<br>网络中作者进行了三次检测，分别是在32倍降采样，16倍降采样，8倍降采样时进行检测，这样在多尺度的feature map上检测跟SSD有点像。在网络中使用up-sample（上采样）的原因:网络越深的特征表达效果越好，比如在进行16倍降采样检测，如果直接使用第四次下采样的特征来检测，这样就使用了浅层特征，这样效果一般并不好。如果想使用32倍降采样后的特征，但深层特征的大小太小，因此YOLOv3使用了步长为2的up-sample（上采样），把32倍降采样得到的feature map的大小提升一倍，也就成了16倍降采样后的维度。同理8倍采样也是对16倍降采样的特征进行步长为2的上采样，这样就可以使用深层特征进行detection。</p>
<p>作者通过上采样将深层特征提取，其维度是与将要融合的特征层维度相同的（channel不同）。如下图所示，85层将13×13×256的特征上采样得到26×26×256，再将其与61层的特征拼接起来得到26×26×768。为了得到channel255，还需要进行一系列的3×3，1×1卷积操作，这样既可以提高非线性程度增加泛化性能提高网络精度，又能减少参数提高实时性。52×52×255的特征也是类似的过程。</p>
]]></content>
      <categories>
        <category>学习</category>
        <category>cv算法</category>
        <category>深度学习</category>
        <category>目标检测</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>论文复现</tag>
        <tag>YOLO</tag>
      </tags>
  </entry>
  <entry>
    <title>目标检测-YOLOv3</title>
    <url>/2021/10/01/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B-YOLOv3/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>背景问题：</p>
<p>方法简述：</p>
<p>优点：</p>
<p>缺点：</p>
<span id="more"></span>

<p><img src="/2021/10/01/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B-YOLOv3/zhendeliu/Documents/GitHub/gitblog/source/_posts/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B-YOLO%E7%B3%BB%E5%88%97/1.png"></p>
<h2 id="改进点"><a href="#改进点" class="headerlink" title="改进点"></a><strong>改进点</strong></h2><ul>
<li><p><strong>网络：</strong>Darknet53，采用简化的residual block 取代了原来 1×1 和 3×3的block; (其实就是加了一个shortcut，也是网络加深必然所要采取的手段)。这和上一点是有关系的，v2的darknet-19变成了v3的darknet-53，为啥呢？就是需要上采样啊，卷积层的数量自然就多了，另外作者还是用了一连串的3<em>3、1</em>1卷积，3<em>3的卷积增加channel，而1</em>1的卷积在于压缩3*3卷积后的特征表示。</p>
</li>
<li><p><strong>分类损失：</strong>在YOLOv3中，每个框用多标签分类来预测边界框可能包含的类。该算法将v2中的softmax替换成了逻辑回归loss，在训练过程中使用二原交叉熵损失来进行类别预测。对于重叠的标签，多标签方法可以更好的模拟数据。</p>
</li>
<li><p><strong>跨尺度预测：</strong>YOLOv3采用多个尺度融合的方式做预测。原来YOLOv2中有一个层叫：passthrough layer，假设最后提取的特征图尺度是13<em>13，那么这个层的作用就是将前面一层的26</em>26的特征图和本层13<em>13的特征图进行连接，有点像ResNet。这样的操作是为了加强YOLO算法对小目标检测的精度。在YOLOv3中，作者采用了类似与FPN的上采样和融合做法（最后融合了3个尺度，其他2个尺度分别是26</em>26和52*52），在多给尺度的特征图上做预测，对于小目标的提升效果还是非常明显的。虽然在YOLOv3中每个网格预测3个边界框，比v2中的5个要少，但v3采用了多个尺度的特征融合，所以边界框的数量也比之前多很多。</p>
</li>
</ul>
<h2 id="尝试，但效果不好的工作"><a href="#尝试，但效果不好的工作" class="headerlink" title="尝试，但效果不好的工作"></a><strong>尝试，但效果不好的工作</strong></h2><ul>
<li><strong>Anchor box坐标的偏移预测。</strong>作者尝试了常规的Anchor box预测方法，比如利用线性激活将坐标x、y的偏移程度预测为边界框宽度或高度的倍数。但发现这种做法降低了模型的稳定性，且效果不佳。 用线性方法预测x,y，而不是使用逻辑方法。我们尝试使用线性激活来直接预测x，y的offset，而不是逻辑激活，还降低了mAP。</li>
</ul>
<ul>
<li><strong>focal loss。</strong>我们尝试使用focal loss，但使我们的mAP降低了2%。 对于focal loss函数试图解决的问题，YOLOv3从理论上来说已经很强大了，因为它具有单独的对象预测和条件类别预测。因此，对于大多数例子来说，类别预测没有损失？或者其他的东西？我们并不完全确定。</li>
</ul>
<ul>
<li><strong>双IOU阈值和真值分配。</strong>在训练期间，Faster RCNN用了两个IOU阈值，如果预测的边框与ground truth的IoU&gt;0.7，那它是个正样本；如果在[0.3，0.7]之间，则忽略；如果和ground truth的IoU&lt;0.3，那它就是个负样本。作者尝试了这种思路，但效果并不好。</li>
</ul>
]]></content>
      <categories>
        <category>学习</category>
        <category>cv算法</category>
        <category>深度学习</category>
        <category>目标检测</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>论文复现</tag>
        <tag>YOLO</tag>
      </tags>
  </entry>
  <entry>
    <title>目标检测-YOLOv4</title>
    <url>/2021/10/01/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B-YOLOv4/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>背景问题：</p>
<p>方法简述：</p>
<p>优点：</p>
<p>缺点：</p>
<span id="more"></span>

<p>YOLO V4使用 CIOU Loss作为bounding box的损失，与其他提到的方法相比，CIOU带来了更快的收敛和更好的性能。</p>
<p>尽管YOLO V5目前仍然计逊一筹，但是YOLO V5仍然具有以下显著的优点：</p>
<ul>
<li>使用Pytorch框架，对用户非常友好，能够方便地训练自己的数据集，相对于YOLO V4采用的Darknet框架，Pytorch框架更容易投入生产</li>
<li>代码易读，整合了大量的计算机视觉技术，非常有利于学习和借鉴</li>
<li>不仅易于配置环境，模型训练也非常快速，并且批处理推理产生实时结果</li>
<li>能够直接对单个图像，批处理图像，视频甚至网络摄像头端口输入进行有效推理</li>
<li>能够轻松的将Pytorch权重文件转化为安卓使用的ONXX格式，然后可以转换为OPENCV的使用格式，或者通过CoreML转化为IOS格式，直接部署到手机应用端</li>
<li>最后YOLO V5s高达140FPS的对象识别速度令人印象非常深刻，使用体验非常棒</li>
</ul>
]]></content>
      <categories>
        <category>学习</category>
        <category>cv算法</category>
        <category>深度学习</category>
        <category>目标检测</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>论文复现</tag>
        <tag>YOLO</tag>
      </tags>
  </entry>
  <entry>
    <title>目标检测-YOLOv5</title>
    <url>/2021/10/01/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B-YOLOv5/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>背景问题：</p>
<p>方法简述：</p>
<p>优点：</p>
<p>缺点：</p>
<span id="more"></span>





<p>YOLO V5使用 GIOU Loss作为bounding box的损失。</p>
<p>YOLO V5使用二进制交叉熵和 Logits 损失函数计算类概率和目标得分的损失。同时我们也可以使用fl _ gamma参数来激活Focal loss计算损失函数。</p>
]]></content>
      <categories>
        <category>学习</category>
        <category>cv算法</category>
        <category>深度学习</category>
        <category>目标检测</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>论文复现</tag>
        <tag>YOLO</tag>
      </tags>
  </entry>
  <entry>
    <title>目标检测-YOLOx</title>
    <url>/2021/10/01/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B-YOLOx/</url>
    <content><![CDATA[<ul>
<li><h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>背景问题：</p>
<p>方法简述：</p>
<p>优点：</p>
<p>缺点：</p>
<span id="more"></span></li>
</ul>
]]></content>
      <categories>
        <category>学习</category>
        <category>cv算法</category>
        <category>深度学习</category>
        <category>目标检测</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>论文复现</tag>
        <tag>YOLO</tag>
      </tags>
  </entry>
  <entry>
    <title>目标检测-YOLOv1</title>
    <url>/2021/09/29/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B-YOLO%E7%B3%BB%E5%88%97/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>YOLOv1是典型的目标检测one stage方法，用回归的方法去做目标检测，执行速度快，达到非常高效的检测，其背后的原理和思想也非常简单。YOLOv1的基本思想是把一副图片，首先reshape成448x448大小（由于网络中使用了全连接层，所以图片的尺寸需固定大小输入到CNN中），然后将划分成SxS个单元格（原文中S=7），以每个格子所在位置和对应内容为基础，来预测：</p>
<p>方法简述：</p>
<p>优点：</p>
<p>缺点：</p>
<span id="more"></span>

<h1 id="YOLOv1"><a href="#YOLOv1" class="headerlink" title="YOLOv1"></a>YOLOv1</h1><ol>
<li><p>核心思想 回归思想</p>
<p>整张图作为网络的输入，在输出层回归Bounding Box的位置和Bounding Box的类别</p>
<p>相比Faster R-CNN虽然也是整张图输入，但Proposal+ classifier没变, 只是将Proposal也放在了CNN中</p>
</li>
<li><p>实现方法</p>
<ol>
<li><p>将整张图分为S*S的网格，如果某个object的中心落在这个网格中，这个网格就负责预测这个object</p>
</li>
<li><p>每个网格要预测B个bounding box，每个bounding box除了要回归自身的位置之外，还要附带预测一个confidence值。这个confidence代表了所预测的box中含有object的置信度和这个box预测的有多准两重信息</p>
</li>
<li><p>每个bounding Box 有五个值要预测(x, y, w, h)和confidence。每个网格还要预测一个类别信息，记为C类。则SxS个网格，每个网格要预测B个bounding box还要预测C个categories。输出就是<strong>S x S x (5*B+C)的一个tenso</strong>r。</p>
</li>
<li><p><strong>简单的概括就是：</strong></p>
<ul>
<li>给个一个输入图像，首先将图像划分成7*7的网格；</li>
<li>对于每个网格，我们都预测2个边框（包括每个边框是目标的置信度以及每个边框区域在多个类别上的概率）；</li>
<li>根据上一步可以预测出7 * 7 * 2个目标窗口，然后根据阈值去除可能性比较低的目标窗口，最后NMS去除冗余窗口即可。</li>
<li>每个网格包含B个检测框和一个C个类别的概率值，每个检测框含有五个值要预测(x, y, w, h)和confidence</li>
</ul>
</li>
<li><p><strong>注意（重要细节）:</strong></p>
<p>\1. x，y，w，h，confidence都被限制在区间[0,1]。</p>
<p>\2. 置信度confidence值只有2种情况，要么为0（边界框中不含目标，P(object)=0），要么为预测框与标注框的IOU，因为P(Object)只有0或1，两种可能，有目标的中心落在格子内，那么P(object)=1，否则为0，不存在（0，1）区间中的值。其他论文中置信度的定义可能跟YOLOv1有些不同，一般置信度指的是预测框中是某类别目标的概率，在[0,1]之间。</p>
<p>\3. 每个格子预测C个类别的概率分数，而不是每个每个检测框都需要预测C个类别的概率分数。</p>
</li>
</ol>
</li>
<li><p>在YOLOv1的损失函数中：</p>
<ol>
<li>YOLOv1对位置误差，confidence误差，分类误差均使用了均方差作为损失函数。</li>
<li>三部分误差损失（位置误差，confidence误差，分类误差），在损失函数中所占权重不一样，位置误差权重系数最大，为5</li>
<li>由于一副图片中没有目标的网格占大多数，有目标的网格占少数，所以损失函数中对没有目标的网格中预测的bbox的confidence误差给予小的权重系数，为0.5。</li>
<li>有目标的网格中预测的bbox的confidence损失和分类损失，权重系数正常为1。</li>
<li>由于相同的位置误差对大目标和小目标的影响是不同的，相同的偏差对于小目标来说影响要比大目标大，故作者选择将预测的bbox的w,h先取其平方根，再求均方差损失。</li>
<li>一个网格预测2个bbox，在计算损失函数的时候，只取与ground truth box中IoU大的那个预测框来计算损失。</li>
<li>分类误差，只有当单元格中含有目标时才计算，没有目标的单元格的分类误差不计算在内。</li>
</ol>
</li>
<li><p>激活函数</p>
<ul>
<li>最后一层全连接层用线性激活函数</li>
<li>其余层采用leak RELU</li>
<li></li>
</ul>
</li>
</ol>
<h1 id="YOLOv1的缺陷"><a href="#YOLOv1的缺陷" class="headerlink" title="YOLOv1的缺陷"></a>YOLOv1的缺陷</h1><ul>
<li>首先，每个单元格只预测2个bbox，然后每个单元格最后只取与gt_bbox的IOU高的那个最为最后的检测框，也只是说每个单元格最多只预测一个目标，若单个单元格有多个目标时，只能检测出其他的一个，导致小目标漏检，因此YOLOv1对小目标检测效果不好。</li>
<li>其次，虽然YOLOv1中损失函数中位置误差，对预测的w，h取平方根处理再求均方差，来缓解相同位置误差对大目标，小目标影响不同的弊端，但是作用甚微，没有根本解决问题对于小物体。小的目标的 置信度误差也会对网络优化过程造成很大的影响，从而降低了物体检测的定位准确性。</li>
<li>由于输出层为全连接层，因此在检测时，YOLO 训练模型只支持与训练图像相同的输入分辨率的图片。</li>
</ul>
<ol>
<li><p>Loss= 坐标预测的Loss + 含object的Box的confidence预测 + 不含object的Box的confidence预测 + 类别预测</p>
<p>只有当某个网格中有object的时候才对classification error进行惩罚。<br>只有当某个box predictor对某个ground truth box负责的时候，才会对box的coordinate error进行惩罚，而对哪个ground truth box负责就看其预测值和ground truth box的IoU是不是在那个cell的所有box中最大。<br>注：</p>
<p>YOLOv1方法模型训练依赖于物体识别标注数据，因此，对于非常规的物体形状或比例，YOLOv1的检测效果并不理想。<br>YOLOv1采用了多个下采样层，网络学到的物体特征并不精细，因此也会影响检测效果。<br>YOLOv1的loss函数中，大物体IOU误差和小物体IOU误差对网络训练中loss贡献值接近（虽然采用求平方根方式，但没有根本解决问题）。因此，对于小物体，小的IOU误差也会对网络优化过程造成很大的影响，从而降低了物体检测的定位准确性。<br>YOLO的缺点</p>
<p>YOLO对相互靠的很近的物体和很小的群体检测效果不好，这是因为一个网格中只预测了两个框，并且只属于一类；<br>同一类物体出现的新的不常见的长宽比和其他情况时，泛化能力偏弱；<br>由于损失函数的问题，定位误差是影响检测效果的主要原因。尤其是大小物体的处理上，还有待加强。</p>
</li>
</ol>
]]></content>
      <categories>
        <category>学习</category>
        <category>cv算法</category>
        <category>深度学习</category>
        <category>目标检测</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>论文复现</tag>
        <tag>YOLO</tag>
      </tags>
  </entry>
  <entry>
    <title>目标检测中候选边界框的处理(NMS,soft-NMS,WBF)</title>
    <url>/2021/10/11/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B8%AD%E5%80%99%E9%80%89%E8%BE%B9%E7%95%8C%E6%A1%86%E7%9A%84%E5%A4%84%E7%90%86-NMS-soft-NMS-WBF/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>目标检测的过程中在同一目标的位置上会产生大量的候选框，这些候选框相互之间可能会有重叠，此时我们需要消除冗余的边界框。</p>
<p><strong>NMS非极大值抑制：</strong>找到局部极大值，并筛除（抑制）邻域内其余的值。</p>
<p><strong>Soft-NMS：</strong>不直接剔除IOU大的框，IoU越大（即，重叠的越多），置信度降低的越强烈</p>
<p><strong>WBF 加权框融合</strong>：WBF采用的融合框的置信度为所有形成他的边界框的平均置信度，而其坐标也是所有形成他的边界框的坐标的加权和，权重为相应的边界框的置信度。也就是置信度越高的边界框对于融合边界框的坐标做出的贡献越大。</p>
<span id="more"></span>

<h1 id="NMS-非极大值抑制"><a href="#NMS-非极大值抑制" class="headerlink" title="NMS 非极大值抑制"></a>NMS 非极大值抑制</h1><p>Non-Maximum Suppression是非“极大值”抑制，作用：找到局部极大值，并筛除（抑制）邻域内其余的值。</p>
<p>算法流程</p>
<ol>
<li>将所有的框按类别划分，并剔除背景类，因为无需NMS。</li>
<li>对每个物体类中的边界框(B_BOX)，按照分类置信度降序排列。</li>
<li>在某一类中，选择置信度最高的边界框B_BOX1，将B_BOX1从输入列表中去除，并加入输出列表。</li>
<li>逐个计算B_BOX1与其余B_BOX2的交并比IoU，若IoU(B_BOX1,B_BOX2) &gt; 阈值TH，则在输入去除B_BOX2。</li>
<li>重复步骤3~4，直到输入列表为空，完成一个物体类的遍历。</li>
<li>重复2~5，直到所有物体类的NMS处理完成。</li>
<li>输出列表，算法结束</li>
</ol>
<p>缺点：</p>
<ol>
<li>密集重叠场景造成误过滤：将得分较低的边框强制性地去掉，如果物体出现较为密集时，本身属于两个物体的边框，其中得分较低的也有可能被抑制掉，降低了模型的召回率。</li>
<li>速度：NMS的实现存在较多的循环步骤，GPU的并行化实现不是特别容易，尤其是预测框较多时，耗时较多。</li>
<li>框的置信度和分类的置信度并不是完全对齐的，NMS简单地将得分作为一个边框的置信度，但在一些情况下，得分高的边框不一定位置更准</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">py_cpu_nms</span>(<span class="params">dets, thresh</span>):</span> </span><br><span class="line">    <span class="comment">#dets某个类的框，x1、y1、x2、y2、以及置信度score</span></span><br><span class="line">    <span class="comment">#eg:dets为[[x1,y1,x2,y2,score],[x1,y1,y2,score]……]]</span></span><br><span class="line">    <span class="comment"># thresh是IoU的阈值     </span></span><br><span class="line">    x1 = dets[:, <span class="number">0</span>] </span><br><span class="line">    y1 = dets[:, <span class="number">1</span>]</span><br><span class="line">    x2 = dets[:, <span class="number">2</span>] </span><br><span class="line">    y2 = dets[:, <span class="number">3</span>] </span><br><span class="line">    scores = dets[:, <span class="number">4</span>] </span><br><span class="line">    <span class="comment">#每一个检测框的面积 </span></span><br><span class="line">    areas = (x2 - x1 + <span class="number">1</span>) * (y2 - y1 + <span class="number">1</span>) </span><br><span class="line">    <span class="comment">#按照score置信度降序排序 </span></span><br><span class="line">    order = scores.argsort()[::-<span class="number">1</span>] </span><br><span class="line">    keep = [] <span class="comment">#保留的结果框集合 </span></span><br><span class="line">    <span class="keyword">while</span> order.size &gt; <span class="number">0</span>: </span><br><span class="line">        i = order[<span class="number">0</span>] </span><br><span class="line">        keep.append(i) <span class="comment">#保留该类剩余box中得分最高的一个 </span></span><br><span class="line">        <span class="comment">#得到相交区域,左上及右下 </span></span><br><span class="line">        xx1 = np.maximum(x1[i], x1[order[<span class="number">1</span>:]]) </span><br><span class="line">        yy1 = np.maximum(y1[i], y1[order[<span class="number">1</span>:]]) </span><br><span class="line">        xx2 = np.minimum(x2[i], x2[order[<span class="number">1</span>:]]) </span><br><span class="line">        yy2 = np.minimum(y2[i], y2[order[<span class="number">1</span>:]]) </span><br><span class="line">        <span class="comment">#计算相交的面积,不重叠时面积为0 </span></span><br><span class="line">        w = np.maximum(<span class="number">0.0</span>, xx2 - xx1 + <span class="number">1</span>) </span><br><span class="line">        h = np.maximum(<span class="number">0.0</span>, yy2 - yy1 + <span class="number">1</span>) </span><br><span class="line">        inter = w * h </span><br><span class="line">        <span class="comment">#计算IoU：重叠面积 /（面积1+面积2-重叠面积） </span></span><br><span class="line">        ovr = inter / (areas[i] + areas[order[<span class="number">1</span>:]] - inter) </span><br><span class="line">       <span class="comment">#保留IoU小于阈值的box </span></span><br><span class="line">        inds = np.where(ovr &lt;= thresh)[<span class="number">0</span>] </span><br><span class="line">        order = order[inds + <span class="number">1</span>] <span class="comment">#因为ovr数组的长度比order数组少一个,所以这里要将所有下标后移一位 </span></span><br><span class="line">    <span class="keyword">return</span> keep</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># NMS算法</span></span><br><span class="line"><span class="comment"># bboxes维度为[N,4]，scores维度为[N,], 均为tensor</span></span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">nms</span>(<span class="params">self, bboxes, scores, threshold=<span class="number">0.5</span></span>):</span></span><br><span class="line">        x1 = bboxes[:,<span class="number">0</span>]</span><br><span class="line">        y1 = bboxes[:,<span class="number">1</span>]</span><br><span class="line">        x2 = bboxes[:,<span class="number">2</span>]</span><br><span class="line">        y2 = bboxes[:,<span class="number">3</span>]</span><br><span class="line">        areas = (x2-x1)*(y2-y1)   <span class="comment"># [N,] 每个bbox的面积</span></span><br><span class="line">        _, order = scores.sort(<span class="number">0</span>, descending=<span class="literal">True</span>)    <span class="comment"># 降序排列</span></span><br><span class="line">        keep = []</span><br><span class="line">        <span class="keyword">while</span> order.numel() &gt; <span class="number">0</span>:       <span class="comment"># torch.numel()返回张量元素个数</span></span><br><span class="line">            <span class="keyword">if</span> order.numel() == <span class="number">1</span>:     <span class="comment"># 保留框只剩一个</span></span><br><span class="line">                i = order.item()</span><br><span class="line">                keep.append(i)</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                i = order[<span class="number">0</span>].item()    <span class="comment"># 保留scores最大的那个框box[i] tensor元素取出作为numpy数字</span></span><br><span class="line">                keep.append(i)</span><br><span class="line">            <span class="comment"># 计算box[i]与其余各框的IOU(思路很好)</span></span><br><span class="line">            <span class="comment"># torch.clamp(min, max) 设置上下限</span></span><br><span class="line">            xx1 = x1[order[<span class="number">1</span>:]].clamp(<span class="built_in">min</span>=x1[i])   <span class="comment"># [N-1,] </span></span><br><span class="line">            yy1 = y1[order[<span class="number">1</span>:]].clamp(<span class="built_in">min</span>=y1[i])</span><br><span class="line">            xx2 = x2[order[<span class="number">1</span>:]].clamp(<span class="built_in">max</span>=x2[i]) <span class="comment">#</span></span><br><span class="line">            yy2 = y2[order[<span class="number">1</span>:]].clamp(<span class="built_in">max</span>=y2[i])</span><br><span class="line">            inter = (xx2-xx1).clamp(<span class="built_in">min</span>=<span class="number">0</span>) * (yy2-yy1).clamp(<span class="built_in">min</span>=<span class="number">0</span>)   <span class="comment"># [N-1,]</span></span><br><span class="line">            iou = inter / (areas[i]+areas[order[<span class="number">1</span>:]]-inter)  <span class="comment"># [N-1,]</span></span><br><span class="line">            idx = (iou &lt;= threshold).nonzero().squeeze() <span class="comment"># 注意此时idx为[N-1,] 而order为[N,]</span></span><br><span class="line"> <span class="keyword">if</span> idx.numel() == <span class="number">0</span>:</span><br><span class="line"> <span class="keyword">break</span></span><br><span class="line">            order = order[idx+<span class="number">1</span>]  <span class="comment"># 修补索引之间的差值</span></span><br><span class="line"> <span class="keyword">return</span> torch.LongTensor(keep)   <span class="comment"># Pytorch的索引值为LongTensor</span></span><br></pre></td></tr></table></figure>



<h1 id="soft-NMS"><a href="#soft-NMS" class="headerlink" title="soft-NMS"></a>soft-NMS</h1><p>NMS 算法存在以下问题：</p>
<ol>
<li><p>有可能会出现<strong>边界框更准确但是置信度较低的预测框</strong>，我们依靠置信度来决定谁是最佳预测框并不十分准确。</p>
</li>
<li><p>假设有两个目标紧在一起，使用NMS算法会造成剔除其中一个目标的预测框</p>
</li>
</ol>
<p>针对NMS算法简单粗暴删除IoU大于给定阈值的候选框的问题，Soft-NMS 并非直接删除这些具有较高IoU的重叠区域，而是降低这些具有较高IoU的候选框的置信度; 最后, 通过与置信度阈值比较筛选出最佳的预测框。Soft-NMS 算法对密集目标的检测具有一定的提升作用。</p>
<p>Soft-NMS 算法的置信度重置算法有两种方式，一种是采样线性加权，另一种是高斯加权：IoU越大（即，重叠的越多），置信度降低的越强烈</p>
<p><strong>NMS 和 soft-NMS 都丢弃了冗余框，因此无法有效地从不同模型中产生平均定位预测</strong>。</p>
<h1 id="WBF-加权框融合"><a href="#WBF-加权框融合" class="headerlink" title="WBF 加权框融合"></a>WBF 加权框融合</h1><p>WBF采用的融合框的置信度为所有形成他的边界框的平均置信度，而其坐标也是所有形成他的边界框的坐标的加权和，权重为相应的边界框的置信度。也就是置信度越高的边界框对于融合边界框的坐标做出的贡献越大。</p>
<p>NMS和soft-NMS都排除了一些框，但是<strong>Weighted Boxes Fusion</strong> WBF利用了所有框的信息。它可以解决某些情况下，模型预测的框不准确。NMS将只能留下一个不准确的框，而WBF将使用多个框的信息来修复它。</p>
<p>假设，我们对来自 N 个不同模型的同一图像进行了边界框预测。或者，我们对同一图像的原始版本和增强版本（即垂直/水平反射）有 N 个相同模型的预测。WBF 算法的工作步骤如下：</p>
<ol>
<li><p>每个模型的每个预测框都添加到单个列表 B 中。该列表按置信度 C 的降序排序。</p>
</li>
<li><p>列表 L是一个框簇储存的是一个个bounding box集合，列表F储存的是融合后的框。列表L中的每个位置可以包含一组框（或单个框），形成一个簇；F 中的每个位置只包含一个框，它是来自 L 中相应簇的融合框。生成融合框的方程将在后面讨论。</p>
</li>
<li><p>循环遍历B中的预测框，尝试在列表F中找到匹配框。匹配定义为与问题框重叠较大的框（IoU &gt; THR）。原论文THR是0.55</p>
</li>
<li><p>如果没有找到匹配项，则将列表 B 中的框添加到列表 L 和 F 的末尾作为新条目；继续到列表 B 中的下一个框。</p>
</li>
<li><p>如果找到匹配，则将此框添加到列表L中与列表F中匹配框对应的位置pos处。</p>
</li>
<li><p>重新计算F[pos]中的box坐标和confidence score，使用集群L[pos]中累积的所有T box，其中 score和坐标的计算方式如下, score计算是取算数平均值得到的，而坐标值是通过框的置信度score 和坐标值相乘然后累加再除以 score的累加值得到，这样做可以使得具有较大置信度的框比较小置信度的框对融合后坐标的贡献值更大。</p>
<p><img src="/2021/10/11/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B8%AD%E5%80%99%E9%80%89%E8%BE%B9%E7%95%8C%E6%A1%86%E7%9A%84%E5%A4%84%E7%90%86-NMS-soft-NMS-WBF/WBF%E8%AE%A1%E7%AE%97.png"></p>
<p>7、当B中所有的框都循环完后，对于F中每个框的re-scale进行 ，原因是因为如果一个cluster中的框的数量太少的话，可能意味着若干模型中只有很少的模型预测到了这个框，因此是需要减少这种情况下对应框的置信度，作者给出了两种如下re-scale的方式。作者在原论文中指出，这两种方式没有显著的差异，第一种方式的性能会略好一点点。</p>
<p><img src="/2021/10/11/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B8%AD%E5%80%99%E9%80%89%E8%BE%B9%E7%95%8C%E6%A1%86%E7%9A%84%E5%A4%84%E7%90%86-NMS-soft-NMS-WBF/re-scale.png"></p>
</li>
</ol>
]]></content>
      <tags>
        <tag>目标检测</tag>
        <tag>候选框</tag>
        <tag>NMS</tag>
        <tag>WBF</tag>
      </tags>
  </entry>
  <entry>
    <title>神经网络-learning rate学习率的优化总结</title>
    <url>/2021/10/01/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-learning-rate%E5%AD%A6%E4%B9%A0%E7%8E%87%E7%9A%84%E4%BC%98%E5%8C%96%E6%80%BB%E7%BB%93/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>背景问题：</p>
<p>方法简述：</p>
<p>优点：</p>
<p>缺点：</p>
<span id="more"></span>

<p>学习率是一个非常非常重要的超参数，在 <strong>梯度下降方法</strong> 中，学习率的取值非常关键，如果过大就不会收敛，如果过小则收敛速度太慢。面对不同规模、不同batch-size、不同优化方式、不同数据集，其最合适的值都是不确定的，无法凭借经验来准确地确定<code>lr</code>的值，就是在训练中不断寻找最合适当前状态的学习率。</p>
<p>可以使用的方法：利用fastai中的lr_find()函数寻找合适的学习率，根据学习率-损失曲线得到合适的学习率。</p>
<h1 id="learning-rate与batch-size的关系"><a href="#learning-rate与batch-size的关系" class="headerlink" title="learning-rate与batch-size的关系"></a>learning-rate与batch-size的关系</h1><p>一般来说，越大的batch-size使用越大的学习率。原理很简单，越大的<code>batch-size</code>意味着我们学习的时候，收敛方向的<code>confidence</code>越大，我们前进的方向更加坚定，而小的<code>batch-size</code>则显得比较杂乱，毫无规律性，因为相比批次大的时候，批次小的情况下无法照顾到更多的情况，所以需要小的学习率来保证不至于出错。可以看下图<code>损失Loss</code>与<code>学习率Lr</code>的关系：</p>
<p><img src="/2021/10/01/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-learning-rate%E5%AD%A6%E4%B9%A0%E7%8E%87%E7%9A%84%E4%BC%98%E5%8C%96%E6%80%BB%E7%BB%93/lr_batchsize.png"></p>
<p>在显存足够的条件下，最好采用较大的batch-size进行训练，找到合适的学习率后，可以加快收敛速度。</p>
<p>另外，较大的batch-size可以避免batch normalization出现的一些小问题，参考如下Pytorch库Issue[3]</p>
<h1 id="学习率衰减-学习率退火"><a href="#学习率衰减-学习率退火" class="headerlink" title="学习率衰减 / 学习率退火"></a>学习率衰减 / 学习率退火</h1><p>从经验上看，<strong>学习率在一开始要保持大些来保证收敛速度，在收敛到最优点附近时要小些以避免来回震荡。</strong>比较简单的学习率调整可以通过 <strong>学习率衰减（Learning Rate Decay）</strong>的方式来实现，也称为 <strong>学习率退火（Learning Rate Annealing）</strong>。</p>
<p>假设初始化学习率为lr0，在第t次迭代时的学习率lrt。常用的衰减方式为可以设置为 <strong>按迭代次数</strong> 进行衰减。</p>
<p>常见的衰减方法有以下几种：</p>
<h5 id="分段常数衰减（Piecewise-Constant-Decay）"><a href="#分段常数衰减（Piecewise-Constant-Decay）" class="headerlink" title="分段常数衰减（Piecewise Constant Decay）"></a>分段常数衰减（Piecewise Constant Decay）</h5><p>即每经过<img src="https://math.jianshu.com/math?formula=T_1,%20T_2,%20%C2%B7%20%C2%B7%20%C2%B7%20,%20T_m" alt="T_1, T_2, · · · , T_m">次迭<br>代将学习率衰减为原来的<img src="https://math.jianshu.com/math?formula=%CE%B2_1,%20%CE%B2_2,%20%C2%B7%20%C2%B7%20%C2%B7%20,%20%CE%B2_m" alt="β_1, β_2, · · · , β_m">倍，其中**<img src="https://math.jianshu.com/math?formula=T_m" alt="T_m">** 和 **<img src="https://math.jianshu.com/math?formula=%CE%B2_m%3C1" alt="β_m&lt;1">**为根据经验设置的 <strong>超参数</strong>。</p>
<h5 id="逆时衰减（Inverse-Time-Decay）"><a href="#逆时衰减（Inverse-Time-Decay）" class="headerlink" title="逆时衰减（Inverse Time Decay）"></a>逆时衰减（Inverse Time Decay）</h5><p>lr-t = lr0 /(1+b*t)  b为衰减率</p>
<h5 id="指数衰减（Exponential-Decay）"><a href="#指数衰减（Exponential-Decay）" class="headerlink" title="指数衰减（Exponential Decay）"></a>指数衰减（Exponential Decay）</h5><p>Lr-t = lr0*(b**t)</p>
<h5 id="自然指数衰减（Natural-Exponential-Decay）"><a href="#自然指数衰减（Natural-Exponential-Decay）" class="headerlink" title="自然指数衰减（Natural Exponential Decay）"></a>自然指数衰减（Natural Exponential Decay）</h5><h5 id="余弦衰减（Cosine-Decay）"><a href="#余弦衰减（Cosine-Decay）" class="headerlink" title="余弦衰减（Cosine Decay）"></a>余弦衰减（Cosine Decay）</h5><p><img src="/2021/10/01/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-learning-rate%E5%AD%A6%E4%B9%A0%E7%8E%87%E7%9A%84%E4%BC%98%E5%8C%96%E6%80%BB%E7%BB%93/%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%A1%B0%E5%87%8F.png"></p>
<h1 id="学习率预热"><a href="#学习率预热" class="headerlink" title="学习率预热"></a>学习率预热</h1><p>在 <strong>小批量梯度下降方法</strong> 中，当批量大小的设置比较大时，通常需要比较大的学习率。但在刚开始训练时，由于参数是随机初始化的，<strong>梯度往往也比较大</strong>，在加上 <strong>比较大的初始学习率，会使得训练不稳定。</strong></p>
<p>为了提高训练稳定性，我们可以 <strong>在最初几轮迭代时，采用比较小的学习率，等梯度下降到一定程度后再恢复到初始的学习率，</strong>这种方法称为 <strong>学习率预热（Learning Rate Warmup）。</strong></p>
<p>一个常用的学习率预热方法是 <strong>逐渐预热（Gradual Warmup）</strong>。<strong>当预热过程结束，再选择一种学习率衰减方法来逐渐降低学习率。</strong></p>
<h1 id="周期性学习率调整"><a href="#周期性学习率调整" class="headerlink" title="周期性学习率调整"></a>周期性学习率调整</h1><p>为了使得 <strong>梯度下降方法</strong> 能够 <strong>逃离局部最小值或鞍点</strong>，一种经验性的方式是在训练过程中 <strong>周期性地增大学习率</strong>。虽然增加学习率可能短期内有损网络的收敛稳定性，但从长期来看有助于找到更好的局部最优解。</p>
<p>一般而言，当一个模型 <strong>收敛到一个平坦（Flat）的局部最小值</strong> 时，其 <strong>鲁棒性会更好</strong>，即 <strong>微小的参数变动不会剧烈影响模型能力</strong>；而当模型 <strong>收敛到一个尖锐（Sharp）的局部最小值</strong> 时，其 <strong>鲁棒性也会比较差</strong>。具备良好泛化能力的模型通常应该是鲁棒的，因此理想的局部最小值应该是平坦的。</p>
<p><strong>周期性学习率调整</strong> 可以使得梯度下降方法在优化过程中 <strong>跳出尖锐的局部极小值</strong>，虽然会短期内会损害优化过程，但最终会收敛到更加理想的局部极小值。</p>
<h5 id="循环学习率"><a href="#循环学习率" class="headerlink" title="循环学习率"></a>循环学习率</h5><p>一种简单的方法是使用循环学习率，即在让学习率在 <strong>一个区间内</strong> <strong>周期性地增大和缩小</strong>。</p>
<p>通常可以使用 <strong>线性缩放</strong> 来调整学习率，称为 <strong>三角循环学习率（Triangular Cyclic Learning Rate）。</strong></p>
<p><img src="/2021/10/01/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-learning-rate%E5%AD%A6%E4%B9%A0%E7%8E%87%E7%9A%84%E4%BC%98%E5%8C%96%E6%80%BB%E7%BB%93/%E5%BE%AA%E7%8E%AF%E5%AD%A6%E4%B9%A0%E7%8E%87.png"></p>
<h5 id="带热重启的随机梯度下降"><a href="#带热重启的随机梯度下降" class="headerlink" title="带热重启的随机梯度下降"></a>带热重启的随机梯度下降</h5><p>带热重启的随机梯度下降（Stochastic Gradient Descent with Warm Restarts，SGDR）是用 <strong>热重启方式</strong> 来替代 <strong>学习率衰减</strong> 的方法。</p>
<p><strong>学习率每间隔一定周期后，重新初始化为某个预先设定值，然后逐渐衰减。</strong>每次重启后模型参数不是从头开始优化，而是从重启前的参数基础上继续优化。</p>
<p>!</p>
]]></content>
      <categories>
        <category>学习</category>
        <category>深度学习</category>
        <category>神经网络</category>
      </categories>
      <tags>
        <tag>总结</tag>
        <tag>编程</tag>
        <tag>优化</tag>
        <tag>调参</tag>
      </tags>
  </entry>
  <entry>
    <title>神经网络-损失函数</title>
    <url>/2021/10/01/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>背景问题：损失函数用来评价模型的预测值和真实值不一样的程度，损失函数越好，通常模型的性能越好。不同的模型用的损失函数一般也不一样。</p>
<p>方法简述：</p>
<ol>
<li>均方差损失 Mean Squared Error Loss</li>
<li>平均绝对误差损失 Mean Absolute Error Loss</li>
<li>Huber Loss</li>
<li>分位数损失 Quantile Loss</li>
<li>交叉熵损失 Cross Entropy Loss</li>
<li>合页损失 Hinge Loss</li>
</ol>
<p>适用于回归的均方差损失 Mean Squared Loss、平均绝对误差损失 Mean Absolute Error Loss，两者的区别以及两者相结合得到的 Huber Loss，接着是应用于分位数回归的分位数损失 Quantile Loss，表明了平均绝对误差损失实际上是分位数损失的一种特例，在分类场景下，本文讨论了最常用的交叉熵损失函数 Cross Entropy Loss，包括二分类和多分类下的形式，并从信息论的角度解释了交叉熵损失函数，最后简单介绍了应用于 SVM 中的 Hinge 损失 Hinge Loss。</p>
<span id="more"></span>



<p>损失函数分为经验风险损失函数和结构风险损失函数。经验风险损失函数指预测结果和实际结果的差别，结构风险损失函数是指经验风险损失函数加上正则项。</p>
<p>Loss Function、Cost Function 和 Objective Function 的区别和联系。在机器学习的语境下这三个术语经常被交叉使用。</p>
<ul>
<li>- 损失函数 Loss Function 通常是<strong>针对单个训练样本而言</strong>，给定一个模型输出 和一个真实 ，损失函数输出一个实值损失 </li>
<li>- 代价函数 Cost Function 通常是<strong>针对整个训练集</strong>（或者在使用 mini-batch gradient descent 时一个 mini-batch）的总损失 </li>
<li>- 目标函数 Objective Function 是一个更通用的术语，表示任意希望被优化的函数，用于机器学习领域和非机器学习领域（比如运筹优化）</li>
</ul>
<p>一句话总结三者的关系就是：A loss function is a part of a cost function which is a type of an objective function.</p>
<h1 id="L2：均方差损失-Mean-Squared-Error-Loss"><a href="#L2：均方差损失-Mean-Squared-Error-Loss" class="headerlink" title="L2：均方差损失 Mean Squared Error Loss"></a>L2：均方差损失 Mean Squared Error Loss</h1><p>损失是机器学习、深度学习回归任务中最常用的一种损失函数，也称为 L2 Loss。</p>
<p>从直觉上理解均方差损失，这个损失函数的最小值为 0（当预测等于真实值时），最大值为无穷大。下图是对于真实值 ，不同的预测值 的均方差损失的变化图。横轴是不同的预测值，纵轴是均方差损失，可以看到随着预测与真实值绝对误差 的增加，均方差损失呈二次方地增加。</p>
<p><strong>在模型输出与真实值的误差服从高斯分布的假设下，最小化均方差损失函数与极大似然估计本质上是一致的</strong>，因此在这个假设能被满足的场景中（比如回归），均方差损失是一个很好的损失函数选择；当这个假设没能被满足的场景中（比如分类），均方差损失不是一个好的选择。</p>
<h1 id="L1：平均绝对误差损失-Mean-Absolute-Error-Loss"><a href="#L1：平均绝对误差损失-Mean-Absolute-Error-Loss" class="headerlink" title="L1：平均绝对误差损失 Mean Absolute Error Loss"></a>L1：平均绝对误差损失 Mean Absolute Error Loss</h1><p>平均绝对误差 Mean Absolute Error (MAE) 是另一类常用的损失函数，也称为 L1 Loss。MAE 损失的最小值为 0（当预测等于真实值时），最大值为无穷大。可以看到随着预测与真实值绝对误差 的增加，MAE 损失呈线性增长。假设模型预测与真实值之间的误差服从拉普拉斯分布 Laplace distribution</p>
<p>MAE 和 MSE 作为损失函数的主要区别是：</p>
<p>MSE 损失相比 MAE 通常可以更快地收敛，但 MAE 损失对于 outlier 更加健壮，即更加不易受到 outlier 影响。</p>
<h1 id="Huber-Loss"><a href="#Huber-Loss" class="headerlink" title="Huber Loss"></a>Huber Loss</h1><p>MSE 损失收敛快但容易受 outlier 影响，MAE 对 outlier 更加健壮但是收敛慢，Huber Loss 则是一种将 MSE 与 MAE 结合起来，取两者优点的损失函数，也被称作 Smooth Mean Absolute Error Loss 。</p>
<p>Huber Loss 结合了 MSE 和 MAE 损失，在误差接近 0 时使用 MSE，使损失函数可导并且梯度更加稳定；在误差较大时使用 MAE 可以降低 outlier 的影响，使训练对 outlier 更加健壮。缺点是需要额外地设置一个 超参数。</p>
<h1 id="分位数损失-Quantile-Loss"><a href="#分位数损失-Quantile-Loss" class="headerlink" title="分位数损失 Quantile Loss"></a>分位数损失 Quantile Loss</h1><p>通常的回归算法是拟合目标值的期望或者中位数，而分位数回归可以通过给定不同的分位点，拟合目标值的不同分位数。例如我们可以分别拟合出多个分位点，得到一个置信区间，</p>
<p>这个损失函数是一个分段的函数 ，将 （高估） 和 （低估） 两种情况分开来，并分别给予不同的系数。当 时，低估的损失要比高估的损失更大，反过来当 时，高估的损失比低估的损失大；分位数损失实现了<strong>分别用不同的系数控制高估和低估的损失，进而实现分位数回归</strong>。特别地，当 时，分位数损失退化为 MAE 损失，从这里可以看出 MAE 损失实际上是分位数损失的一个特例</p>
<h1 id="交叉熵损失-Cross-Entropy-Loss"><a href="#交叉熵损失-Cross-Entropy-Loss" class="headerlink" title="交叉熵损失 Cross Entropy Loss"></a>交叉熵损失 Cross Entropy Loss</h1><p>上文介绍的几种损失函数都是适用于回归问题损失函数，对于分类问题，最常用的损失函数是交叉熵损失函数 Cross Entropy Loss。</p>
<h3 id="二分类"><a href="#二分类" class="headerlink" title="二分类"></a>二分类</h3><p>考虑二分类，在二分类中我们通常使用 Sigmoid 函数将模型的输出压缩到 (0, 1) 区间内 ，用来代表给定输入 ，模型判断为正类的概率。由于只有正负两类，因此同时也得到了负类的概率。</p>
<p>二分类的交叉熵损失越接近目标值损失越小，随着误差变差，损失呈指数增长。</p>
<h3 id="多分类"><a href="#多分类" class="headerlink" title="多分类"></a>多分类</h3><p>在多分类的任务中，交叉熵损失函数的推导思路和二分类是一样的，变化的地方是真实值 现在是一个 One-hot 向量，同时模型输出的压缩由原来的 Sigmoid 函数换成 Softmax 函数。Softmax 函数将每个维度的输出范围都限定在 之间，同时所有维度的输出和为 1，</p>
<p>#合页损失 Hinge Loss</p>
<p>合页损失 Hinge Loss 是另外一种二分类损失函数，适用于 maximum-margin 的分类，支持向量机 Support Vector Machine (SVM) 模型的损失函数本质上就是 Hinge Loss + L2 正则化。</p>
]]></content>
      <categories>
        <category>学习</category>
        <category>Pytorch</category>
        <category>深度学习</category>
        <category>神经网络</category>
      </categories>
      <tags>
        <tag>总结</tag>
        <tag>编程</tag>
        <tag>优化</tag>
        <tag>调参</tag>
      </tags>
  </entry>
  <entry>
    <title>神经网络-权值初始化的方法</title>
    <url>/2021/10/01/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E6%9D%83%E5%80%BC%E5%88%9D%E5%A7%8B%E5%8C%96%E7%9A%84%E6%96%B9%E6%B3%95/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>权值初始化的原因：1. 防止层激活输出在深度神经网络的正向传递过程中爆炸或消失。2. 有助于加速收敛。</p>
<p>权值初始化的方法：</p>
<p>常量初始化（constant）、高斯分布初始化（gaussian）、positive_unitball初始化、均匀分布初始化（uniform）、xavier初始化、msra初始化、双线性初始化（bilinear）</p>
<span id="more"></span>

<p>权重初始化相比于其他的trick来说在平常使用并不是很频繁。因为大部分人使用的模型都是预训练模型，使用的权重都是在大型数据集上训练好的模型，当然不需要自己去初始化权重了。只有没有预训练模型的领域会自己初始化权重，或者在模型中去初始化神经网络最后那几个全连接层的权重。常用的权重初始化算法是 :</p>
<h1 id="常量初始化"><a href="#常量初始化" class="headerlink" title="常量初始化"></a>常量初始化</h1><h1 id="均匀分布初始化"><a href="#均匀分布初始化" class="headerlink" title="均匀分布初始化"></a>均匀分布初始化</h1><h1 id="高斯分布初始化"><a href="#高斯分布初始化" class="headerlink" title="高斯分布初始化"></a>高斯分布初始化</h1><ul>
<li>normal高斯分布初始化， 其中stdev为高斯分布的标准差，均值设为0：</li>
</ul>
<h1 id="Xavier初始化"><a href="#Xavier初始化" class="headerlink" title="Xavier初始化"></a>Xavier初始化</h1><ul>
<li>Xavier初始法，适用于普通激活函数(tanh, sigmoid)：</li>
</ul>
<p>Xavier初始化将每层权重设置为在有界的随机均匀分布中选择的值</p>
<p><img src="/2021/10/01/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E6%9D%83%E5%80%BC%E5%88%9D%E5%A7%8B%E5%8C%96%E7%9A%84%E6%96%B9%E6%B3%95/Xavier.png"></p>
<h1 id="MSRA初始化"><a href="#MSRA初始化" class="headerlink" title="MSRA初始化"></a>MSRA初始化</h1><h1 id="双线性初始化"><a href="#双线性初始化" class="headerlink" title="双线性初始化"></a>双线性初始化</h1><h1 id="Kaiming初始化"><a href="#Kaiming初始化" class="headerlink" title="Kaiming初始化"></a>Kaiming初始化</h1><p>保持层激活的标准偏差大约为1将允许我们在深度神经网络中堆叠更多层而不会出现梯度爆炸或消失。</p>
<p>关于探索如何在类ReLU的激活的网络中最佳地初始化权重促使何凯明等优秀学者提出自己的初始化方案，这些方案是专门用来处理这些非对称，非线性激活的深层神经网络的。</p>
<p>在他们的2015年论文中何凯明等人证明了如果采用以下输入权重初始化策略，深层网络（例如22层CNN）会更早收敛：</p>
<p>\1. 使用适合给定图层的权重矩阵创建张量，并使用从标准正态分布中随机选择的数字填充它。</p>
<p>\2. 将每个随机选择的数字乘以√2/√n，其中n是从前一层输出到指定层的连接数（也称为“fan-in”）。</p>
<p>\3. 偏差张量初始化为零。</p>
]]></content>
  </entry>
  <entry>
    <title>神经网络-梯度优化方法</title>
    <url>/2021/10/01/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E6%A2%AF%E5%BA%A6%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>背景问题：</p>
<p>方法简述：</p>
<p>深度学习优化算法经历了 SGD -&gt; SGDM -&gt; NAG -&gt;AdaGrad -&gt; AdaDelta -&gt; Adam -&gt; Nadam 这样的发展历程。</p>
<span id="more"></span>



<p>深度学习优化算法经历了 SGD -&gt; SGDM -&gt; NAG -&gt;AdaGrad -&gt; AdaDelta -&gt; Adam -&gt; Nadam 这样的发展历程。</p>
<h1 id="优化算法通用框架"><a href="#优化算法通用框架" class="headerlink" title="优化算法通用框架"></a>优化算法通用框架</h1><p>首先定义：待优化参数：w ，目标函数： f(w)，初始学习率 α。</p>
<p>而后，开始进行迭代优化。在每个epoch t：</p>
<ol>
<li><p>计算目标函数关于当前参数的梯度： </p>
</li>
<li><p>根据历史梯度计算一阶动量和二阶动量：</p>
</li>
<li><p>计算当前时刻的下降梯度</p>
</li>
<li><p>根据下降梯度进行更新</p>
<p><img src="/2021/10/01/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E6%A2%AF%E5%BA%A6%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95/%E9%80%9A%E7%94%A8%E6%A1%86%E6%9E%B6.png"></p>
</li>
</ol>
<p>一般而言，一个物体的动量指的的是这个物体在它运动方向上保持运动的趋势。动量是矢量这里我们可以把梯度理解成力，力是有大小和方向的，而且力可以改变速度的大小和方向，并且速度可以累积。这里把权值理解成速度，当力（梯度）改变时就会有一段逐渐加速或逐渐减速的过程，我们通过引入动量就可以加速我们的学习过程，可以在鞍点处继续前行，也可以逃离一些较小的局部最优区域下面类比物理学定义这里的动量</p>
<p><strong>一阶动量</strong>是各个时刻梯度方向的指数移动平均值，约等于最近 个时刻的梯度向量和的平均值（移动平均是啥看最上面的文章）。也就是说， 时刻的下降方向，不仅由当前点的梯度方向决定，而且由此前累积的下降方向决定。 的经验值为0.9，这就意味着下降方向主要是此前累积的下降方向，并略微偏向当前时刻的下降方向。在梯度方向改变时，momentum能够降低参数更新速度，从而减少震荡，在梯度方向相同时，momentum可以加速参数更新， 从而加速收敛，</p>
<p><strong>二阶动量</strong>用来度量历史更新频率的，二阶动量是迄今为止所有梯度值的平方和，即 ，在最上面的框架中 （在这里 ）， 也就是说，我们的学习率现在是 （一般为了避免分母为0，会在分母上加一个小的平滑项 ），从这里我们就会发现 是恒大于0的，而且参数更新越频繁，二阶动量越大，学习率就越小，这一方法在稀疏数据场景下表现非常好</p>
<p>reference:<a href="https://www.163.com/dy/article/GKBN2P0H0519EA27.html">https://www.163.com/dy/article/GKBN2P0H0519EA27.html</a></p>
<h1 id="SGD"><a href="#SGD" class="headerlink" title="SGD"></a>SGD</h1><p><strong>与SGD相关的都是固定学习率的优化算法</strong></p>
<p><img src="/2021/10/01/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E6%A2%AF%E5%BA%A6%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95/SGD.PNG"></p>
<p>SGD的优点：</p>
<ol>
<li><p>虽然看起来SGD波动非常大，会走很多弯路，但是对梯度的要求很低（计算梯度快），而且对于引入噪声，大量的理论和实践工作证明，只要噪声不是特别大，SGD都能很好地收敛。</p>
</li>
<li><p>应用大型数据集时，训练速度很快。比如每次从百万数据样本中，取几百个数据点，算一个SGD梯度，更新一下模型参数。相比于标准梯度下降法的遍历全部样本，每输入一个样本更新一次参数，要快得多。</p>
</li>
</ol>
<p>SGD的缺点：</p>
<ol>
<li>SGD也没能单独克服局部最优解的问题（主要）。</li>
<li>SGD在随机选择梯度的同时会引入噪声，使得权值更新的方向不一定正确（次要）。</li>
</ol>
<h1 id="SGD-with-Momentum"><a href="#SGD-with-Momentum" class="headerlink" title="SGD with Momentum"></a>SGD with Momentum</h1><p>为了抑制SGD的震荡，SGDM认为梯度下降过程可以加入惯性。下坡的时候，如果发现是陡坡，那就利用惯性跑的快一些。SGDM全称是SGD with momentum，在SGD基础上引入了一阶动量：</p>
<p><img src="/2021/10/01/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E6%A2%AF%E5%BA%A6%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95/SGDM.PNG"></p>
<p>一阶动量是各个时刻梯度方向的指数移动平均值，约等于最近 1/(1-β1) 个时刻的梯度向量和的平均值。</p>
<p>也就是说，t 时刻的下降方向，不仅由当前点的梯度方向决定，而且由<strong>此前累积的下降方向</strong>决定。β1的经验值为0.9，这就意味着下降方向主要是此前累积的下降方向，并略微偏向当前时刻的下降方向。就如高速公路上汽车转弯，在高速向前的同时略微偏向，不会急转弯。</p>
<h1 id="SGD-with-Nesterov-Acceleration-（NAG）"><a href="#SGD-with-Nesterov-Acceleration-（NAG）" class="headerlink" title="SGD with Nesterov Acceleration （NAG）"></a>SGD with Nesterov Acceleration （NAG）</h1><p>SGD 还有一个问题是困在局部最优的沟壑里面震荡。比如走到一个盆地，四周都是略高的小山，没有下坡的方向，那就只能待在这里了。可是如果爬上高地，就会发现外面的世界还很广阔。因此，不能停留在当前位置去观察未来的方向，而要向前一步、多看一步、看远一些。</p>
<p><img src="/2021/10/01/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E6%A2%AF%E5%BA%A6%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95/NAG.PNG"></p>
<p>SGD及其变种以同样的学习率更新每个参数，但深度神经网络往往包含大量的参数，这些参数并不是总会用得到（想想大规模的embedding）。对于经常更新的参数，我们已经积累了大量关于它的知识，不希望被单个样本影响太大，希望学习速率慢一些；对于偶尔更新的参数，我们了解的信息太少，希望能从每个偶然出现的样本身上多学一些，即学习速率大一些。</p>
<h1 id="AdaGrad（自适应梯度算法）"><a href="#AdaGrad（自适应梯度算法）" class="headerlink" title="AdaGrad（自适应梯度算法）"></a>AdaGrad（自适应梯度算法）</h1><p>前面的随机梯度和动量随机梯度算法都是使用全局的学习率，所有的参数都是统一步伐的进行更新的，如果可以针对每个参数设置学习率可能会更好，让他根据情况进行调整，也就是自适应梯度下降。怎么样去度量历史更新频率呢？那就是二阶动量——该维度上，迄今为止所有梯度值的平方和</p>
<p><img src="/2021/10/01/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E6%A2%AF%E5%BA%A6%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95/AdaGrad.PNG"></p>
<p>缺点：</p>
<p>仍需要手工设置一个全局学习率 , 如果 设置过大的话，会使regularizer过于敏感，对梯度的调节太大</p>
<p>中后期，分母上梯度累加的平方和会越来越大，使得参数更新量趋近于0，使得训练提前结束，无法学习</p>
<h1 id="AdaDelta-RMSProp"><a href="#AdaDelta-RMSProp" class="headerlink" title="AdaDelta/RMSProp"></a>AdaDelta/RMSProp</h1><p>由于AdaGrad单调递减的学习率变化过于激进，我们考虑一个改变二阶动量计算方法的策略：不累积全部历史梯度，而只关注过去一段时间窗口的下降梯度。这也就是AdaDelta名称中Delta的来历。</p>
<p>修改的思路很简单。前面我们讲到，指数移动平均值大约就是过去一段时间的平均值，因此我们用这一方法来计算二阶累积动量：</p>
<p><img src="/2021/10/01/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E6%A2%AF%E5%BA%A6%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95/AdaDelta.PNG"></p>
<p>就避免了二阶动量持续累积、导致训练过程提前结束的问题了。</p>
<p>特点：</p>
<p>​    训练初中期，加速效果不错，很快</p>
<p>​    训练后期，反复在局部最小值附近抖动</p>
<h1 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h1><p>我们看到，SGD-M在SGD基础上增加了一阶动量，AdaGrad和AdaDelta在SGD基础上增加了二阶动量。把一阶动量和二阶动量都用起来，就是Adam了——Adaptive + Momentum。</p>
<p><img src="/2021/10/01/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E6%A2%AF%E5%BA%A6%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95/Adam.PNG"></p>
<p>特点如下：</p>
<ol>
<li>Adam梯度经过偏置校正后，每一次迭代学习率都有一个固定范围，使得参数比较平稳。</li>
<li>结合了Adagrad善于处理稀疏梯度和RMSprop善于处理非平稳目标的优点</li>
<li>为不同的参数计算不同的自适应学习率</li>
<li>也适用于大多非凸优化问题——适用于大数据集和高维空间。</li>
</ol>
<h1 id="Nadam"><a href="#Nadam" class="headerlink" title="Nadam"></a>Nadam</h1><p>Nesterov + Adam是集大成者，将Adam中的第一步替换为Nesterov</p>
<p><img src="/2021/10/01/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E6%A2%AF%E5%BA%A6%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95/NAdam.PNG"></p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p><strong>Adam可能的问题1:</strong></p>
<p>SGD没有用到二阶动量，因此学习率是恒定的（实际使用过程中会采用学习率衰减策略，因此学习率递减）。AdaGrad的二阶动量不断累积，单调递增，因此学习率是单调递减的。因此，这两类算法会使得学习率不断递减，最终收敛到0，模型也得以收敛。</p>
<p><img src="/2021/10/01/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E6%A2%AF%E5%BA%A6%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95/Adam%E4%BF%AE%E6%AD%A3.PNG"></p>
<p><strong>Adam可能的问题2:</strong></p>
<p><strong>后期Adam的学习率太低，影响了有效的收敛</strong>。</p>
<p>于是提出了一个用来改进Adam的方法：<strong>前期用Adam，享受Adam快速收敛的优势；后期切换到SGD，慢慢寻找最优解。</strong>这一方法以前也被研究者们用到，不过主要是根据经验来选择切换的时机和切换后的学习率。这篇文章把这一切换过程傻瓜化，给出了切换SGD的时机选择方法，以及学习率的计算方法，效果看起来也不错。</p>
<p><strong>优化算法的选择和使用方面的一些tricks</strong></p>
<ol>
<li><strong>首先，各大算法孰优孰劣并无定论</strong>。如果是刚入门，<strong>优先考虑 SGD+Nesterov Momentum或者Adam</strong>.（Standford 231n : The two recommended updates to use are either SGD+Nesterov Momentum or Adam）</li>
<li><strong>选择你熟悉的算法</strong>——这样你可以更加熟练地利用你的经验进行调参。</li>
<li><strong>充分了解你的数据</strong>——如果模型是非常稀疏的，那么优先考虑自适应学习率的算法。</li>
<li><strong>根据你的需求来选择</strong>——在模型设计实验过程中，要快速验证新模型的效果，可以先用Adam进行快速实验优化；在模型上线或者结果发布前，可以用精调的SGD进行模型的极致优化。</li>
<li><strong>先用小数据集进行实验</strong>。有论文研究指出，随机梯度下降算法的收敛速度和数据集的大小的关系不大。（The mathematics of stochastic gradient descent are amazingly independent of the training set size. In particular, the asymptotic SGD convergence rates are independent from the sample size. [2]）因此可以先用一个具有代表性的小数据集进行实验，测试一下最好的优化算法，并通过参数搜索来寻找最优的训练参数。</li>
<li><strong>考虑不同算法的组合</strong>。先用Adam进行快速下降，而后再换到SGD进行充分的调优。切换策略可以参考本文介绍的方法。</li>
<li><strong>数据集一定要充分的打散（shuffle）</strong>。这样在使用自适应学习率算法的时候，可以避免某些特征集中出现，而导致的有时学习过度、有时学习不足，使得下降方向出现偏差的问题。</li>
<li>训练过程中<strong>持续监控训练数据和验证数据</strong>上的目标函数值以及精度或者AUC等指标的变化情况。对训练数据的监控是要保证模型进行了充分的训练——下降方向正确，且学习率足够高；对验证数据的监控是为了避免出现过拟合。</li>
<li><strong>制定一个合适的学习率衰减策略</strong>。可以使用定期衰减策略，比如每过多少个epoch就衰减一次；或者利用精度或者AUC等性能指标来监控，当测试集上的指标不变或者下跌时，就降低学习率。</li>
<li>对于稀疏数据，尽量使用学习率可自适应的优化方法，不用手动调节，而且最好采用默认值</li>
<li>SGD通常训练时间更长，但是在好的初始化和学习率调度方案的情况下，结果更可靠</li>
<li>如果在意更快的收敛，并且需要训练较深较复杂的网络时，推荐使用学习率自适应的优化方法。</li>
<li>Adadelta，RMSprop，Adam是比较相近的算法，在相似的情况下表现差不多。</li>
<li>在想使用带动量的RMSprop，或者Adam的地方，大多可以使用Nadam取得更好的效果</li>
<li>如果验证损失较长时间没有得到改善，可以停止训练。</li>
<li>添加梯度噪声（高斯分布 ）到参数更新，可使网络对不良初始化更加健壮，并有助于训练特别深而复杂的网络。</li>
</ol>
]]></content>
      <categories>
        <category>学习</category>
        <category>深度学习</category>
        <category>神经网络</category>
      </categories>
      <tags>
        <tag>总结</tag>
        <tag>编程</tag>
        <tag>优化</tag>
        <tag>调参</tag>
      </tags>
  </entry>
  <entry>
    <title>神经网络-梯度爆炸和梯度消失</title>
    <url>/2021/10/01/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8%E5%92%8C%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>产生原因：反向传播过程的链式法则会导致梯度剧烈变化。如果每一层神经元对上一层的输出的偏导乘上权重结果都小于1的会导致梯度消失，如果都大于1的会导致梯度爆炸。</p>
<p>解决办法：</p>
<ol>
<li><p>预训练加微调</p>
</li>
<li><p>针对梯度爆炸：梯度剪切、权重正则，因此，如果发生梯度爆炸，权值的范数就会变的非常大，通过正则化项，可以<strong>部分</strong>限制<strong>梯度爆炸</strong>的发生。</p>
</li>
<li><p>使用不同的激活函数，用ReLU、Leaky-ReLU、P-ReLU、R-ReLU、Maxout等替代sigmoid函数。</p>
</li>
<li><p>BN（Batch Normalization），batchnorm就是通过对每一层的输出规范为均值和方差一致的方法，消除了w带来的放大缩小的影响，进而解决梯度消失和爆炸的问题。</p>
</li>
<li><p>残差的捷径（shortcut）</p>
</li>
<li><p>使用LSTM网络，它内部的“门”可以接下来更新的时候“记住”前几次训练的”残留记忆“</p>
</li>
</ol>
<span id="more"></span>

<h1 id="什么是梯度"><a href="#什么是梯度" class="headerlink" title="什么是梯度"></a>什么是梯度</h1><p>在深度神经网络中BP（反向传播算法）就是基于梯度下降策略，以目标的负梯度方向对参数进行调整， 参数的更新为 <img src="https://www.zhihu.com/equation?tex=w%5Cleftarrow+w+%5CDelta+w" alt="[公式]"> ，给定学习率 <img src="https://www.zhihu.com/equation?tex=%5Calpha" alt="[公式]"> ，得出 <img src="https://www.zhihu.com/equation?tex=%5CDelta+w=-%5Calpha%5Cfrac%7B%5Cpartial+Loss%7D%7Bw%7D%5C" alt="[公式]"> 。如果更新第二层隐层的权值信息，根据链式求导法则，更新梯度信息为：</p>
<p><img src="https://pic1.zhimg.com/80/v2-718946a3296668d9221045a092e42fe0_1440w.jpg" alt="img"></p>
<p>上式中， <img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+f_%7B2%7D%7D%7B%5Cpartial+w_%7B2%7D%7D=f_%7B1%7D" alt="[公式]"> ，即第二隐藏层的输入。<img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+f_%7B4%7D%7D%7B%5Cpartial+f_%7B3%7D%7D" alt="[公式]"> 就是对激活函数进行求导，，那么层数增多的时候，最终的求出的梯度更新将以指数形式变化。</p>
<h1 id="梯度消失-弥散"><a href="#梯度消失-弥散" class="headerlink" title="梯度消失/弥散"></a>梯度消失/弥散</h1><p>根据链式法则，如果每一层神经元对上一层的输出的偏导乘上权重结果都小于1的话，那么即使这个结果是0.99，在经过足够多层传播之后，误差对输入层的偏导会趋于0。</p>
<p>这种情况会导致靠近输入层的隐含层神经元调整极小。</p>
<p>梯度消失经常出现，一是在<strong>深层网络</strong>中，二是采用了<strong>不合适的损失函数</strong>，比如sigmoid。</p>
<h1 id="梯度爆炸"><a href="#梯度爆炸" class="headerlink" title="梯度爆炸"></a>梯度爆炸</h1><p>据链式法则，如果每一层神经元对上一层的输出的偏导乘上权重结果都大于1的话，在经过足够多层传播之后，误差对输入层的偏导会趋于无穷大。</p>
<p>这种情况又会导致靠近输入层的隐含层神经元调整变动极大。</p>
<p>梯度爆炸一般出现在深层网络和<strong>权值初始化值太大</strong>的情况下</p>
<p>注：事实上，在深度神经网络中，往往是梯度消失出现的更多一些。</p>
<h1 id="梯度消失-爆炸解决方案："><a href="#梯度消失-爆炸解决方案：" class="headerlink" title="梯度消失/爆炸解决方案："></a>梯度消失/爆炸解决方案：</h1><ol>
<li>预训练加微调</li>
</ol>
<p>此方法来自Hinton在2006年发表的一篇论文，Hinton为了解决梯度的问题，提出采取无监督逐层训练方法，其基本思想是每次训练一层隐节点，训练时将上一层隐节点的输出作为输入，而本层隐节点的输出作为下一层隐节点的输入，此过程是逐层“预训练”（pre-training）；在预训练完成后，再对整个网络进行“微调”（fine-tunning）。Hinton在训练深度信念网络（Deep Belief Networks中，使用了这个方法，在各层预训练完成后，再利用BP算法对整个网络进行训练。此思想相当于是先寻找局部最优，然后整合起来寻找全局最优，此方法有一定的好处，但是目前应用的不是很多了。</p>
<ol start="2">
<li>针对梯度爆炸：梯度剪切、权重正则</li>
</ol>
<p>梯度剪切这个方案主要是针对梯度爆炸提出的，其思想是设置一个梯度剪切阈值，然后更新梯度的时候，如果梯度超过这个阈值，那么就将其强制限制在这个范围之内。</p>
<p>权重正则化：</p>
<p>正则化是通过对网络权重做正则限制过拟合，仔细看正则项在损失函数的形式：</p>
<p><img src="https://pic3.zhimg.com/80/v2-29298ec3d2b9094b20abdc6d9d7b1272_1440w.png" alt="img"></p>
<p>其中， <img src="https://www.zhihu.com/equation?tex=%5Calpha" alt="[公式]"> 是指正则项系数，因此，如果发生梯度爆炸，权值的范数就会变的非常大，通过正则化项，可以<strong>部分</strong>限制<strong>梯度爆炸</strong>的发生。</p>
<ol start="3">
<li><p>使用不同的激活函数</p>
<p>用ReLU、Leaky-ReLU、P-ReLU、R-ReLU、Maxout等替代sigmoid函数。</p>
</li>
<li><p>BN（Batch Normalization）</p>
<p>反向传播中，经过每一层的梯度会乘以该层的权重，举个简单例子：</p>
<p>正向传播中f2=f1(wT∗x+b)，那么反向传播中</p>
<p><img src="https://pic4.zhimg.com/80/v2-614cb67f4bfae76e8452b3dd06223cdf_1440w.png" alt="img"></p>
<p>反向传播式子中有w的存在，所以w的大小影响了梯度的消失和爆炸，batchnorm就是通过对每一层的输出规范为均值和方差一致的方法，消除了w带来的放大缩小的影响，进而解决梯度消失和爆炸的问题。</p>
</li>
<li><p>残差结构</p>
<p>残差的捷径（shortcut）</p>
</li>
<li><p>使用LSTM网络</p>
<p>LSTM内部复杂的“门”(gates)，LSTM通过它内部的“门”可以接下来更新的时候“记住”前几次训练的”残留记忆“，因此，经常用于生成文本中。</p>
</li>
</ol>
]]></content>
      <categories>
        <category>学习</category>
        <category>机器学习</category>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>梯度</tag>
      </tags>
  </entry>
  <entry>
    <title>神经网络-激活函数</title>
    <url>/2021/10/01/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>背景问题：</p>
<p>方法简述：</p>
<p>优点：</p>
<p>缺点：</p>
<span id="more"></span>

<h2 id="激活函数的作用"><a href="#激活函数的作用" class="headerlink" title="激活函数的作用"></a>激活函数的作用</h2><ol>
<li>激活函数对模型学习、理解非常复杂和非线性的函数具有重要作用。</li>
<li>激活函数可以引入非线性因素。如果不使用激活函数，则输出信号仅是一个简单的线性函数。线性函数一个一级多项式，线性方程的复杂度有限，从数据中学习复杂函数映射的能力很小。没有激活函数，神经网络将无法学习和模拟其他复杂类型的数据，例如图像、视频、音频、语音等。</li>
<li>激活函数可以把当前特征空间通过一定的线性映射转换到另一个空间，让数据能够更好的被分类。</li>
<li>假若网络中全部是线性部件，那么线性的组合还是线性，与单独一个线性分类器无异。这样就做不到用非线性来逼近任意函数。</li>
<li>使用非线性激活函数 ，以便使网络更加强大，增加它的能力，使它可以学习复杂的事物，复杂的表单数据，以及表示输入输出之间非线性的复杂的任意函数映射。使用非线性激活函数，能够从输入输出之间生成非线性映射。</li>
</ol>
<h2 id="激活函数的性质"><a href="#激活函数的性质" class="headerlink" title="激活函数的性质"></a>激活函数的性质</h2><ol>
<li>单调性：当激活函数是单调的时候，单层网络能够保证是凸函数，</li>
<li>可导性: 在一定范围内处处可导;</li>
<li>输出值的范围：当激活函数输出值是有限的时候，基于梯度的优化方法会更加稳定，因为特征的表示受有限权值的影响更显著；当激活函数的输出是无限的时候，模型的训练会更加高效，不过在这种情况下，一般需要更小的 Learning Rate。</li>
<li>非线性：当激活函数是非线性的，一个两层的神经网络就可以基本上逼近所有的函数。但如果激活函数是恒等激活函数的时候，即 ，就不满足这个性质，而且如果 MLP 使用的是恒等激活函数，那么其实整个网络跟单层神经网络是等价的；</li>
<li>可微性：当优化方法是基于梯度的时候，就体现了该性质；：当激活函数满足这个性质的时候，如果参数的初始化是随机的较小值，那么神经网络的训练将会很高效；如果不满足这个性质，那么就需要详细地去设置初始值；</li>
</ol>
<h2 id="不同激活函数适用场景"><a href="#不同激活函数适用场景" class="headerlink" title="不同激活函数适用场景"></a>不同激活函数适用场景</h2><ol>
<li>sigmoid 激活函数：除了输出层是一个二分类问题基本不会用它。</li>
<li>tanh 激活函数：tanh 是非常优秀的，几乎适合所有场合。</li>
<li>ReLu 激活函数：最常用的默认函数，如果不确定用哪个激活函数，就使用 ReLu 或者 Leaky ReLu，再去尝试其他的激活函数。</li>
<li>Leaky ReLU激活函数，应该很好的应对神经元坏死二等情况。</li>
<li>如果在隐藏层上不确定使用哪个激活函数，那么通常会使用 Relu 激活函数。有时，也会使用 tanh 激活函数，但 Relu 的一个优点是：当是负值的时候，导数等于 0。</li>
</ol>
<h2 id="线性激活函数适用场景"><a href="#线性激活函数适用场景" class="headerlink" title="线性激活函数适用场景"></a>线性激活函数适用场景</h2><ol>
<li>输出层，大多使用线性激活函数。</li>
<li>在隐含层可能会使用一些线性激活函数。</li>
<li>一般用到的线性激活函数很少。</li>
</ol>
<h1 id="非线性激活函数"><a href="#非线性激活函数" class="headerlink" title="非线性激活函数"></a>非线性激活函数</h1><p><img src="/2021/10/01/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/%E5%B8%B8%E8%A7%81%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0.png"></p>
<h2 id="ReLu-激活函数的优点"><a href="#ReLu-激活函数的优点" class="headerlink" title="ReLu 激活函数的优点"></a>ReLu 激活函数的优点</h2><ol>
<li>在区间变动很大的情况下，ReLu 激活函数的导数或者激活函数的斜率都会远大于 0，在程序实现就是一个 if-else 语句，而 sigmoid 函数需要进行浮点四则运算，在实践中，使用 ReLu 激活函数神经网络通常会比使用 sigmoid 或者 tanh 激活函数学习的更快。</li>
<li>sigmoid 和 tanh 函数的导数在正负饱和区的梯度都会接近于 0，这会造成梯度弥散，而 Relu 和Leaky ReLu 函数大于 0 部分都为常数，不会产生梯度弥散现象。</li>
<li>需注意，Relu 进入负半区的时候，梯度为 0，神经元此时不会训练，产生所谓的稀疏性，而 Leaky ReLu 不会产生这个问题。</li>
</ol>
<h2 id="Relu（-lt-0-时）为什么是非线性激活函数？"><a href="#Relu（-lt-0-时）为什么是非线性激活函数？" class="headerlink" title="Relu（&lt; 0 时）为什么是非线性激活函数？"></a>Relu（&lt; 0 时）为什么是非线性激活函数？</h2><p>Relu 激活函数图像由上图</p>
<p>根据图像可看出具有如下特点：</p>
<ol>
<li><p>单侧抑制；</p>
</li>
<li><p>相对宽阔的兴奋边界；</p>
</li>
<li><p>稀疏激活性；</p>
<p>ReLU 函数从图像上看，是一个分段线性函数，把所有的负值都变为 0，而正值不变，这样就成为单侧抑制。</p>
<p>因为有了这单侧抑制，才使得神经网络中的神经元也具有了稀疏激活性。</p>
<p><strong>稀疏激活性</strong>：从信号方面来看，即神经元同时只对输入信号的少部分选择性响应，大量信号被刻意的屏蔽了，这样可以提高学习的精度，更好更快地提取稀疏特征。当 时，ReLU 硬饱和，而当 时，则不存在饱和问题。ReLU 能够在 时保持梯度不衰减，从而缓解梯度消失问题。</p>
</li>
</ol>
]]></content>
      <categories>
        <category>学习</category>
        <category>深度学习</category>
        <category>神经网络</category>
      </categories>
      <tags>
        <tag>总结</tag>
        <tag>编程</tag>
        <tag>优化</tag>
        <tag>调参</tag>
      </tags>
  </entry>
  <entry>
    <title>站内说明</title>
    <url>/2021/09/29/%E7%AB%99%E5%86%85%E8%AF%B4%E6%98%8E/</url>
    <content><![CDATA[<p>本blog是作为Leo同学的自我拓展过程中的自律自省记录</p>
<p>对该blog的计划是对一下类别和领域的学习与探索：</p>
<p>理论方面：</p>
<p>​    计算机视觉</p>
<p>​    深度学习算法</p>
<p>​    机器学习算法</p>
<p>Coding方面：</p>
<p>​    OpenCV</p>
<p>​    Python</p>
<p>​    Pytorch</p>
<p>​    </p>
]]></content>
      <categories>
        <category>其他</category>
      </categories>
      <tags>
        <tag>概览</tag>
      </tags>
  </entry>
  <entry>
    <title>计算机视觉CV算法概览（更新ing）</title>
    <url>/2021/09/29/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89CV%E7%AE%97%E6%B3%95%E6%A6%82%E8%A7%88/</url>
    <content><![CDATA[<h1 id="图像识别"><a href="#图像识别" class="headerlink" title="图像识别"></a>图像识别</h1><h2 id="1-AlexNet"><a href="#1-AlexNet" class="headerlink" title="1. AlexNet"></a>1. AlexNet</h2><h2 id="2-VGGNet"><a href="#2-VGGNet" class="headerlink" title="2. VGGNet"></a>2. VGGNet</h2><h2 id="3-GoogLeNet"><a href="#3-GoogLeNet" class="headerlink" title="3. GoogLeNet"></a>3. GoogLeNet</h2><h2 id="4-ResNet"><a href="#4-ResNet" class="headerlink" title="4. ResNet"></a>4. ResNet</h2><h2 id="5-DenseNet"><a href="#5-DenseNet" class="headerlink" title="5. DenseNet"></a>5. DenseNet</h2><h1 id="目标检测"><a href="#目标检测" class="headerlink" title="目标检测"></a>目标检测</h1><h2 id="1-R-CNN-系列"><a href="#1-R-CNN-系列" class="headerlink" title="1. R-CNN 系列"></a>1. R-CNN 系列</h2><h3 id="1-R-CNN"><a href="#1-R-CNN" class="headerlink" title="1. R-CNN"></a>1. R-CNN</h3><h3 id="2-Fast-R-CNN"><a href="#2-Fast-R-CNN" class="headerlink" title="2. Fast R-CNN"></a>2. Fast R-CNN</h3><h3 id="3-Faster-R-CNN"><a href="#3-Faster-R-CNN" class="headerlink" title="3. Faster R-CNN"></a>3. Faster R-CNN</h3><h2 id="2-Yolo系列"><a href="#2-Yolo系列" class="headerlink" title="2. Yolo系列"></a>2. Yolo系列</h2><h3 id="1-Yolo-V1"><a href="#1-Yolo-V1" class="headerlink" title="1.Yolo V1"></a>1.Yolo V1</h3><h3 id="2-Yolo-V2"><a href="#2-Yolo-V2" class="headerlink" title="2.Yolo V2"></a>2.Yolo V2</h3><h3 id="3-Yolo-V3"><a href="#3-Yolo-V3" class="headerlink" title="3.Yolo V3"></a>3.Yolo V3</h3><h3 id="4-Yolo-V4"><a href="#4-Yolo-V4" class="headerlink" title="4.Yolo V4"></a>4.Yolo V4</h3><h3 id="5-Yolo-V5"><a href="#5-Yolo-V5" class="headerlink" title="5.Yolo V5"></a>5.Yolo V5</h3><h2 id="3-SSD"><a href="#3-SSD" class="headerlink" title="3. SSD"></a>3. SSD</h2><h2 id="4-Retina-Net"><a href="#4-Retina-Net" class="headerlink" title="4. Retina-Net"></a>4. Retina-Net</h2><h1 id="图像分割"><a href="#图像分割" class="headerlink" title="图像分割"></a>图像分割</h1><h2 id="1-FCN"><a href="#1-FCN" class="headerlink" title="1. FCN"></a>1. FCN</h2><h2 id="2-Mask-R-CNN"><a href="#2-Mask-R-CNN" class="headerlink" title="2. Mask R-CNN"></a>2. Mask R-CNN</h2><h1 id="目标追踪"><a href="#目标追踪" class="headerlink" title="目标追踪"></a>目标追踪</h1><h2 id="1-Goturn"><a href="#1-Goturn" class="headerlink" title="1. Goturn"></a>1. Goturn</h2><h2 id="2"><a href="#2" class="headerlink" title="2."></a>2.</h2><h1 id="图像生成"><a href="#图像生成" class="headerlink" title="图像生成"></a>图像生成</h1><h2 id="1-GAN"><a href="#1-GAN" class="headerlink" title="1. GAN"></a>1. GAN</h2>]]></content>
      <categories>
        <category>学习</category>
        <category>cv算法</category>
      </categories>
      <tags>
        <tag>概览</tag>
      </tags>
  </entry>
  <entry>
    <title>计算机视觉模型库-timm</title>
    <url>/2021/10/06/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E6%A8%A1%E5%9E%8B%E5%BA%93-timm/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>Py<strong>T</strong>orch <strong>Im</strong>age <strong>M</strong>odels (<strong>timm</strong>)由Ross Wightman创建的深度学习库，是一个图像模型（models）、层（layers）、实用程序（utilities）、优化器（optimizers）、调度器（schedulers）、数据加载/增强（data-loaders / augmentations）和参考训练/验证脚本（reference training / validation scripts）的集合是一个关于SOTA的计算机视觉模型、层、实用工具、optimizers, schedulers, data-loaders, augmentations，可以复现ImageNet训练结果的训练/验证代码。</p>
<span id="more"></span>

<h1 id="创建模型"><a href="#创建模型" class="headerlink" title="创建模型"></a>创建模型</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> timm </span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="comment"># 创建模型</span></span><br><span class="line">model = timm.create_model(<span class="string">&#x27;resnet34&#x27;</span>)</span><br><span class="line"><span class="comment"># 如果是要创建一个预训练的模型，则只要设置pretrained=True即可。</span></span><br><span class="line">pretrained_resnet_34 = timm.create_model(<span class="string">&#x27;resnet34&#x27;</span>, pretrained=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 创建一个定制化类别数量的模型，只需设置num_classes</span></span><br><span class="line">model = timm.create_model(<span class="string">&#x27;resnet34&#x27;</span>, num_classes=<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">x     = torch.randn(<span class="number">1</span>, <span class="number">3</span>, <span class="number">224</span>, <span class="number">224</span>)</span><br><span class="line">model(x).shape</span><br></pre></td></tr></table></figure>

<h1 id="预训练权重的模型列表"><a href="#预训练权重的模型列表" class="headerlink" title="预训练权重的模型列表"></a>预训练权重的模型列表</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 返回timm中所有模型的列表</span></span><br><span class="line">timm.list_models()</span><br><span class="line"><span class="comment"># 查看具有预训练权重的模型，只需设置pretrained=True</span></span><br><span class="line">avail_pretrained_models = timm.list_models(pretrained=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 使用关键词检索模型架构</span></span><br><span class="line">all_densenet_models = timm.list_models(<span class="string">&#x27;*densenet*&#x27;</span>)</span><br></pre></td></tr></table></figure>

<h1 id="其他设置"><a href="#其他设置" class="headerlink" title="其他设置"></a>其他设置</h1><h2 id="通道数设置"><a href="#通道数设置" class="headerlink" title="通道数设置"></a>通道数设置</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 设置单通道模型</span></span><br><span class="line">m = timm.create_model(<span class="string">&#x27;resnet34&#x27;</span>,pretrained=<span class="literal">True</span>, in_chans=<span class="number">1</span>)</span><br><span class="line"><span class="comment"># single channel image</span></span><br><span class="line">x = torch.randn(<span class="number">1</span>, <span class="number">1</span>, <span class="number">224</span>, <span class="number">224</span>)</span><br><span class="line">m(x).shape</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置多通道模型</span></span><br><span class="line">m = timm.create_model(<span class="string">&#x27;resnet34&#x27;</span>, pretrained=<span class="literal">True</span>, in_chans=<span class="number">25</span>)</span><br><span class="line"><span class="comment"># 25-channel image</span></span><br><span class="line">x = torch.randn(<span class="number">1</span>, <span class="number">25</span>, <span class="number">224</span>, <span class="number">224</span>)</span><br><span class="line"></span><br><span class="line">m(x).shape</span><br></pre></td></tr></table></figure>

<p>timm源码中的处理方式是</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 将通道数设置为1时， timm简单地将原3通道权重相加成单通道权重，并更新权重的shape</span></span><br><span class="line">conv1_weight = state_dict[<span class="string">&#x27;conv1.weight&#x27;</span>]</span><br><span class="line">conv1_weight.<span class="built_in">sum</span>(dim=<span class="number">1</span>, keepdim=<span class="literal">True</span>).shape</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将通道设置为8时，则先将原先的3通道复制3次变为9通道，然后选择前8个通道</span></span><br><span class="line">conv1_name = cfg[<span class="string">&#x27;first_conv&#x27;</span>]</span><br><span class="line">conv1_weight = state_dict[conv1_name + <span class="string">&#x27;.weight&#x27;</span>]</span><br><span class="line">conv1_type = conv1_weight.dtype</span><br><span class="line">conv1_weight = conv1_weight.<span class="built_in">float</span>()</span><br><span class="line">repeat = <span class="built_in">int</span>(math.ceil(in_chans / <span class="number">3</span>))</span><br><span class="line">conv1_weight = conv1_weight.repeat(<span class="number">1</span>, repeat, <span class="number">1</span>, <span class="number">1</span>)[:, :in_chans, :, :]</span><br><span class="line">conv1_weight *= (<span class="number">3</span> / <span class="built_in">float</span>(in_chans))</span><br><span class="line">conv1_weight = conv1_weight.to(conv1_type)</span><br><span class="line">state_dict[conv1_name + <span class="string">&#x27;.weight&#x27;</span>] = conv1_weight</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>学习</category>
        <category>Pytorch</category>
        <category>深度学习</category>
        <category>图像识别</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>源码阅读</tag>
      </tags>
  </entry>
  <entry>
    <title>论文精度-AIPerf: Automated machine learning as an AI-HPC benchmark</title>
    <url>/2021/11/11/%E8%AE%BA%E6%96%87%E7%B2%BE%E5%BA%A6-AIPerf-Automated-machine-learning-as-an-AI-HPC-benchmark/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>论文精读-Deep-Learninge-Driven Quantification of Interstitial Fibrosis in Digitized Kidney Biopsies</title>
    <url>/2021/11/22/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB-Deep-Learninge-Driven-Quantification-of-Interstitial-Fibrosis-in-Digitized-Kidney-Biopsies/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>背景问题：</p>
<p>方法简述：</p>
<p>优点：</p>
<p>缺点：</p>
<span id="more"></span>



<p>肾活检显示间质纤维化和肾小管萎缩 (IFTA) 是疾病慢性化和预后的有力指标。通常用于 IFTA 分级的技术仍然是手动的，导致病理学家之间存在差异。使用计算技术进行准确的 IFTA 估计可以减少这种可变性并提供定量评估。使用从人类肾脏活检处理的三色染色全幻灯片图像 (WSI)，我们开发了一个深度学习框架，该框架以高分辨率和 WSI 级别的整体背景捕获更精细的病理结构，以预测 IFTA 等级。WSI ( <em>n</em> = 67) 来自俄亥俄州立大学韦克斯纳医学中心。五名肾脏病理学家对它们进行了独立审查，并提供了转换为 IFTA 等级的纤维化评分：≤10%（无或极小）、11% 至 25%（轻度）、26% 至 50%（中度）和 &gt;50%（重度） . 该模型是通过将 WSI 与由多数投票决定的 IFTA 等级（参考估计）相关联而开发的。模型性能在 WSI 上进行评估 ( <em>n</em> = 28) 从肾脏精准医学项目中获得。病理学家和参考估计之间的 IFTA 分级非常一致（κ = 0.622 ± 0.071）。深度学习模型在俄亥俄州立大学韦克斯纳医学中心的准确率为 71.8% ± 5.3%，在肾脏精准医学项目数据集上的准确率为 65.0% ± 4.2%。我们分析肾活检微观和 WSI 水平变化的方法试图模仿病理学家，并提供 IFTA 的区域和上下文估计。这样的方法可以帮助临床病理学诊断。</p>
<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>病理切片检查是临床诊断的重要依据。</p>
<p>肾间质纤维化和肾小管萎缩指标的重要性。</p>
<p>强调了利用数字病理学的进步和开发现代数据分析技术（如深度学习（DL））来对肾脏病理学进行全面图像分析的必要性</p>
<p>深度学习技术，例如卷神经网络，已被广泛用于组织病理学图像分析。在肾脏的背景下疾病，研究人员已经能够产生高准确度的方法去评估疾病等级的方法，细分各种肾脏结构，并预测临床表型。尽管这项工作非常有价值，但几乎所有工作都侧重于分析<strong>高分辨率全幻灯片图像 （WSI）</strong>，方法是将它们分为较小的板块，或将图像大小调整为较低的分辨率，并将它们与感兴趣的各种输出相关联。这些技术具有各种优点和局限性。虽然基于patch的方法保持了图像分辨率，但独立分析每个patch并不能保持该patch在整个WSI背景下的空间相关性。相比之下，将WSI调整为较低的分辨率可能是一种计算效率较高的方法，但可能无法在高分辨率WSI中捕捉到较好的细节。这里就是说如果将病理图变成一小块一小块的，虽然清晰度高，但是没有整体分析。但是缩小成完整的图片，就是会丢失很多细节信息。</p>
<p>本研究的目标是开发一种计算管道，该管道可以处理WSI以准确捕获IFTA等级。<strong>模拟了肾病理学家在显微镜下对活检载玻片进行分级的方法。</strong>专家的典型工作流程涉及手动操作，例如平移以及放大和缩小载玻片上的特定区域，以评估病理学的各个方面。在缩小评估中，病理学家会检查整个载玻片，并对肾核进行全局或 WSI 水平的评估。在放大评估中，他们对感兴趣区域的局部病理学进行深入的显微镜评估。这两项评估使他们能够全面评估肾活检，包括IFTA等级的估计。我们认为，基于DL的计算方法将模仿肾病理学家在评估肾脏活检图像时使用的过程。使用来自两个不同队列的WSI及其相应的IFTA等级，解决了以下目标。首先，该框架需要处理图像子区域（或块），并量化这些块中IFTA的范围。其次，该框架需要在其环境的上下文中处理每个图像块，并在WSI上评估IFTA。开发了一个基于深度学习的计算管道，该管道可以整合来自本地图像块的模式和特征以及来自WSI的完整信息，为图像块提供上下文。通过这种图像块和全局的组合，该模型旨在准确预测IFTA等级。一个由执业<strong>肾病理学家组成的国际团队评估了数字化活检</strong>，并提供了IFTA等级。WSI及其相应的IFTA等级用于训练和验证DL模型。<strong>深度学习模型还与基于传统计算机视觉和机器学习的建模框架进行了比较</strong>，该框架使用图像描述符和纹理特征。报告了DL模型的性能以及与IFTA等级高度相关的已识别图像子区域。</p>
<h1 id="Material-and-Method"><a href="#Material-and-Method" class="headerlink" title="Material and Method"></a>Material and Method</h1><p>获得提交给俄亥<strong>俄州立大学Wexner医学中心（OSUWMC</strong>）的患者的三色染色肾活检的去识别WSI。肾活检以及患者数据收集，染色和数字化遵循OSUWMC机构审查委员会批准的方案（研究编号：2018H0495）（表1）。<strong>从肾脏精准医学项目（KPMP）的以下招聘地点也获得了去识别的WS</strong>I：布莱根妇女医院，克利夫兰诊所，哥伦比亚大学，约翰霍普金斯大学和德克萨斯大学西南大学达拉斯分校。KPMP是一个由国家糖尿病和消化和肾脏疾病研究所资助的多年项目，旨在了解和寻找治疗慢性肾脏疾病和急性肾损伤的新方法。种族/族裔信息直接来自OSUWMC记录和KPMP网站。</p>
<p>所有 WSI 都上传到一个安全的、基于 Web 的软件（PixelView; deepPath， Inc.， Boston， MA）。C.A.C. 担任软件帐户的组管理员，并向其他肾病理学家（K.R.、S.S..B、L.M.B.和 P.B.）提供对 WSI 的单独访问权限，这些肾病学家作为用户被分配到组帐户。该过程允许每位专家独立评估数字化活检。KPMP WSI和相关临床数据是在审查和批准KPMP与波士顿大学之间的数据使用协议（表2和补充表S1）后获得的。所有方法均按照喂养指南和法规进行。肾组织由OSUWMC和KMP参与者接受的活检的针刺活检样本组成。所有OSUWMC活检均使用WSI扫描仪[Aperio（Leica Biosystems，Wet-zlar，德国）或NanoZoomer（日本滨松市滨松）]以40视在镜放大倍率下进行扫描，从而产生每像素分辨率为0.25 mm的WSI</p>
<p>KPMP的所有WSI都是通过使用Aperio AT2大容量扫描仪（徕卡病理系统）以40视在视放大倍率下数字化肾活检而生成的，分辨率为每像素0.25毫米（图1）。有关病理学方案的更多详细信息可以直接从KPMP网站获得（<a href="https://www.kpmp.org,最后一次访问时间为2021年5月1日).基于aperio的wsi以svs格式获得,基于hamamatsu的wsi以ndpi格式获得./">https://www.kpmp.org，最后一次访问时间为2021年5月1日）。基于Aperio的WSI以SVS格式获得，基于Hamamatsu的WSI以NDPI格式获得。</a><br>由肾病理学家（C.A.C.）对所有WSI进行了手动质量检查。此检查确保所选 WSI 区域没有伪影，例如气泡、折叠、压缩、撕裂、过度染色或染色不足、污渍批次变化、刀子颤动和厚度差异。由于大多数WSI具有多个核心，因此肾病理学家能够选择在所有病例上都没有质量问题的核心（补充图S2）。然后仔细裁剪WSI的选定部分并将其转换为数字矩阵以进行进一步分析。</p>
<p>肾病理学家（C.A.C.）鉴定并注释了每个WSI中的皮质区域（图1），其中皮质和髓质都存在。所有肾病理学家都从各自的计算机上使用PixelView在Web浏览器上访问并独立审查了IFTA的WSI。评分以IFTA皮质区域的百分比（0%至100%）提供，然后将其转换为半定量等级：10%（无或最小），11%至25%（轻度），26%至50%（中度）和&gt;50%（重度）。最终成绩是通过对从每位肾病理学家获得的等级进行多数投票来计算的。KPMP数据集的纤维化评分直接从研究者那里获得，并使用相同的标准转换为IFTA等级。从<strong>两个数据集中</strong>得出的IFTA等级用于进一步分析。</p>
<h2 id="Deep-Learning"><a href="#Deep-Learning" class="headerlink" title="Deep Learning"></a>Deep Learning</h2><p>FPN</p>
<p>裁剪后的图像斑块（Np Np 像素）从原始 WSI 中自动提取，并使用以下标准标记为组织或背景。包含至少50%或更多像素内组织的图像贴片被标记为组织，否则标记为背景。包含标记为组织的图像贴片的本地分支被馈送到FPN模型中。包含原始 WSI 的缩减采样低分辨率版本（Ng Ng 像素）的全局分支用作另一个 FPN 模型的输入。为了实现本地和全局特征交互，来自任一分支的所有层的特征图都与其他分支共享（图 2B）。全球分支在与当前本地补丁相同的空间位置裁剪其特征图。要与本地分支进行交互，glpathnet 会将裁剪的特征图上采样，使其与图层中具有相同深度的本地分支的特征图大小相同。随后，glpathnet 连接本地特征图和裁剪的全局特征图，这些特征图被馈送到本地分支的下一层。以对称方式，本地分支将其特征映射缩减采样为与从原始输入图像裁剪的面片相同的相对空间比率。根据裁剪面块的位置，将缩减采样的局部特征图合并为同一图层中具有相同大小的全局分支特征的要素图。所有零的特征图都用于标记为背景的补丁。全局特征图和合并的局部特征图被连接起来并馈送到全局分支的下一层。<br>glpathnet 中的融合分支包含一个卷积层，后跟一个完全连接的层。它从本地分支的最后一层获取串联的特征图，并从全局分支获取相同的特征图。融合分支的输出是补丁级 IFTA 等级，最终的 IFTA 等级被确定为最常见的补丁级 IFTA 等级。<br>交叉熵损失用于使用预训练的DL架构（ResNet50）在OSUWMC数据上训练glpathnet，26作为FPN模型卷积网络的一部分。为了最大限度地提高效率，Np和Ng都设置为508像素。Adam 优化器 （b1 Z 0.9; b2 Z 0.999） 用于优化批大小为 6 的模型训练。我们将本地分支机构的初始学习率分配为2 105，全球分支机构的初始学习率为1 104。Glpathnet是使用PyTorch实现的，模型训练是在包含GeForce RTX 2080 Ti显卡（NVIDIA，Santa Clara，CA）的图形处理单元（GPU）工作站上执行的，该工作站具有11 Gb GDDR6内存。模型训练花了&lt;16个小时才达到收敛。在未用于模型训练的新WSI上预测IFTA等级大约需要30秒。</p>
<h2 id="Traditional-Method"><a href="#Traditional-Method" class="headerlink" title="Traditional Method"></a>Traditional Method</h2><p>传统机器学习模型<br>为了进行比较，基于传统的机器学习构建了IFTA分类模型，该模型使用来自OSUWMC WSI数据的派生特征。使用加权邻居距离，包括表示形态的算法的复合分层，27e29是一种多用途图像分类器，可以提取近似于3000个通用图像描述符，包括多级分解，高对比度特征，像素统计和纹理（补充表S2）。这些特征直接来自原始WSI，WSI的变换和WSI的复合变换（变换的跨式）。使用这些特征作为输入，构建了一个四标签分类器来预测最终的IFTA等级。该模型在OSUWMC数据集上进行训练，KPMP数据集用于测试。</p>
]]></content>
      <categories>
        <category>学习</category>
        <category>深度学习</category>
        <category>医学图像</category>
      </categories>
      <tags>
        <tag>论文</tag>
      </tags>
  </entry>
  <entry>
    <title>语义分割-算法概述</title>
    <url>/2021/10/04/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2-%E7%AE%97%E6%B3%95%E6%A6%82%E8%BF%B0/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>什么是语义分割？</p>
<p>架构与方法</p>
<p>用卷积神经网络分类</p>
<p>用条件随机场优化</p>
<span id="more"></span>

<h1 id="什么是语义分割？"><a href="#什么是语义分割？" class="headerlink" title="什么是语义分割？"></a>什么是语义分割？</h1><p>语义分割是计算机视觉中的基本任务，在语义分割中我们需要将视觉输入分为不同的语义可解释类别，「语义的可解释性」即分类类别在真实世界中是有意义的。语义分割是在像素级别上的分类，属于同一类的像素都要被归为一类，因此语义分割是从像素级别来理解图像的。比如说如下的照片，属于人的像素都要分成一类，属于小汽车的像素也要分成一类，除此之外还有背景像素也被分为一类。注意语义分割不同于实例分割，实例分割还要将不同人的像素归为不同的类，实例分割比语义分割更进一步。</p>
<p><img src="/2021/10/04/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2-%E7%AE%97%E6%B3%95%E6%A6%82%E8%BF%B0/1.png"></p>
<p>Reference: Zhao 等人的 ICNet（2017）：语义分割的演示视频</p>
<p>与图像分类或目标检测相比，语义分割使我们对图像有更加细致的了解。这种了解在诸如自动驾驶、机器人以及图像搜索引擎等许多领域都是非常重要的。</p>
<h1 id="常用于训练语义分割模型的数据集"><a href="#常用于训练语义分割模型的数据集" class="headerlink" title="常用于训练语义分割模型的数据集"></a>常用于训练语义分割模型的数据集</h1><ul>
<li>Pascal VOC 2012：有 20 类目标，这些目标包括人类、机动车类以及其他类，可用于目标类别或背景的分割</li>
<li>Cityscapes：50 个城市的城市场景语义理解数据集</li>
<li>Pascal Context：有 400 多类的室内和室外场景</li>
<li>Stanford Background Dataset：至少有一个前景物体的一组户外场景。</li>
</ul>
<p>评估语义分割算法性能的标准指标是平均 IOU（Intersection Over Union，交并比）</p>
<h1 id="语义分割思路"><a href="#语义分割思路" class="headerlink" title="语义分割思路"></a>语义分割思路</h1><h2 id><a href="#" class="headerlink" title></a></h2><p><strong>1.Patch classification</strong></p>
<p>最初的深度学习方法应用于图像分割就是Patch classification。Patch classification方法，顾名思义，图像是切成块喂给深度模型的，然后对像素进行分类。使用图像块的主要原因是因为全连接层需要固定大小的图像。</p>
<p><strong>2.全卷积方法</strong></p>
<p>2014年，全卷积网络（FCN）横空出世，FCN将网络全连接层用卷积取代，因此使任意图像大小的输入都变成可能，而且速度比Patch classification方法快很多。</p>
<p>尽管移除了全连接层，但是CNN模型用于语义分割还存在一个问题，就是下采样操作（比如，pooling）。pooling操作可以扩大感受野因而能够很好地整合上下文信息（context中文称为语境或者上下文，通俗的理解就是综合了更多的信息来进行决策），对high-level的任务（比如分类），这是很有效的。但同时，由于pooling下采样操作，使得分辨率降低，因此削弱了位置信息，而语义分割中需要score map和原图对齐，因此需要丰富的位置信息。</p>
<p><strong>3.encoder-decoder架构</strong> 如Unet</p>
<p>encoder-decoder是基于FCN的架构。encoder由于pooling逐渐减少空间维度，而decoder逐渐恢复空间维度和细节信息。通常从encoder到decoder还有shortcut connetction（捷径连接，也就是跨层连接）。其中U-net就是这种架构很流行的一种</p>
<p><strong>4.空洞卷积</strong></p>
<p>dilated/atrous （空洞卷积）架构，这种结构代替了pooling，一方面它可以保持空间分辨率，另外一方面它由于可以扩大感受野因而可以很好地整合上下文信息。</p>
<p><strong>5.条件随机场</strong></p>
<p>除了以上思路，还有一种对分割结果进行后处理的方法，那就是条件随机场(Conditional Random Fields (CRFs))后处理用来改善分割效果。DeepLab系列文章基本都采用这种后处理方法，可以较好地改善分割结果，如下图：</p>
<p><strong>6.对抗学习</strong></p>
<p>作为分割的端到端解决方案有非常强大的性能。</p>
<h1 id="深度学习语义分割方法"><a href="#深度学习语义分割方法" class="headerlink" title="深度学习语义分割方法"></a>深度学习语义分割方法</h1><p><img src="/2021/10/04/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2-%E7%AE%97%E6%B3%95%E6%A6%82%E8%BF%B0/dl%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2.jpeg"></p>
<h2 id="FCN"><a href="#FCN" class="headerlink" title="FCN"></a>FCN</h2><p>通常cnn网络在卷积之后会接上若干个全连接层，将卷积层产生的特征图（feature map）映射成为一个固定长度的特征向量。一般的CNN结构适用于图像级别的分类和回归任务，因为它们最后都期望得到输入图像的分类的概率，如ALexNet网络最后输出一个1000维的向量表示输入图像属于每一类的概率。<br>FCN对图像进行像素级的分类，从而解决了语义级别的图像分割问题。与经典的CNN在卷积层使用全连接层得到固定长度的特征向量进行分类不同，FCN可以接受任意尺寸的输入图像，采用反卷积层对最后一个卷基层的特征图（feature map）进行上采样，使它恢复到输入图像相同的尺寸，从而可以对每一个像素都产生一个预测，同时保留了原始输入图像中的空间信息，最后奇偶在上采样的特征图进行像素的分类。<br>-全卷积网络(FCN)是从抽象的特征中恢复出每个像素所属的类别。即从图像级别的分类进一步延伸到像素级别的分类。<br>FCN将传统CNN中的全连接层转化成一个个的卷积层。如下图所示，在传统的CNN结构中，前5层是卷积层，第6层和第7层分别是一个长度为4096的一维向量，第8层是长度为1000的一维向量，分别对应1000个类别的概率。FCN将这3层表示为卷积层，卷积核的大小(通道数，宽，高)分别为（4096,7,7）、（4096,1,1）、（1000,1,1）。所有的层都是卷积层，故称为全卷积网络。 </p>
<p><img src="/2021/10/04/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2-%E7%AE%97%E6%B3%95%E6%A6%82%E8%BF%B0/FCN.png"></p>
<p>简单的说，FCN与CNN的区别在于FCN把CNN最后的全连接层换成卷积层，输出一张已经label好的图。</p>
<p>全连接层转换为卷积层：在两种变换中，将全连接层转化为卷积层在实际运用中更加有用。假设一个卷积神经网络的输入是 224x224x3 的图像，一系列的卷积层和下采样层将图像数据变为尺寸为 7x7x512 的激活数据体。AlexNet使用了两个尺寸为4096的全连接层，最后一个有1000个神经元的全连接层用于计算分类评分。我们可以将这3个全连接层中的任意一个转化为卷积层：</p>
<p>针对第一个连接区域是[7x7x512]的全连接层，令其滤波器尺寸为F=7，这样输出数据体就为[1x1x4096]了。</p>
<p>针对第二个全连接层，令其滤波器尺寸为F=1，这样输出数据体为[1x1x4096]。</p>
<p>对最后一个全连接层也做类似的，令其F=1，最终输出为[1x1x1000]</p>
<p><img src="/2021/10/04/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2-%E7%AE%97%E6%B3%95%E6%A6%82%E8%BF%B0/FCN2.png"></p>
<h2 id="逐像素预测"><a href="#逐像素预测" class="headerlink" title="逐像素预测"></a>逐像素预测</h2><p>采用反卷积层对最后一个卷积层的feature map进行上采样, 使它恢复到输入图像相同的尺寸，从而可以对每个像素都产生了一个预测, 同时保留了原始输入图像中的空间信息, 最后在上采样的特征图上进行逐像素分类。</p>
<p>经过多次卷积和pooling以后，得到的图像越来越小，分辨率越来越低。其中图像到 H/32∗W/32 的时候图片是最小的一层时，所产生图叫做heatmap热图，热图就是我们最重要的高维特征图。</p>
<p>得到高维特征的heatmap之后就是最重要的一步也是最后的一步对原图像进行upsampling，把图像进行放大、放大、放大，到原图像的大小。</p>
<p>（也就是将高维特征图翻译成原图时对应的分割图像！！）<br>最后的输出是21张heatmap经过upsampling变为原图大小的图片，为了对每个像素进行分类预测label成最后已经进行语义分割的图像，这里有一个小trick，就是最后通过逐个像素地求其在21张图像该像素位置的最大数值描述（概率）作为该像素的分类。因此产生了一张已经分类好的图片，</p>
<p>原文链接：<a href="https://blog.csdn.net/qq_36269513/article/details/80420363">https://blog.csdn.net/qq_36269513/article/details/80420363</a></p>
<h2 id="Segnet"><a href="#Segnet" class="headerlink" title="Segnet"></a>Segnet</h2><p>Segnet是用于进行像素级别图像分割的全卷积网络，分割的核心组件是一个encoder 网络，及其相对应的decoder网络，后接一个象素级别的分类网络。encoder网络：其结构与VGG16网络的前13层卷积层的结构相似。decoder网络：作用是将由encoder的到的低分辨率的feature maps 进行映射得到与输入图像featuremap相同的分辨率进而进行像素级别的分类。Segnet的亮点：decoder进行上采样的方式，直接利用与之对应的encoder阶段中进行max-pooling时的polling index 进行非线性上采样，这样做的好处是上采样阶段就不需要进行学习。 上采样后得到的feature maps 是非常稀疏的，因此，需要进一步选择合适的卷积核进行卷积得到dense featuremaps 。作者与FCN，DeepLab-LargeFOV, DenconvNet结构进行比较，统筹内存与准确率，Segnet实现良好的分割效果。SegNet主要用于场景理解应用，需要在进行inference时考虑内存的占用及分割的准确率。同时，Segnet的训练参数较少（将前面提到的VGG16的全连接层剔除），可以用SGD进行end-to-end训练。</p>
<p>在decoder 网络中重用encder 网络中对应max pooling index的三点好处：（i）提高边界划分 (ii)减少训练的参数 (iii)这种形式可以广泛的应用在其他encoder-decoder结构。</p>
<h2 id="Dilated-convolution"><a href="#Dilated-convolution" class="headerlink" title="Dilated convolution"></a>Dilated convolution</h2><h2 id="DeepLab-v1-v2"><a href="#DeepLab-v1-v2" class="headerlink" title="DeepLab(v1,v2)"></a>DeepLab(v1,v2)</h2><h2 id="RefineNet"><a href="#RefineNet" class="headerlink" title="RefineNet"></a>RefineNet</h2><h2 id="PSPNet"><a href="#PSPNet" class="headerlink" title="PSPNet"></a>PSPNet</h2><h2 id="Large-Kernel-Matters"><a href="#Large-Kernel-Matters" class="headerlink" title="Large Kernel Matters"></a>Large Kernel Matters</h2>]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>概述</tag>
      </tags>
  </entry>
  <entry>
    <title>轻量化网络-MobileNet</title>
    <url>/2021/09/29/%E8%BD%BB%E9%87%8F%E5%8C%96%E7%BD%91%E7%BB%9C-MobileNet/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>背景问题：</p>
<p>方法简述：</p>
<p>优点：</p>
<p>缺点：</p>
<span id="more"></span>
]]></content>
      <categories>
        <category>学习</category>
        <category>cv算法</category>
        <category>深度学习</category>
        <category>轻量化模型</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>论文复现</tag>
      </tags>
  </entry>
  <entry>
    <title>针对深度学习网络的PyTorch训练代码结构</title>
    <url>/2021/10/09/%E9%92%88%E5%AF%B9%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%BD%91%E7%BB%9C%E7%9A%84PyTorch%E8%AE%AD%E7%BB%83%E4%BB%A3%E7%A0%81%E7%BB%93%E6%9E%84/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>根目录文件夹下包含三个文件夹和两个python代码文件：</p>
<p><code>/data</code> #文件夹内分布保存</p>
<p>—-<code>/test</code>测试集数据：</p>
<p>—-<code>/train</code>训练集数据和对应的标签数据</p>
<p>——–<code>/image</code> #训练数据</p>
<p>——–<code>/label</code> # 分割标签数据集，分类问题的话是一个label.csv</p>
<p>—-<code>/mode</code> 模型文件夹</p>
<p>——– model.py 定义自己的模型类</p>
<p>—- <code>/utils</code>文件夹 辅助方法</p>
<p>——– dataset.py 定义自己的数据集类 Dataset，Dataset_loader</p>
<p>—-train.py 实例化模型，设置loss 优化器，开始训练，调整l r</p>
<p>—-predict.py 加载测试数据集，开启eval，获得预测结果</p>
<span id="more"></span>

<h1 id="根目录结构"><a href="#根目录结构" class="headerlink" title="根目录结构"></a>根目录结构</h1><p><img src="/2021/10/09/%E9%92%88%E5%AF%B9%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%BD%91%E7%BB%9C%E7%9A%84PyTorch%E8%AE%AD%E7%BB%83%E4%BB%A3%E7%A0%81%E7%BB%93%E6%9E%84/%E6%96%87%E4%BB%B6%E7%9B%AE%E5%BD%95.png"></p>
<p>根目录文件夹下包含三个文件夹和两个python代码文件：</p>
<p><code>/data</code> #文件夹内分布保存</p>
<p>—-<code>/test</code>测试集数据：</p>
<p>—-<code>/train</code>训练集数据和对应的标签数据</p>
<p>——–<code>/image</code> #训练数据</p>
<p>——–<code>/label</code> # 分割标签数据集，分类问题的话是一个label.csv</p>
<p>—-<code>/mode</code> 模型文件夹</p>
<p>——– model.py 定义自己的模型类</p>
<p>—- <code>/utils</code>文件夹 辅助方法</p>
<p>——– dataset.py 定义自己的数据集类 Dataset，Dataset_loader</p>
<p>——– others.py 如绘图等</p>
<p>—-train.py 实例化模型，设置loss 优化器，开始训练，调整l r</p>
<p>—-predict.py 加载测试数据集，开启eval，获得预测结果</p>
<p>从参数定义，到网络模型定义，再到训练步骤，验证步骤，测试步骤，总结了一套较为直观的模板。目录如下：</p>
<ol>
<li>导入包以及设置随机种子</li>
<li>以类的方式定义超参数</li>
<li>定义自己的模型</li>
<li>定义早停类(此步骤可以省略)</li>
<li>定义自己的数据集Dataset,DataLoader</li>
<li>实例化模型，设置loss，优化器等</li>
<li>开始训练以及调整lr</li>
<li>绘图</li>
<li>预测</li>
</ol>
<h2 id="导入包以及设置随机种子"><a href="#导入包以及设置随机种子" class="headerlink" title="导入包以及设置随机种子"></a>导入包以及设置随机种子</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader, Dataset</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line">seed = <span class="number">42</span></span><br><span class="line">torch.manual_seed(seed)</span><br><span class="line">np.random.seed(seed)</span><br><span class="line">random.seed(seed)</span><br></pre></td></tr></table></figure>

<h1 id="定义超参数"><a href="#定义超参数" class="headerlink" title="定义超参数"></a>定义超参数</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">argparse</span>():</span></span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">args = argparse()</span><br><span class="line">args.epochs, args.learning_rate, args.patience = [<span class="number">30</span>, <span class="number">0.001</span>, <span class="number">4</span>]</span><br><span class="line">args.hidden_size, args.input_size= [<span class="number">40</span>, <span class="number">30</span>]</span><br><span class="line">args.device, = [torch.device(<span class="string">&quot;cuda:0&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>),]</span><br></pre></td></tr></table></figure>

<h1 id="定义自己的模型"><a href="#定义自己的模型" class="headerlink" title="定义自己的模型"></a>定义自己的模型</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Your_model</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Your_model, self).__init__()</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self,x</span>):</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>



<h1 id="定义数据集类Dataset-DataLoader"><a href="#定义数据集类Dataset-DataLoader" class="headerlink" title="定义数据集类Dataset,DataLoader"></a>定义数据集类Dataset,DataLoader</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Dataset_name</span>(<span class="params">Dataset</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, flag=<span class="string">&#x27;train&#x27;</span></span>):</span></span><br><span class="line">        <span class="keyword">assert</span> flag <span class="keyword">in</span> [<span class="string">&#x27;train&#x27;</span>, <span class="string">&#x27;test&#x27;</span>, <span class="string">&#x27;valid&#x27;</span>]</span><br><span class="line">        self.flag = flag</span><br><span class="line">        self.__load_data__()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self, index</span>):</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__load_data__</span>(<span class="params">self, csv_paths: <span class="built_in">list</span></span>):</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">        <span class="built_in">print</span>(</span><br><span class="line">            <span class="string">&quot;train_X.shape:&#123;&#125;\ntrain_Y.shape:&#123;&#125;\nvalid_X.shape:&#123;&#125;\nvalid_Y.shape:&#123;&#125;\n&quot;</span></span><br><span class="line">            .<span class="built_in">format</span>(self.train_X.shape, self.train_Y.shape, self.valid_X.shape, self.valid_Y.shape))</span><br><span class="line"></span><br><span class="line">train_dataset = Dataset_name(flag=<span class="string">&#x27;train&#x27;</span>)</span><br><span class="line">train_dataloader = DataLoader(dataset=train_dataset, batch_size=<span class="number">64</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line">valid_dataset = Dataset_name(flag=<span class="string">&#x27;valid&#x27;</span>)</span><br><span class="line">valid_dataloader = DataLoader(dataset=valid_dataset, batch_size=<span class="number">64</span>, shuffle=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>



<h1 id="实例化模型，设置loss，优化器等"><a href="#实例化模型，设置loss，优化器等" class="headerlink" title="实例化模型，设置loss，优化器等"></a>实例化模型，设置loss，优化器等</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = Your_model().to(args.device)</span><br><span class="line">criterion = torch.nn.MSELoss()</span><br><span class="line">optimizer = torch.optim.Adam(Your_model.parameters(),lr=args.learning_rate)</span><br><span class="line"></span><br><span class="line">train_loss = []</span><br><span class="line">valid_loss = []</span><br><span class="line">train_epochs_loss = []</span><br><span class="line">valid_epochs_loss = []</span><br><span class="line"></span><br><span class="line">early_stopping = EarlyStopping(patience=args.patience,verbose=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>



<h1 id="训练以及调整lr"><a href="#训练以及调整lr" class="headerlink" title="训练以及调整lr"></a>训练以及调整lr</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(args.epochs):</span><br><span class="line">    Your_model.train()</span><br><span class="line">    train_epoch_loss = []</span><br><span class="line">    <span class="keyword">for</span> idx,(data_x,data_y) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_dataloader,<span class="number">0</span>):</span><br><span class="line">        data_x = data_x.to(torch.float32).to(args.device)</span><br><span class="line">        data_y = data_y.to(torch.float32).to(args.device)</span><br><span class="line">        outputs = Your_model(data_x)</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss = criterion(data_y,outputs)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        train_epoch_loss.append(loss.item())</span><br><span class="line">        train_loss.append(loss.item())</span><br><span class="line">        <span class="keyword">if</span> idx%(<span class="built_in">len</span>(train_dataloader)//<span class="number">2</span>)==<span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;epoch=&#123;&#125;/&#123;&#125;,&#123;&#125;/&#123;&#125;of train, loss=&#123;&#125;&quot;</span>.<span class="built_in">format</span>(</span><br><span class="line">                epoch, args.epochs, idx, <span class="built_in">len</span>(train_dataloader),loss.item()))</span><br><span class="line">    train_epochs_loss.append(np.average(train_epoch_loss))</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#=====================valid============================</span></span><br><span class="line">    Your_model.<span class="built_in">eval</span>()</span><br><span class="line">    valid_epoch_loss = []</span><br><span class="line">    <span class="keyword">for</span> idx,(data_x,data_y) <span class="keyword">in</span> <span class="built_in">enumerate</span>(valid_dataloader,<span class="number">0</span>):</span><br><span class="line">        data_x = data_x.to(torch.float32).to(args.device)</span><br><span class="line">        data_y = data_y.to(torch.float32).to(args.device)</span><br><span class="line">        outputs = Your_model(data_x)</span><br><span class="line">        loss = criterion(outputs,data_y)</span><br><span class="line">        valid_epoch_loss.append(loss.item())</span><br><span class="line">        valid_loss.append(loss.item())</span><br><span class="line">    valid_epochs_loss.append(np.average(valid_epoch_loss))</span><br><span class="line">    <span class="comment">#==================early stopping======================</span></span><br><span class="line">    early_stopping(valid_epochs_loss[-<span class="number">1</span>],model=Your_model,path=<span class="string">r&#x27;c:\\your_model_to_save&#x27;</span>)</span><br><span class="line">    <span class="keyword">if</span> early_stopping.early_stop:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Early stopping&quot;</span>)</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    <span class="comment">#====================adjust lr========================</span></span><br><span class="line">    lr_adjust = &#123;</span><br><span class="line">            <span class="number">2</span>: <span class="number">5e-5</span>, <span class="number">4</span>: <span class="number">1e-5</span>, <span class="number">6</span>: <span class="number">5e-6</span>, <span class="number">8</span>: <span class="number">1e-6</span>,</span><br><span class="line">            <span class="number">10</span>: <span class="number">5e-7</span>, <span class="number">15</span>: <span class="number">1e-7</span>, <span class="number">20</span>: <span class="number">5e-8</span></span><br><span class="line">        &#125;</span><br><span class="line">    <span class="keyword">if</span> epoch <span class="keyword">in</span> lr_adjust.keys():</span><br><span class="line">        lr = lr_adjust[epoch]</span><br><span class="line">        <span class="keyword">for</span> param_group <span class="keyword">in</span> optimizer.param_groups:</span><br><span class="line">            param_group[<span class="string">&#x27;lr&#x27;</span>] = lr</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Updating learning rate to &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(lr))</span><br></pre></td></tr></table></figure>

<h1 id="绘图查看loss趋势"><a href="#绘图查看loss趋势" class="headerlink" title="绘图查看loss趋势"></a>绘图查看loss趋势</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">12</span>,<span class="number">4</span>))</span><br><span class="line">plt.subplot(<span class="number">121</span>)</span><br><span class="line">plt.plot(train_loss[:])</span><br><span class="line">plt.title(<span class="string">&quot;train_loss&quot;</span>)</span><br><span class="line">plt.subplot(<span class="number">122</span>)</span><br><span class="line">plt.plot(train_epochs_loss[<span class="number">1</span>:],<span class="string">&#x27;-o&#x27;</span>,label=<span class="string">&quot;train_loss&quot;</span>)</span><br><span class="line">plt.plot(valid_epochs_loss[<span class="number">1</span>:],<span class="string">&#x27;-o&#x27;</span>,label=<span class="string">&quot;valid_loss&quot;</span>)</span><br><span class="line">plt.title(<span class="string">&quot;epochs_loss&quot;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<h1 id="预测"><a href="#预测" class="headerlink" title="预测"></a>预测</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 此处可定义一个预测集的Dataloader。也可以直接将你的预测数据reshape,添加batch_size=1</span></span><br><span class="line">Your_model.<span class="built_in">eval</span>()</span><br><span class="line">predict = Your_model(data)</span><br></pre></td></tr></table></figure>





]]></content>
      <categories>
        <category>学习</category>
        <category>Pytorch</category>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>基础操作</tag>
        <tag>编程</tag>
      </tags>
  </entry>
  <entry>
    <title>项目实战-延时显微镜下的细胞检测和跟踪</title>
    <url>/2021/10/01/%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98-%E5%BB%B6%E6%97%B6%E6%98%BE%E5%BE%AE%E9%95%9C%E4%B8%8B%E7%9A%84%E7%BB%86%E8%83%9E%E6%A3%80%E6%B5%8B%E5%92%8C%E8%B7%9F%E8%B8%AA/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>这个项目是9517 计算机视觉课程的一个project</p>
<p>要求是检测显微镜下图片中的细胞以及后续判断细胞的移动轨迹</p>
<p>Version1.0是使用了传统的OpenCV方法做的</p>
<p>对延时显微镜下的细胞图像做训练，通过OpenCV形态学操作对图像做预处理</p>
<p>Version1.0: 使用Unet网络框架进行细胞检测并多目标跟踪绘制细胞运动轨迹及运动属性计算</p>
]]></content>
  </entry>
  <entry>
    <title>深度学习-常用卷积方法梳理</title>
    <url>/2021/10/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%B8%B8%E7%94%A8%E5%8D%B7%E7%A7%AF%E6%96%B9%E6%B3%95%E6%A2%B3%E7%90%86/</url>
    <content><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><ol>
<li>2D卷积（单通道，多通道）</li>
<li>3D 卷积</li>
<li>1×1 卷积</li>
<li>转置卷积（反卷积）</li>
<li>空洞卷积</li>
<li>可分卷积（空间可分卷积，深度可分卷积）</li>
<li>分组卷积</li>
</ol>
<span id="more"></span>

<p>卷积的目的是为了从输入中提取有用的特征。</p>
<p>卷积的优势在于，权重共享和平移不变性。同时还考虑到了像素空间的关系， 有些任务通常涉及识别具有空间关系的对象。（例如：狗的身体通常连接头部、四肢和尾部）</p>
<h1 id="2D-卷积"><a href="#2D-卷积" class="headerlink" title="2D 卷积"></a>2D 卷积</h1><h2 id="单通道卷积"><a href="#单通道卷积" class="headerlink" title="单通道卷积"></a>单通道卷积</h2><p><img src="/2021/10/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%B8%B8%E7%94%A8%E5%8D%B7%E7%A7%AF%E6%96%B9%E6%B3%95%E6%A2%B3%E7%90%86/cov_deeplearning_single.gif"></p>
<p>在深度学习中，卷积是元素对元素的加法和乘法。</p>
<p>对于具有单通道的图像，卷积如上图所示（stride=1，padding=0）。在这里的滤波器是一个3x3的矩阵[[0,1,2],[2,2,0],[0,1,2]]。滤波器滑过输入，在每个位置完成一次卷积，每个滑动位置得到一个数字。最终输出仍然是一个3x3的矩阵。</p>
<p>filter和kernel是不同的。一个“Kernel”更倾向于是2D的权重矩阵。而“filter”则是指多个Kernel堆叠的3D结构。如果是一个2D的filter，那么两者就是一样的。但是一个3Dfilter，在大多数深度学习的卷积中，它是包含kernel的。每个卷积核都是独一无二的，主要在于强调输入通道的不同方面。</p>
<h2 id="多通道卷积"><a href="#多通道卷积" class="headerlink" title="多通道卷积"></a>多通道卷积</h2><p><img src="/2021/10/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%B8%B8%E7%94%A8%E5%8D%B7%E7%A7%AF%E6%96%B9%E6%B3%95%E6%A2%B3%E7%90%86/multi_conv.gif"></p>
<p><img src="/2021/10/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%B8%B8%E7%94%A8%E5%8D%B7%E7%A7%AF%E6%96%B9%E6%B3%95%E6%A2%B3%E7%90%86/multi_conv2.gif"></p>
<p>输入是一个5x5x3的矩阵，有三个通道。filter是一个3x3x3的矩阵。首先，filter中的每个卷积核分别应用于输入层中的三个通道。执行三次卷积，产生3个3x3的通道。然后，这三个通道相加（矩阵加法），得到一个3x3x1的单通道。这个通道就是在输入层（5x5x3矩阵）应用filter（3x3x3矩阵）的结果。</p>
<p>这个过程可以看作是一个3Dfilter矩阵滑过输入层。值得注意的是，输入层和filter有相同的深度（通道数量=卷积核数量）。3Dfilter只需要在2维图像的高和宽方向上移动，因此这种操作被称为2D卷积，尽管是使用的3D滤波器来处理3D数据。在每一个滑动位置，执行卷积，得到一个数字。就像下面的例子中体现的，滑动水平的5个位置和垂直的5个位置进行。然后可以得到了一个单一通道的2D图像。</p>
<p><img src="/2021/10/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%B8%B8%E7%94%A8%E5%8D%B7%E7%A7%AF%E6%96%B9%E6%B3%95%E6%A2%B3%E7%90%86/multi_conv3.jpeg"></p>
<p>当输入通道和输出通道不同时的转换：假设输入层有Din个通道，我们想得到输出有Dout个通道。我们只需要将Dout个 filters应用到输入层。每一个 filter有Din个卷积核。每个filter提供一个输出通道。完成该过程，将结果堆叠在一起形成输出层。</p>
<p><img src="/2021/10/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%B8%B8%E7%94%A8%E5%8D%B7%E7%A7%AF%E6%96%B9%E6%B3%95%E6%A2%B3%E7%90%86/multi_conv4.jpeg"></p>
<h2 id="conv-with-padding-and-stride"><a href="#conv-with-padding-and-stride" class="headerlink" title="conv with padding and stride"></a>conv with padding and stride</h2><p>2D 卷积是在height&amp;width方向上的卷积</p>
<p>Kernel size（卷积核尺寸）：积核大小定义了卷积的视图。</p>
<p>Stride（步长）：定义了卷积核在图像中移动的每一步的大小。比如Stride=1，那么卷积核就是按一个像素大小移动。Stride=2，那么卷积核在图像中就是按2个像素移动（即，会跳过一个像素）。stride&gt;=2可以用来对图像进行下采样。</p>
<p>Padding：可以将Padding理解为在图像外围补充一些像素点。padding可以保持空间输出维度等于输入维度，必要的话，可以在输入外围填充0。另一方面，unpadded卷积只对输入图像的像素执行卷积，没有填充0。输出的尺寸将小于输入。</p>
<p>下图是2D卷积，Kernel size=3，Stride=1，Padding=1：</p>
<p><img src="/2021/10/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%B8%B8%E7%94%A8%E5%8D%B7%E7%A7%AF%E6%96%B9%E6%B3%95%E6%A2%B3%E7%90%86/stride_padding.gif"></p>
<p>卷积输出的size计算：</p>
<p>输入图像大小是<em>i</em>，kernel size=k，padding=p，stride=s，那么卷积后的输出<em>o</em>计算如下：</p>
<p><img src="/2021/10/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%B8%B8%E7%94%A8%E5%8D%B7%E7%A7%AF%E6%96%B9%E6%B3%95%E6%A2%B3%E7%90%86/size%E8%AE%A1%E7%AE%97.png"></p>
<h1 id="1X1的卷积"><a href="#1X1的卷积" class="headerlink" title="1X1的卷积"></a>1X1的卷积</h1><p>1X1的在图片depth维度的卷积</p>
<p>如果是单通道，1X1的卷积操作就是将每个元素乘上一个数字。</p>
<p>如果输入数据是多通道的。输入的数据是尺寸是H x W x D，滤波器尺寸是1 x 1x D，输出通道尺寸是H x W x 1。如果我们执行N次1x1卷积，并将结果连接在一起，那可以得到一个H x W x N的输出</p>
<p><img src="/2021/10/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%B8%B8%E7%94%A8%E5%8D%B7%E7%A7%AF%E6%96%B9%E6%B3%95%E6%A2%B3%E7%90%86/1x1.jpeg"></p>
<p>1 x 1卷积的优势如下：</p>
<ul>
<li>降低维度以实现高效计算： 完成1 x 1卷积操作后，显著的降低了depth-wise的维度</li>
<li>高效的低维嵌入，或特征池： 如果原始输入有200个通道，那么1 x 1卷积操作可以将这些通道嵌入到单一通道</li>
<li>卷积后再次应用非线性：在1 x 1卷积后，可以添加ReLU等非线性激活。非线性允许网络学习更加复杂的函数。</li>
</ul>
<h1 id="3D-卷积"><a href="#3D-卷积" class="headerlink" title="3D 卷积"></a>3D 卷积</h1><p>3D卷积是2D卷积的推广。在3D卷积中，滤波器的深度小于输入层的深度（也可以说卷积核尺寸小于通道尺寸）。所以，3D滤波器需要在数据的三个维度上移动（图像的长、宽、高）。在滤波器移动的每个位置，执行一次卷积，得到一个数字。当滤波器滑过整个3D空间，输出的结果也是一个3D的。</p>
<p><img src="/2021/10/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%B8%B8%E7%94%A8%E5%8D%B7%E7%A7%AF%E6%96%B9%E6%B3%95%E6%A2%B3%E7%90%86/3d_conv.jpeg"></p>
<p>在 3D 卷积中，3D 过滤器可以在所有三个方向（图像的高度、宽度、通道）上移动。在每个位置，逐元素的乘法和加法都会提供一个数值。因为过滤器是滑过一个 3D 空间，所以输出数值也按 3D 空间排布。也就是说输出是一个 3D 数据。</p>
<p>3D 卷积可以描述 3D 空间中目标的空间关系。对某些应用（比如生物医学影像中的 3D 分割/重构）而言，这样的 3D 关系很重要，比如在 CT 和 MRI 中，血管之类的目标会在 3D 空间中蜿蜒曲折。</p>
<h1 id="转置卷积-反卷积"><a href="#转置卷积-反卷积" class="headerlink" title="转置卷积-反卷积"></a>转置卷积-反卷积</h1><p>对于很多网络架构，我们往往需要进行上采样操作，与普通卷积方向相反的转换。比如在生成高分辨率图像以及将低维特征图映射到高维空间，比如在自动编码器或形义分割中。（在后者的例子中，形义分割首先会提取编码器中的特征图，然后在解码器中恢复原来的图像大小，使其可以分类原始图像中的每个像素。）</p>
<p>实现上采样的传统方法是应用插值方案或人工创建规则。而神经网络等现代架构则倾向于让网络自己自动学习合适的变换，无需人类干预。为了做到这一点，就产生了转置卷积的操作。</p>
<p>对于下图的例子，我们在一个 2×2 的输入（周围加了 2×2 的单位步长的零填充）上应用一个 3×3 核的转置卷积。上采样输出的大小是 4×4。</p>
<p><img src="/2021/10/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%B8%B8%E7%94%A8%E5%8D%B7%E7%A7%AF%E6%96%B9%E6%B3%95%E6%A2%B3%E7%90%86/%E8%BD%AC%E7%BD%AE%E5%8D%B7%E7%A7%AF1.jpeg"></p>
<p><img src="/2021/10/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%B8%B8%E7%94%A8%E5%8D%B7%E7%A7%AF%E6%96%B9%E6%B3%95%E6%A2%B3%E7%90%86/%E8%BD%AC%E7%BD%AE%E5%8D%B7%E7%A7%AF2.jpeg"></p>
<p>转置卷积如何通过计算机的矩阵乘法实现? 在卷积中，定义 C 为卷积核，Large 为输入图像，Small 为输出图像。经过卷积（矩阵乘法）后，我们将大图像下采样为小图像。这种矩阵乘法的卷积的实现遵照：C x Large = Small。</p>
<p>例如：将输入平展为 16×1 的矩阵，并将卷积核转换为一个稀疏矩阵（4×16）。然后，在稀疏矩阵和平展的输入之间使用矩阵乘法。之后，再将所得到的矩阵（4×1）转换为 2×2 的输出。</p>
<p><img src="/2021/10/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%B8%B8%E7%94%A8%E5%8D%B7%E7%A7%AF%E6%96%B9%E6%B3%95%E6%A2%B3%E7%90%86/%E8%BD%AC%E7%BD%AE%E5%8D%B7%E7%A7%AF3.jpeg"></p>
<p>现在，在等式的两边都乘上矩阵的转置 CT，并借助「一个矩阵与其转置矩阵的乘法得到一个单位矩阵」这一性质，那么我们就能得到公式 CT x Small = Large，如下图所示。</p>
<p><img src="/2021/10/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%B8%B8%E7%94%A8%E5%8D%B7%E7%A7%AF%E6%96%B9%E6%B3%95%E6%A2%B3%E7%90%86/%E8%BD%AC%E7%BD%AE%E5%8D%B7%E7%A7%AF4.jpeg"></p>
<p>这样就完成了从小图像到大图像的上采样</p>
<h1 id="空洞卷积（Dilated-卷积）"><a href="#空洞卷积（Dilated-卷积）" class="headerlink" title="空洞卷积（Dilated 卷积）"></a>空洞卷积（Dilated 卷积）</h1><p>扩张卷积就是通过在核元素之间插入空格来使核「膨胀」。新增的参数 l（扩张率）表示我们希望将核加宽的程度。具体实现可能各不相同，但通常是在核元素之间插入 l-1 个空格。下面展示了 l = 1, 2, 4 时的核大小。</p>
<p><img src="/2021/10/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%B8%B8%E7%94%A8%E5%8D%B7%E7%A7%AF%E6%96%B9%E6%B3%95%E6%A2%B3%E7%90%86/%E7%A9%BA%E6%B4%9E%E5%8D%B7%E7%A7%AF.jpeg"></p>
<p><em>基本上无需添加额外的成本就能有较大的感受野。</em></p>
<p>在这张图像中，3×3 的红点表示经过卷积后，输出图像是 3×3 像素。尽管所有这三个空洞卷积的输出都是同一尺寸，但模型观察到的感受野有很大的不同。l=1 时感受野为 3×3，l=2 时为 7×7。l=3 时，感受野的大小就增加到了 15×15。但是，与这些操作相关的参数的数量是相等的。「观察」更大的感受野不会有额外的成本。因此，空洞卷积可用于廉价地增大输出单元的感受野，而不会增大其核大小，这在多个空洞卷积彼此堆叠时尤其有效。</p>
<h1 id="可分离卷积"><a href="#可分离卷积" class="headerlink" title="可分离卷积"></a>可分离卷积</h1><p>使用模型： MobileNet， Xcaption</p>
<h2 id="空间可分卷积"><a href="#空间可分卷积" class="headerlink" title="空间可分卷积"></a>空间可分卷积</h2><p>空间可分卷积操作的是图像的 2D 空间维度，即高和宽。从概念上看，空间可分卷积是将一个卷积分解为两个单独的运算。对于下面的示例，3×3 的 Sobel 核被分成了一个 3×1 核和一个 1×3 核。</p>
<p><img src="/2021/10/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%B8%B8%E7%94%A8%E5%8D%B7%E7%A7%AF%E6%96%B9%E6%B3%95%E6%A2%B3%E7%90%86/%E5%8F%AF%E5%88%86%E7%A6%BB1.png"></p>
<p>在卷积中，3×3 核直接与图像卷积。在空间可分卷积中，3×1 核首先与图像卷积，然后再应用 1×3 核。这样，执行同样的操作时仅需 6 个参数，而不是 9 个。此外，使用空间可分卷积时所需的矩阵乘法也更少。</p>
<p>例如，5×5 图像与 3×3 核的卷积（步幅=1，填充=0）要求在 3 个位置水平地扫描核（还有 3 个垂直的位置）。总共就是 9 个位置，表示为下图中的点。在每个位置，会应用 9 次逐元素乘法。总共就是 9×9=81 次乘法。</p>
<p>但是，先在 5×5 的图像上应用一个 3×1 的过滤器。我们可以在水平 5 个位置和垂直 3 个位置扫描这样的核。总共就是 5×3=15 个位置，表示为下图中的点。在每个位置，会应用 3 次逐元素乘法。总共就是 15×3=45 次乘法。现在得到了一个 3×5 的矩阵。这个矩阵再与一个 1×3 核卷积，即在水平 3 个位置和垂直 3 个位置扫描这个矩阵。对于这 9 个位置中的每一个，应用 3 次逐元素乘法。这一步需要 9×3=27 次乘法。因此，总体而言，空间可分卷积需要 45+27=72 次乘法，少于普通卷积。</p>
<p><img src="/2021/10/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%B8%B8%E7%94%A8%E5%8D%B7%E7%A7%AF%E6%96%B9%E6%B3%95%E6%A2%B3%E7%90%86/%E5%8F%AF%E5%88%86%E7%A6%BB2.jpg"></p>
<p><strong>归纳：</strong>假设我们现在将卷积应用于一张 N×N 的图像上，卷积核为 m×m，步幅为 1，填充为 0。传统卷积需要 (N-2) x (N-2) x m x m 次乘法，空间可分卷积需要 N x (N-2) x m + (N-2) x (N-2) x m = (2N-2) x (N-2) x m 次乘法。空间可分卷积与标准卷积的计算成本比为：</p>
<p><img src="/2021/10/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%B8%B8%E7%94%A8%E5%8D%B7%E7%A7%AF%E6%96%B9%E6%B3%95%E6%A2%B3%E7%90%86/%E5%8F%AF%E5%88%86%E7%A6%BB3.png"></p>
<p>因为图像尺寸 N 远大于过滤器大小（N&gt;&gt;m），所以这个比就变成了 2/m。也就是说，在这种渐进情况（N&gt;&gt;m）下，当过滤器大小为 3×3 时，空间可分卷积的计算成本是标准卷积的 2/3。过滤器大小为 5×5 时这一数值是 2/5；过滤器大小为 7×7 时则为 2/7。</p>
<p><strong>优点：</strong>空间可分卷积能节省成本</p>
<p><strong>缺点：</strong>并非所有的核都能分成两个更小的核。如果用空间可分卷积替代所有的传统卷积，在训练过程中搜索所有可能的核。这样得到的训练结果可能是次优的。</p>
<h2 id="深度可分卷积"><a href="#深度可分卷积" class="headerlink" title="深度可分卷积"></a>深度可分卷积</h2><p>MobileNet 和 Xception</p>
<p>深度可分卷积包含两个步骤： 深度卷积 +  1×1 卷积</p>
<p>例如：输入层的大小是 7×7×3（高×宽×通道） 需要转换成输出层（5×5×128）</p>
<p>标准的2D卷积的操作如下：128 个 5×5×1 的输出映射图（map）。然后我们将这些映射图堆叠成大小为 5×5×128 的单层</p>
<p><img src="/2021/10/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%B8%B8%E7%94%A8%E5%8D%B7%E7%A7%AF%E6%96%B9%E6%B3%95%E6%A2%B3%E7%90%86/%E6%B7%B1%E5%BA%A6%E5%8F%AF%E5%88%86%E7%A6%BB1.jpg"></p>
<p>深度可分卷积实现同样的变换: </p>
<p>深度可分卷积——第一步：分开使用 3 个核。每个过滤器的大小为 3×3×1。每个核与输入层的一个通道卷积（仅一个通道，而非所有通道！）。每个这样的卷积都能提供大小为 5×5×1 的映射图。然后将这些映射图堆叠在一起，创建一个 5×5×3 的图像。操作之后得到大小为 5×5×3 的输出。现在降低空间维度了，但深度还是和之前一样。</p>
<p><img src="/2021/10/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%B8%B8%E7%94%A8%E5%8D%B7%E7%A7%AF%E6%96%B9%E6%B3%95%E6%A2%B3%E7%90%86/%E6%B7%B1%E5%BA%A6%E5%8F%AF%E5%88%86%E7%A6%BB2.jpg"></p>
<p>在深度可分卷积的第二步，为了扩展深度，应用128个核大小为 1×1×3 的 1×1 卷积。将 5×5×3 的输入图像与每个 1×1×3 的核卷积，可得到大小为 5×5×1 的映射图。</p>
<p><img src="/2021/10/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%B8%B8%E7%94%A8%E5%8D%B7%E7%A7%AF%E6%96%B9%E6%B3%95%E6%A2%B3%E7%90%86/%E6%B7%B1%E5%BA%A6%E5%8F%AF%E5%88%86%E7%A6%BB3.jpg"></p>
<p><img src="/2021/10/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%B8%B8%E7%94%A8%E5%8D%B7%E7%A7%AF%E6%96%B9%E6%B3%95%E6%A2%B3%E7%90%86/%E6%B7%B1%E5%BA%A6%E5%8F%AF%E5%88%86%E7%A6%BB4.jpg"></p>
<p><strong>2D 卷积例子的成本：</strong>有 128 个 3×3×3 个核移动了 5×5 次，也就是 128 x 3 x 3 x 3 x 5 x 5 = 86400 次乘法。</p>
<p><strong>可分卷积的成本：</strong></p>
<p>在第一个深度卷积步骤，有 3 个 3×3×1 核移动 5×5 次，也就是 3x3x3x1x5x5 = 675 次乘法。</p>
<p>在 1×1 卷积的第二步，有 128 个 1×1×3 核移动 5×5 次，即 128 x 1 x 1 x 3 x 5 x 5 = 9600 次乘法。因此，深度可分卷积共有 675 + 9600 = 10275 次乘法。这样的成本大概仅有 2D 卷积的 12%！</p>
<p><strong>归纳：</strong>对于大小为 H×W×D 的输入图像，如果使用 Nc 个大小为 h×h×D 的核执行 2D 卷积（步幅为 1，填充为 0，其中 h 是偶数）。为了将输入层（H×W×D）变换到输出层（(H-h+1)x (W-h+1) x Nc），所需的总乘法次数为：</p>
<p><strong>Nc x h x h x D x (H-h+1) x (W-h+1)</strong></p>
<p>另一方面，对于同样的变换，深度可分卷积所需的乘法次数为：</p>
<p>**D x h x h x 1 x (H-h+1) x (W-h+1) + Nc x 1 x 1 x D x (H-h+1) x (W-h+1) **</p>
<p><strong>= (h x h + Nc) x D x (H-h+1) x (W-h+1)</strong></p>
<p>则深度可分卷积与 2D 卷积所需的乘法次数比为：</p>
<p><img src="/2021/10/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%B8%B8%E7%94%A8%E5%8D%B7%E7%A7%AF%E6%96%B9%E6%B3%95%E6%A2%B3%E7%90%86/%E6%B7%B1%E5%BA%A6%E5%8F%AF%E5%88%86%E7%A6%BB5.png"></p>
<p>大多数架构的输出层通常都有很多通道，可达数百甚至上千。对于这样的层（Nc &gt;&gt; h），则上式可约简为 1 / h²。基于此，如果使用 3×3 过滤器，则 2D 卷积所需的乘法次数是深度可分卷积的 9 倍。如果使用 5×5 过滤器，则 2D 卷积所需的乘法次数是深度可分卷积的 25 倍。</p>
<p>优点：深度可分卷积会降低卷积中参数的数量</p>
<p>缺点：对于较小的模型而言，如果用深度可分卷积替代 2D 卷积，模型的能力可能会显著下降。但是，如果使用得当，深度可分卷积能在不降低模型性能的前提下帮助你实现效率提升。</p>
<h1 id="分组卷积"><a href="#分组卷积" class="headerlink" title="分组卷积"></a>分组卷积</h1><p>AlexNet 论文（<a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf%EF%BC%89%E5%9C%A8">https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf）在</a> 2012 年引入了分组卷积。实现分组卷积的主要原因是让网络训练可在 2 个内存有限（每个 GPU 有 1.5 GB 内存）的 GPU 上进行。下面的 AlexNet 表明在大多数层中都有两个分开的卷积路径。这是在两个 GPU 上执行模型并行化（当然如果可以使用更多 GPU，还能执行多 GPU 并行化）。</p>
<p>即通过应用 Dout 个大小为 h x w x Din 的核将输入层（Hin x Win x Din）变换到输出层（Hout x Wout x Dout）。在分组卷积中，过滤器会被分为不同的组。每一组都负责特定深度的典型 2D 卷积。如下图：</p>
<p><img src="/2021/10/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%B8%B8%E7%94%A8%E5%8D%B7%E7%A7%AF%E6%96%B9%E6%B3%95%E6%A2%B3%E7%90%86/%E5%88%86%E7%BB%84%E5%8D%B7%E7%A7%AF1.jpg"></p>
<p>上图展示了具有两个过滤器分组的分组卷积。在每个过滤器分组中，每个过滤器的深度仅有名义上的 2D 卷积的一半。它们的深度是 Din/2。每个过滤器分组包含 Dout/2 个过滤器。第一个过滤器分组（红色）与输入层的前一半（[:, :, 0:Din/2]）卷积，而第二个过滤器分组（橙色）与输入层的后一半（[:, :, Din/2:Din]）卷积。最后将这些通道堆叠在一起，得到有 Dout 个通道的输出层。</p>
<p>分组卷积是更具过滤器数量来进行分组，每个分组都包含 Dout/Din 个过滤器</p>
<p>分组卷积的优点：</p>
<p>第一个优点是高效训练。因为卷积模型可以并行方式在多个 GPU 上进行训练。相比于在单个 GPU 上完成所有任务，这样的在多个 GPU 上的模型并行化能让网络在每个步骤处理更多图像。人们一般认为模型并行化比数据并行化更好。后者是将数据集分成多个批次，然后分开训练每一批。但是，当批量大小变得过小时，我们本质上是执行随机梯度下降，而非批梯度下降。这会造成更慢，有时候更差的收敛结果。 在训练非常深的神经网络时，分组卷积会非常重要，比如 ResNeXt 网络中<u>【后期补充】</u>。</p>
<p>第二个优点是模型会更高效，即模型参数会随过滤器分组数的增大而减少。在之前的例子中，完整的标准 2D 卷积有 h x w x Din x Dout 个参数。具有 2 个过滤器分组的分组卷积有 (h x w x Din/2 x Dout/2) x 2 个参数。参数数量减少了一半。</p>
<p>第三个优点：分组卷积也许能提供比标准完整 2D 卷积更好的模型。每个过滤器分组都会学习数据的一个独特表征。正如 AlexNet 的作者指出的那样，过滤器分组似乎会将学习到的过滤器结构性地组织成两个不同的分组——黑白过滤器和彩色过滤器。 另外和稀疏过滤器的关系有关 需要参考<a href="https://blog.yani.io/filter-group-tutorial%E3%80%90%E6%9A%82%E6%97%B6%E6%B2%A1%E8%AF%BB%E6%87%82%E3%80%91">https://blog.yani.io/filter-group-tutorial【暂时没读懂】</a></p>
]]></content>
      <categories>
        <category>学习</category>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>汇总</tag>
      </tags>
  </entry>
</search>
